
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 1: Modeling sequencies and encoding text — Neuromatch Academy: Deep Learning</title>
<link href="../../../_static/css/theme.css" rel="stylesheet"/>
<link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<link as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script src="../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
<script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
<script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-dl-logo-square-4xp.jpeg" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="W2D3_Tutorial2.html" rel="next" title="Tutorial 2: Modern RNNs and their variants"/>
<link href="../chapter_title.html" rel="prev" title="Modern Recurrent Neural Networks"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</link></link></link></link></head>
<body data-offset="80" data-spy="scroll" data-target="#bd-toc-nav">
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
<div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<img alt="logo" class="logo" src="../../../_static/nma-dl-logo-square-4xp.jpeg"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main navigation" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
   Introduction
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using Discord
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html">
     Tutorial 1: PyTorch
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html">
     Tutorial 3: Deep linear neural networks
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_Optimization/chapter_title.html">
   Optimization (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_Optimization/student/W1D4_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_Regularization/chapter_title.html">
   Regularization (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Doing more with fewer parameters
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/chapter_title.html">
   Convnets And Recurrent Neural Networks (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.html">
     Tutorial 2: Training loop of CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial3.html">
     Tutorial 3: Introduction to RNNs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial2.html">
     Tutorial 2: Facial recognition using modern convnets
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Modern Recurrent Neural Networks (W2D3)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 1: Modeling sequencies and encoding text
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W2D3_Tutorial2.html">
     Tutorial 2: Modern RNNs and their variants
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/student/W2D4_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D5_GenerativeModels/chapter_title.html">
   Generative Models (W2D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial2.html">
     Tutorial 2: Introduction to GANs and Density Ratio Estimation Perspective of GANs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial3.html">
     Tutorial 3: Conditional GANs and Implications of GAN Technology
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Advanced topics
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/chapter_title.html">
   Unsupervised And Self Supervised Learning (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html">
     Tutorial 1: Un/Self-supervised learning methods
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/chapter_title.html">
   Basic Reinforcement Learning (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.html">
     Tutorial 1: Introduction to Reinforcement Learning
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/chapter_title.html">
   Reinforcement Learning For Games (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/student/W3D3_Tutorial1.html">
     Tutorial 1: Learn to play games with RL
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ContinualLearning/chapter_title.html">
   Continual Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Introduction to Continual Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Out-of-distribution (OOD) Learning
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction to projects
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through2_DL.html">
     Modeling Steps 1 - 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_3through4_DL.html">
     Modeling Steps 3 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through6_DL.html">
     Modeling Steps 5 - 6
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_7through9_DL.html">
     Modeling Steps 7 - 9
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_10_DL.html">
     Modeling Steps 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProjectDL.html">
     Example Data Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModelingProjectDL.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/Example_Deep_Learning_Project.html">
     Example Deep Learning Project
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/projects_overview.html">
   Project Templates
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ComputerVision/README.html">
     Computer Vision
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/em_synapses.html">
       Knowledge Extraction from a Convolutional Neural Network
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/screws.html">
       Something Screwy - image recognition, detection, and classification of screws
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/image_alignment.html">
       Image Alignment
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/data_augmentation.html">
       Data Augmentation in image classification models
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/transfer_learning.html">
       Transfer Learning
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ReinforcementLearning/README.html">
     Reinforcement Learning
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/slides.html">
       Slides
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/README.html">
     Natural Language Processing
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/slides.html">
       Slides
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/Neuroscience/README.html">
     Neuroscience
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/pose_estimation.html">
       Animal Pose Estimation
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/cellular_segmentation.html">
       Segmentation and Denoising
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/algonauts_videos.html">
       Load algonauts videos
      </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</nav> <!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
<div class="topbar container-xl fixed-top">
<div class="topbar-contents row">
<div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
<div class="col pl-md-4 topbar-main">
<button aria-controls="site-navigation" aria-expanded="true" aria-label="Toggle navigation" class="navbar-toggler ml-0" data-placement="left" data-target=".site-navigation" data-toggle="tooltip" id="navbar-toggler" title="Toggle navigation" type="button">
<i class="fas fa-bars"></i>
<i class="fas fa-arrow-left"></i>
<i class="fas fa-arrow-up"></i>
</button>
<div class="dropdown-buttons-trigger">
<button aria-label="Download this page" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fas fa-download"></i></button>
<div class="dropdown-buttons">
<!-- ipynb file if we had a myst markdown file -->
<!-- Download raw file -->
<a class="dropdown-buttons" href="../../../_sources/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.ipynb"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Download source file" type="button">.ipynb</button></a>
<!-- Download PDF via print -->
<button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" id="download-print" onclick="window.print()" title="Print to PDF" type="button">.pdf</button>
</div>
</div>
<!-- Source interaction buttons -->
<div class="dropdown-buttons-trigger">
<button aria-label="Connect with source repository" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fab fa-github"></i></button>
<div class="dropdown-buttons sourcebuttons">
<a class="repository-button" href="https://github.com/NeuromatchAcademy/course-content-dl"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Source repository" type="button"><i class="fab fa-github"></i>repository</button></a>
<a class="issues-button" href="https://github.com/NeuromatchAcademy/course-content-dl/issues/new?title=Issue%20on%20page%20%2Ftutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.html&amp;body=Your%20issue%20content%20here."><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Open an issue" type="button"><i class="fas fa-lightbulb"></i>open issue</button></a>
</div>
</div>
<!-- Full screen (wrap in <a> to have style consistency -->
<a class="full-screen-button"><button aria-label="Fullscreen mode" class="btn btn-secondary topbarbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode" type="button"><i class="fas fa-expand"></i></button></a>
<!-- Launch buttons -->
</div>
<!-- Table of contents -->
<div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
            </div>
<nav id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 1: Modeling sequencies and encoding text
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-slides">
     Tutorial slides
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
     Setup
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-dependencies">
       Install dependencies
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
       Figure Settings
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#load-dataset-from-nltk">
       Load Dataset from
       <code class="docutils literal notranslate">
<span class="pre">
         nltk
        </span>
</code>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
       Helper functions
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
       Set random seed
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
       Set device (GPU or CPU). Execute
       <code class="docutils literal notranslate">
<span class="pre">
         set_device()
        </span>
</code>
</a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-sequences-markov-chains-hmms">
   Section 1: Sequences, Markov Chains &amp; HMMs
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-sequences-markov-processes">
     Video 1: Sequences &amp; Markov Processes
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#why-is-this-relevant-how-are-these-sequences-related-to-modern-recurrent-neural-networks">
     Why is this relevant? How are these sequences related to modern recurrent neural networks?
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-what-data-are-sequences">
     Section 1.1: What data are sequences?
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#creating-matrices-and-distinct-words">
       Creating Matrices and Distinct Words
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#populating-matric-that-tracks-next-word">
       Populating Matric that tracks next word
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-what-is-a-markov-chain-or-model">
     Section 1.2: What is a Markov Chain or Model?
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#modelling-transitions-between-states">
       Modelling transitions between states
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-most-likely-word">
         Function for most likely word
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-building-naive-chain">
         Function for building Naive Chain
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-weighted-choice">
         Function for weighted choice
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-sampling-next-word-with-weights">
         Function for sampling next word with weights
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-a-stochastic-chain-using-weighted-choice">
         Function for a stochastic chain using weighted choice
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#code-to-populate-matrix-of-sets-of-words">
         Code to populate matrix of sets of words
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-for-stochastic-chain-for-sets-of-words">
         Function for stochastic Chain for sets of words
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-sample-next-word-after-sequence">
         Function to sample next word after sequence
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-1-2-how-does-changing-parameters-the-sentences-generated">
       Think! 1.2: How does changing parameters the sentences generated?
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-3-what-is-a-hidden-markov-model">
     Section 1.3: What is a Hidden Markov Model?
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-create-default-multinomial-hmm-model">
       Function to create default Multinomial HMM model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-create-default-multinomial-hmm-model-information-of-relative-frequencies-of-words">
       Function to create default Multinomial HMM model information of relative frequencies of words
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#function-to-generate-words-given-a-hmm-model">
       Function to generate words given a hmm model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-1-3-transition-probabilities">
       Exercise 1.3: Transition probabilities
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#useful-links-for-markov-models-and-hmm">
       Useful links for Markov Models and HMM:
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-word-embeddings">
   Section 2: Word Embeddings
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-textual-dimension-reduction">
     Video 2: Textual Dimension Reduction
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-creating-word-embeddings">
     Section 2.1: Creating Word Embeddings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-visualizing-word-embedding">
     Section 2.2: Visualizing Word Embedding
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-3-exploring-meaning-with-word-embeddings">
     Section 2.3: Exploring meaning with word embeddings
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#download-fasttext-english-embeddings-of-dimension-100">
       Download FastText English Embeddings of dimension 100
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#word-similarity">
       Word Similarity
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-semantic-measurements">
         Video 3: Semantic Measurements
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#homonym-words-dagger">
       Homonym Words
       <span class="math notranslate nohighlight">
        \(^\dagger\)
       </span>
</a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#word-analogies">
       Word Analogies
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-neural-net-with-word-embeddings">
   Section 3: Neural Net with word embeddings
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-1-simple-feed-forward-net">
     Coding Exercise 3.1: Simple Feed Forward Net
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#download-embeddings-and-clear-old-variables-to-clean-memory">
       Download embeddings and clear old variables to clean memory.
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#execute-this-cell">
         Execute this cell!
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#training-and-testing-functions">
       Training and Testing Functions
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="row" id="main-content">
<div class="col-12 col-md-9 pl-md-3 pr-md-0">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<div class="section" id="tutorial-1-modeling-sequencies-and-encoding-text">
<h1>Tutorial 1: Modeling sequencies and encoding text<a class="headerlink" href="#tutorial-1-modeling-sequencies-and-encoding-text" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 2, Day 3: Modern RNNs</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Bhargav Srinivasa Desikan, Anis Zahedifard, James Evans</p>
<p><strong>Content reviewers:</strong> Lily Cheng, Melvin Selim Atay, Ezekiel Williams</p>
<p><strong>Content editors:</strong> Nina Kudryashova, Spiros Chavlis</p>
<p><strong>Production editors:</strong> Roberto Guidotti, Spiros Chavlis</p>
<p><strong>Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs</strong></p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p>Before we begin with exploring how RNNs excel at modelling sequences, we will explore some of the other ways we can model sequences, encode text, and make meaningful measurements using such encodings and embeddings.</p>
<div class="section" id="tutorial-slides">
<h2>Tutorial slides<a class="headerlink" href="#tutorial-slides" title="Permalink to this headline">¶</a></h2>
<p>These are the slides for the videos in this tutorial</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe allowfullscreen="" frameborder="0" height="480" src="https://mfr.ca-1.osf.io/render?url=https://osf.io/n263c/?direct%26mode=render%26action=download%26mode=render" width="854"></iframe>
</div></div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<div class="section" id="install-dependencies">
<h3>Install dependencies<a class="headerlink" href="#install-dependencies" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Install dependencies</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>
<span class="o">!</span>pip install <span class="nv">torchtext</span><span class="o">==</span><span class="m">0</span>.4.0 --quiet
<span class="o">!</span>pip install --upgrade gensim --quiet
<span class="o">!</span>pip install unidecode --quiet
<span class="o">!</span>pip install hmmlearn --quiet
<span class="o">!</span>pip install fasttext --quiet
<span class="o">!</span>pip install nltk --quiet
<span class="o">!</span>pip install pandas --quiet
<span class="o">!</span>pip install python-Levenshtein --quiet
<span class="n">clear_output</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 21.2.1; however, version 21.2.2 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.11/x64/bin/python -m pip install --upgrade pip' command.</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 21.2.1; however, version 21.2.2 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.11/x64/bin/python -m pip install --upgrade pip' command.</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 21.2.1; however, version 21.2.2 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.11/x64/bin/python -m pip install --upgrade pip' command.</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 21.2.1; however, version 21.2.2 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.11/x64/bin/python -m pip install --upgrade pip' command.</span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>^C
<span class="-Color -Color-Red">ERROR: Operation cancelled by user</span>
<span class="-Color -Color-Yellow">WARNING: You are using pip version 21.2.1; however, version 21.2.2 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.11/x64/bin/python -m pip install --upgrade pip' command.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">import</span> <span class="nn">fasttext</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">urllib.request</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">import</span> <span class="nn">matplotlib.cm</span> <span class="k">as</span> <span class="nn">cm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm_notebook</span> <span class="k">as</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">hmmlearn</span> <span class="kn">import</span> <span class="n">hmm</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">dok_matrix</span>

<span class="kn">from</span> <span class="nn">torchtext</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchtext.vocab</span> <span class="kn">import</span> <span class="n">FastText</span>

<span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">FreqDist</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">brown</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h3>Figure Settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure Settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="load-dataset-from-nltk">
<h3>Load Dataset from <code class="docutils literal notranslate"><span class="pre">nltk</span></code><a class="headerlink" href="#load-dataset-from-nltk" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title  Load Dataset from `nltk`</span>
<span class="c1"># no critical warnings, so we supress it</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">)</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'punkt'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'averaged_perceptron_tagger'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'brown'</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">'webtext'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions">
<h3>Helper functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper functions</span>
<span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">vec_a</span><span class="p">,</span> <span class="n">vec_b</span><span class="p">):</span>
    <span class="sd">"""Compute cosine similarity between vec_a and vec_b"""</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vec_a</span><span class="p">,</span> <span class="n">vec_b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vec_a</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vec_b</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">sentences</span><span class="p">):</span>
  <span class="c1">#Tokenize the sentence</span>
  <span class="c1">#from nltk.tokenize library use word_tokenize</span>
  <span class="n">token</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">token</span>


<span class="k">def</span> <span class="nf">plot_train_val</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">val_label</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">y_label</span><span class="p">,</span>
                   <span class="n">color</span><span class="p">):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">train_label</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">val_label</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'lower right'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'epoch'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">y_label</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="n">emb_vectors</span><span class="p">,</span> <span class="n">sentence_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">522</span><span class="p">):</span>
  <span class="n">TEXT</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">sequential</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">tokenize</span><span class="o">=</span><span class="n">tokenize</span><span class="p">,</span>
                    <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">include_lengths</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">fix_length</span><span class="o">=</span><span class="n">sentence_length</span><span class="p">)</span>
  <span class="n">LABEL</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">LabelField</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>

  <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">IMDB</span><span class="o">.</span><span class="n">splits</span><span class="p">(</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">LABEL</span><span class="p">)</span>

  <span class="n">TEXT</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">vectors</span><span class="o">=</span><span class="n">emb_vectors</span><span class="p">)</span>
  <span class="n">LABEL</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

  <span class="n">train_data</span><span class="p">,</span> <span class="n">valid_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">split_ratio</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                                            <span class="n">random_state</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">))</span>
  <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">BucketIterator</span><span class="o">.</span><span class="n">splits</span><span class="p">((</span><span class="n">train_data</span><span class="p">,</span>
                                                                  <span class="n">valid_data</span><span class="p">,</span>
                                                                  <span class="n">test_data</span><span class="p">),</span>
                                                                  <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                                                                  <span class="n">sort_key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">text</span><span class="p">),</span>
                                                                  <span class="n">repeat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                                  <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Data are loaded. sentence length: </span><span class="si">{</span><span class="n">sentence_length</span><span class="si">}</span><span class="s1"> '</span>
        <span class="sa">f</span><span class="s1">'seed: </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">TEXT</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span> <span class="n">test_iter</span>


<span class="k">def</span> <span class="nf">download_file_from_google_drive</span><span class="p">(</span><span class="nb">id</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
  <span class="n">URL</span> <span class="o">=</span> <span class="s2">"https://docs.google.com/uc?export=download"</span>

  <span class="n">session</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>

  <span class="n">response</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">URL</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span> <span class="s1">'id'</span><span class="p">:</span> <span class="nb">id</span> <span class="p">},</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">token</span> <span class="o">=</span> <span class="n">get_confirm_token</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">token</span><span class="p">:</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span> <span class="s1">'id'</span><span class="p">:</span> <span class="nb">id</span><span class="p">,</span> <span class="s1">'confirm'</span><span class="p">:</span> <span class="n">token</span> <span class="p">}</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">URL</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="n">save_response_content</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">destination</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_confirm_token</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">cookies</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">'download_warning'</span><span class="p">):</span>
      <span class="k">return</span> <span class="n">value</span>

  <span class="k">return</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">save_response_content</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
  <span class="n">CHUNK_SIZE</span> <span class="o">=</span> <span class="mi">32768</span>

  <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">iter_content</span><span class="p">(</span><span class="n">CHUNK_SIZE</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">chunk</span><span class="p">:</span> <span class="c1"># filter out keep-alive new chunks</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-random-seed">
<h3>Set random seed<a class="headerlink" href="#set-random-seed" title="Permalink to this headline">¶</a></h3>
<p>Executing <code class="docutils literal notranslate"><span class="pre">set_seed(seed=seed)</span></code> you are setting the seed</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set random seed</span>

<span class="c1"># @markdown Executing `set_seed(seed=seed)` you are setting the seed</span>

<span class="c1"># for DL its critical to set the random seed so that students can have a</span>
<span class="c1"># baseline to compare their results to expected results.</span>
<span class="c1"># Read more here: https://pytorch.org/docs/stable/notes/randomness.html</span>

<span class="c1"># Call `set_seed` function in the exercises to ensure reproducibility.</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Random seed </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1"> has been set.'</span><span class="p">)</span>

<span class="c1"># In case that `DataLoader` is used</span>
<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
  <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-device-gpu-or-cpu-execute-set-device">
<h3>Set device (GPU or CPU). Execute <code class="docutils literal notranslate"><span class="pre">set_device()</span></code><a class="headerlink" href="#set-device-gpu-or-cpu-execute-set-device" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set device (GPU or CPU). Execute `set_device()`</span>

<span class="c1"># inform the user if the notebook uses GPU or CPU.</span>

<span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: For this notebook to perform best, "</span>
        <span class="s2">"if possible, in the menu under `Runtime` -&gt; "</span>
        <span class="s2">"`Change runtime type.`  select `GPU` "</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook."</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>
<span class="n">SEED</span> <span class="o">=</span> <span class="mi">2021</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-sequences-markov-chains-hmms">
<h1>Section 1: Sequences, Markov Chains &amp; HMMs<a class="headerlink" href="#section-1-sequences-markov-chains-hmms" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-1-sequences-markov-processes">
<h2>Video 1: Sequences &amp; Markov Processes<a class="headerlink" href="#video-1-sequences-markov-processes" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>In this notebook we will be exploring the world of sequences - thinking of what kind of data can be thought of as sequences, and how these sequences can be represented as Markov Chains and Hidden Markov Models. These ideas and methods were an important part of natural language processing and language modelling, and serve as a useful way to ground ourselves before we dive into neural network methods.</p>
</div>
<div class="section" id="why-is-this-relevant-how-are-these-sequences-related-to-modern-recurrent-neural-networks">
<h2>Why is this relevant? How are these sequences related to modern recurrent neural networks?<a class="headerlink" href="#why-is-this-relevant-how-are-these-sequences-related-to-modern-recurrent-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>Like we mentioned before, the notion of modelling sequences of data - in this particular case, <strong>language</strong>, is an ideal place to start. RNNs themselves were constructed keeping in mind sequences, and the ability to temporally model sequences is what inspired RNNs (and the family of LSTM, GRUs - we will see this in the next notebook).</p>
<p>Markov models and hidden markov models serve as an introduction to these concepts because they were some of the earliest ways to think about sequences. They do not capture a lot of the complexity that RNNs excel at, but are an useful way of thinking of sequences, probabilities, and how we can use these concepts to perform  tasks such as text generation, or classification - tasks that RNNs excel at today.</p>
<p>Think of this section as an introduction to thinking with sequences and text data, and as a historical introduction to the world of modelling sequential data.</p>
</div>
<div class="section" id="section-1-1-what-data-are-sequences">
<h2>Section 1.1: What data are sequences?<a class="headerlink" href="#section-1-1-what-data-are-sequences" title="Permalink to this headline">¶</a></h2>
<p>Native Sequences:</p>
<ul class="simple">
<li><p>Temporally occurring events (e.g., history, stock prices)</p></li>
<li><p>Temporally processed events (e.g., communication)</p></li>
<li><p>Topologically connected components (e.g., polymers, peptides)</p></li>
</ul>
<p>Synthetic Sequences:</p>
<ul class="simple">
<li><p>Anything processed as a sequence (e.g., scanned pixels in an image)</p></li>
</ul>
<p>Sequences can be represented as a Markov Process - since this notion of sequential data is intrinsically linked to RNNs, it is a good place for us to start, and natural language (text!) will be our sequence of choice.</p>
<p>We will be using the Brown corpus which comes loaded with NLTK, and using the entire corpus - this requires a lot of RAM for some of the methods, so we recommend using a smaller subset of categories if you do not have enough RAM.</p>
<p>We will be using some of the code from this <a class="reference external" href="https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html">tutorial</a> and this <a class="reference external" href="https://github.com/StrikingLoo/ASOIAF-Markov/blob/master/ASOIAF.ipynb">Jupyter notebook</a></p>
<p>The first few cells of code all involve set-up; some of this code will be hidden because they are not necessary to understand the ideas of markov models, but the way data is setup can be vital to the way the model performs (something in common with neural network models!).</p>
<p>Let us start with loading our corpus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">category</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'editorial'</span><span class="p">,</span> <span class="s1">'fiction'</span><span class="p">,</span> <span class="s1">'government'</span><span class="p">,</span> <span class="s1">'news'</span><span class="p">,</span> <span class="s1">'religion'</span><span class="p">]</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">brown</span><span class="o">.</span><span class="n">sents</span><span class="p">(</span><span class="n">categories</span><span class="o">=</span><span class="n">category</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have our sentences, let us look at some statistics to get an idea of what we are dealing with.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Find the 80-th percentile: the minimal length of such a sentence, which is longer than at least 80% of sentences in the <em>Brown corpus</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lengths</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">.8</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lengths</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>This gives us an idea of what our dataset looks like, along with some average lengths. This kind of quick data exploration can be very useful - we know how long different sequences are, and how we might want to collect these words.</p>
<p>Since we will be modelling words as sequences in sentences, let us first collect all the words in our corpus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_words</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
    <span class="k">if</span> <span class="s2">"''"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word</span> <span class="ow">and</span> <span class="s2">"``"</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word</span><span class="p">:</span>
      <span class="n">corpus_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Corpus length: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_words</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll now get distinct (unique) words and create a matrix to represent all these words. This is necessary because we will be using this matrix to look at the probability of the words in sequences.</p>
<div class="section" id="creating-matrices-and-distinct-words">
<h3>Creating Matrices and Distinct Words<a class="headerlink" href="#creating-matrices-and-distinct-words" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Creating Matrices and Distinct Words</span>
<span class="n">distinct_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">))</span>
<span class="n">word_idx_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">distinct_words</span><span class="p">)}</span>
<span class="n">distinct_words_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">)))</span>
<span class="n">next_word_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">distinct_words_count</span><span class="p">,</span> <span class="n">distinct_words_count</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Number of distinct words: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">distinct_words_count</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>In the following lines of code we are populating the matrix that tracks the next word in a sentence.</p>
</div>
<div class="section" id="populating-matric-that-tracks-next-word">
<h3>Populating Matric that tracks next word<a class="headerlink" href="#populating-matric-that-tracks-next-word" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Populating Matric that tracks next word</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
  <span class="n">first_word_idx</span> <span class="o">=</span> <span class="n">word_idx_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
  <span class="n">next_word_idx</span> <span class="o">=</span> <span class="n">word_idx_dict</span><span class="p">[</span><span class="n">corpus_words</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span>
  <span class="n">next_word_matrix</span><span class="p">[</span><span class="n">first_word_idx</span><span class="p">][</span><span class="n">next_word_idx</span><span class="p">]</span> <span class="o">+=</span><span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>Now we have the information ready to construct a markov chain. The next word matrix is crucial in this, as it allows us to go from one word in the sequence to the next. We will soon see how this is used.</p>
</div>
</div>
<div class="section" id="section-1-2-what-is-a-markov-chain-or-model">
<h2>Section 1.2: What is a Markov Chain or Model?<a class="headerlink" href="#section-1-2-what-is-a-markov-chain-or-model" title="Permalink to this headline">¶</a></h2>
<p>A Markov Chain (or Model) is a:</p>
<ul class="simple">
<li><p>stochastic model describing a sequence of possible events</p></li>
<li><p>the probability of each event depends only on the state attained in the previous event.</p></li>
<li><p>a countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC) [vs. a continuous-time process or CTMC].</p></li>
<li><p>The classic formal language model is a Markov Model</p></li>
</ul>
<p><em>Helpful explanations from <a class="reference external" href="https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/#non-autoregressive-homoskedastic-emissions">eric mjl’s tutorial</a></em>!</p>
<p>The simplest Markov models assume that we have a <em>system</em> that contains a finite set of states,
and that the <em>system</em> transitions between these states with some probability at each time step <span class="math notranslate nohighlight">\(t\)</span>,
thus generating a sequence of states over time.
Let’s call these states <span class="math notranslate nohighlight">\(S\)</span>, where</p>
<div class="amsmath math notranslate nohighlight" id="equation-c8abd8f5-cd22-4162-8432-4c00ba5cd062">
<span class="eqno">(60)<a class="headerlink" href="#equation-c8abd8f5-cd22-4162-8432-4c00ba5cd062" title="Permalink to this equation">¶</a></span>\[\begin{equation}
S = \{s_1, s_2, ..., s_n\}
\end{equation}\]</div>
<p>To keep things simple, let’s start with three states:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7acffa39-07d4-4482-820f-6ca5f263bc3c">
<span class="eqno">(61)<a class="headerlink" href="#equation-7acffa39-07d4-4482-820f-6ca5f263bc3c" title="Permalink to this equation">¶</a></span>\[\begin{equation}
S = \{s_1, s_2, s_3\}
\end{equation}\]</div>
<p>A Markov model generates a sequence of states, with one possible realization being:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1fb74b28-b46c-4ab3-b89f-719e11037df9">
<span class="eqno">(62)<a class="headerlink" href="#equation-1fb74b28-b46c-4ab3-b89f-719e11037df9" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\}
\end{equation}\]</div>
<p>And generically, we represent it as a sequence of states <span class="math notranslate nohighlight">\(x_t, x_{t+1}... x_{t+n}\)</span>. (We have chosen a different symbol to not confuse the “generic” state with the specific realization. Graphically, a plain and simple Markov model looks like the following:</p>
<center><img src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/cell_chain.png" width="500"/></center><div class="section" id="modelling-transitions-between-states">
<h3>Modelling transitions between states<a class="headerlink" href="#modelling-transitions-between-states" title="Permalink to this headline">¶</a></h3>
<p>To know how a system transitions between states, we now need a <strong>transition matrix</strong>.</p>
<p>The transition matrix describes the probability of transitioning from one state to another (The probability of staying in the same state is semantically equivalent to transitioning to the same state).</p>
<p>By convention, transition matrix rows correspond to the state at time <span class="math notranslate nohighlight">\(t\)</span>,
while columns correspond to state at time <span class="math notranslate nohighlight">\(t+1\)</span>.
Hence, row probabilities sum to one, because the probability of transitioning to the next state depends on only the current state, and all possible states are known and enumerated.</p>
<p>Let’s call the transition matrix <span class="math notranslate nohighlight">\(P_{transition}\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4ad89fbe-4e66-4429-bdd1-b2348fddf840">
<span class="eqno">(63)<a class="headerlink" href="#equation-4ad89fbe-4e66-4429-bdd1-b2348fddf840" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P_{transition} = 
  \begin{pmatrix}
  p_{11} &amp; p_{12} &amp; p_{13} \\
  p_{21} &amp; p_{22} &amp; p_{23} \\
  p_{31} &amp; p_{32} &amp; p_{33} \\
  \end{pmatrix}
\end{equation}\]</div>
<p>Using the transition matrix, we can express different behaviors of the system. For example:</p>
<ol class="simple">
<li><p>by assigning larger probability mass to the diagonals, we can express that the system likes to stay in the current state;</p></li>
<li><p>by assigning larger probability mass to the off-diagonal, we can express that the system likes to transition out of its current state.</p></li>
</ol>
<p>In our case, this matrix is created by measuring how often one word appeared after another.</p>
<div class="section" id="function-for-most-likely-word">
<h4>Function for most likely word<a class="headerlink" href="#function-for-most-likely-word" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for most likely word</span>
<span class="k">def</span> <span class="nf">most_likely_word_after</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
  <span class="c1"># we check for the word most likely to occur using the matrix</span>
  <span class="n">most_likely</span> <span class="o">=</span> <span class="n">next_word_matrix</span><span class="p">[</span><span class="n">word_idx_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">distinct_words</span><span class="p">[</span><span class="n">most_likely</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Using our most likely word function, we can begin to create chains of words and create sequences. In the code below we create a naive chain that simply choses the most likely word.</p>
</div>
<div class="section" id="function-for-building-naive-chain">
<h4>Function for building Naive Chain<a class="headerlink" href="#function-for-building-naive-chain" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for building Naive Chain</span>
<span class="k">def</span> <span class="nf">naive_chain</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">15</span><span class="p">):</span>
  <span class="n">current_word</span> <span class="o">=</span> <span class="n">word</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">word</span>
  <span class="c1"># we now build a naive chain by picking up the most likely word</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="s1">' '</span>
    <span class="n">next_word</span> <span class="o">=</span> <span class="n">most_likely_word_after</span><span class="p">(</span><span class="n">current_word</span><span class="p">)</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="n">next_word</span>
    <span class="n">current_word</span> <span class="o">=</span> <span class="n">next_word</span>
  <span class="k">return</span> <span class="n">sentence</span>
</pre></div>
</div>
</div>
</div>
<p>Let us now use this naive chain to see what comes up, using some simple words.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'the'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'I'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'What'</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">naive_chain</span><span class="p">(</span><span class="s1">'park'</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We notice that after the word <code class="docutils literal notranslate"><span class="pre">the</span></code>, <code class="docutils literal notranslate"><span class="pre">United</span> <span class="pre">States</span></code> comes up each time. All the other sequencies starting from other words also end up at <code class="docutils literal notranslate"><span class="pre">the</span></code> quite often. Since we use a <em>deterministic</em> markov chain model, its next state only depends on the previous one. Therefore, once the sequence comes to <code class="docutils literal notranslate"><span class="pre">the</span></code>, it inevitably continues the sequence with the <code class="docutils literal notranslate"><span class="pre">United</span> <span class="pre">States</span></code>.</p>
<p>We can now be a little more sophisticated, and return words in a sequence using a <em>weighted choice</em>, which randomly selects the next word from a set of words with some probability (weight).</p>
</div>
<div class="section" id="function-for-weighted-choice">
<h4>Function for weighted choice<a class="headerlink" href="#function-for-weighted-choice" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for weighted choice</span>
<span class="k">def</span> <span class="nf">weighted_choice</span><span class="p">(</span><span class="n">objects</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  Returns randomly an element from the sequence of 'objects',</span>
<span class="sd">      the likelihood of the objects is weighted according</span>
<span class="sd">      to the sequence of 'weights', i.e. percentages.</span>
<span class="sd">  """</span>

  <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
  <span class="n">sum_of_weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
  <span class="c1"># standardization:</span>
  <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">sum_of_weights</span><span class="p">)</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
      <span class="k">return</span> <span class="n">objects</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="function-for-sampling-next-word-with-weights">
<h4>Function for sampling next word with weights<a class="headerlink" href="#function-for-sampling-next-word-with-weights" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for sampling next word with weights</span>
<span class="k">def</span> <span class="nf">sample_next_word_after</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="n">next_word_vector</span> <span class="o">=</span> <span class="n">next_word_matrix</span><span class="p">[</span><span class="n">word_idx_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">+</span> <span class="n">alpha</span>
  <span class="n">likelihoods</span> <span class="o">=</span> <span class="n">next_word_vector</span><span class="o">/</span><span class="n">next_word_vector</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">weighted_choice</span><span class="p">(</span><span class="n">distinct_words</span><span class="p">,</span> <span class="n">likelihoods</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_next_word_after</span><span class="p">(</span><span class="s1">'The'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sample_next_word_after</span><span class="p">(</span><span class="s1">'The'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>There! We don’t see the same word twice, because of the added randomisation (i.e., stochasticity). Our algorithm calculates how likely it is to find a certain word after a given word (<code class="docutils literal notranslate"><span class="pre">The</span></code> in this case) in the corpus, and then generates 1 sample of the next word with a matching probability.</p>
<p>In this example, we generated only one next word. Now, using this function, we’ll build a chain.</p>
</div>
<div class="section" id="function-for-a-stochastic-chain-using-weighted-choice">
<h4>Function for a stochastic chain using weighted choice<a class="headerlink" href="#function-for-a-stochastic-chain-using-weighted-choice" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for a stochastic chain using weighted choice</span>
<span class="k">def</span> <span class="nf">stochastic_chain</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">15</span><span class="p">):</span>
  <span class="n">current_word</span> <span class="o">=</span> <span class="n">word</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">word</span>

  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="s1">' '</span>
    <span class="n">next_word</span> <span class="o">=</span> <span class="n">sample_next_word_after</span><span class="p">(</span><span class="n">current_word</span><span class="p">)</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="n">next_word</span>
    <span class="n">current_word</span> <span class="o">=</span> <span class="n">next_word</span>

  <span class="k">return</span> <span class="n">sentence</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_chain</span><span class="p">(</span><span class="s1">'Hospital'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Neat - we can create stochastic chains for a single word. For a more effective language model, we would want to model sets of words - in the following cells, we create sets of words to predict a chain after a sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sequences_matrices</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
  <span class="c1"># @title Code to build sets of words for more realistic sequences</span>
  <span class="n">sets_of_k_words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">corpus_words</span><span class="p">[:</span><span class="o">-</span><span class="n">k</span><span class="p">])]</span>
  <span class="n">sets_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sets_of_k_words</span><span class="p">)))</span>
  <span class="n">next_after_k_words_matrix</span> <span class="o">=</span> <span class="n">dok_matrix</span><span class="p">((</span><span class="n">sets_count</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">distinct_words</span><span class="p">)))</span>
  <span class="n">distinct_sets_of_k_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sets_of_k_words</span><span class="p">))</span>
  <span class="n">k_words_idx_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">distinct_sets_of_k_words</span><span class="p">)}</span>
  <span class="n">distinct_k_words_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sets_of_k_words</span><span class="p">)))</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">sets_of_k_words</span><span class="p">[:</span><span class="o">-</span><span class="n">k</span><span class="p">])):</span>
    <span class="n">word_sequence_idx</span> <span class="o">=</span> <span class="n">k_words_idx_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
    <span class="n">next_word_idx</span> <span class="o">=</span> <span class="n">word_idx_dict</span><span class="p">[</span><span class="n">corpus_words</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">k</span><span class="p">]]</span>
    <span class="n">next_after_k_words_matrix</span><span class="p">[</span><span class="n">word_sequence_idx</span><span class="p">,</span> <span class="n">next_word_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">k_words_idx_dict</span><span class="p">,</span><span class="n">distinct_sets_of_k_words</span><span class="p">,</span><span class="n">next_after_k_words_matrix</span>

<span class="n">k_words_idx_dict</span><span class="p">,</span> <span class="n">distinct_sets_of_k_words</span><span class="p">,</span> <span class="n">next_after_k_words_matrix</span> <span class="o">=</span> <span class="n">sequences_matrices</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s have a look at what that bit of code did.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">distinct_sets_of_k_words</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Great! Now we are going to create a transition matrix for the sets of words.</p>
</div>
<div class="section" id="code-to-populate-matrix-of-sets-of-words">
<h4>Code to populate matrix of sets of words<a class="headerlink" href="#code-to-populate-matrix-of-sets-of-words" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Code to populate matrix of sets of words</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">distinct_sets_of_k_words</span><span class="p">[:</span><span class="o">-</span><span class="n">k</span><span class="p">])):</span>
  <span class="n">word_sequence_idx</span> <span class="o">=</span> <span class="n">k_words_idx_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
  <span class="n">next_word_idx</span> <span class="o">=</span> <span class="n">word_idx_dict</span><span class="p">[</span><span class="n">corpus_words</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">k</span><span class="p">]]</span>
  <span class="n">next_after_k_words_matrix</span><span class="p">[</span><span class="n">word_sequence_idx</span><span class="p">,</span> <span class="n">next_word_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>We now have what we need to build a stochastic chain over a <code class="docutils literal notranslate"><span class="pre">K</span></code> set of words.</p>
</div>
<div class="section" id="function-for-stochastic-chain-for-sets-of-words">
<h4>Function for stochastic Chain for sets of words<a class="headerlink" href="#function-for-stochastic-chain-for-sets-of-words" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function for stochastic Chain for sets of words</span>
<span class="k">def</span> <span class="nf">stochastic_chain_sequence</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">chain_length</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
  <span class="n">current_words</span> <span class="o">=</span> <span class="n">words</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">current_words</span><span class="p">)</span> <span class="o">!=</span> <span class="n">k</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">'wrong number of words, expected </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">words</span>

  <span class="c1"># pre-calculate seq embedding + transition matrix for a given k</span>
  <span class="n">matrices</span> <span class="o">=</span> <span class="n">sequences_matrices</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">chain_length</span><span class="p">):</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="s1">' '</span>
    <span class="n">next_word</span> <span class="o">=</span> <span class="n">sample_next_word_after_sequence</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span><span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">current_words</span><span class="p">))</span>
    <span class="n">sentence</span> <span class="o">+=</span> <span class="n">next_word</span>
    <span class="n">current_words</span> <span class="o">=</span> <span class="n">current_words</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="p">[</span><span class="n">next_word</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">sentence</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="function-to-sample-next-word-after-sequence">
<h4>Function to sample next word after sequence<a class="headerlink" href="#function-to-sample-next-word-after-sequence" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function to sample next word after sequence</span>
<span class="k">def</span> <span class="nf">sample_next_word_after_sequence</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span> <span class="n">word_sequence</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
  <span class="c1"># unpack a tuple of matrices</span>
  <span class="n">k_words_idx_dict</span><span class="p">,</span><span class="n">distinct_sets_of_k_words</span><span class="p">,</span> <span class="n">next_after_k_words_matrix</span> <span class="o">=</span> <span class="n">matrices</span>

  <span class="n">next_word_vector</span> <span class="o">=</span> <span class="n">next_after_k_words_matrix</span><span class="p">[</span><span class="n">k_words_idx_dict</span><span class="p">[</span><span class="n">word_sequence</span><span class="p">]]</span> <span class="o">+</span> <span class="n">alpha</span>
  <span class="n">likelihoods</span> <span class="o">=</span> <span class="n">next_word_vector</span><span class="o">/</span><span class="n">next_word_vector</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">weighted_choice</span><span class="p">(</span><span class="n">distinct_words</span><span class="p">,</span> <span class="n">likelihoods</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_chain_sequence</span><span class="p">(</span><span class="s1">'Judges under the'</span><span class="p">,</span> <span class="n">chain_length</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Great! This sentence was created using two of the techniques we recently saw - creating sets of words, and using a weighted average stochastic chain. Both of these methods contributed in making it a more meaningful sequence of words. Some of these notions are also captured by Recurrent Neural Networks!</p>
</div>
</div>
<div class="section" id="think-1-2-how-does-changing-parameters-the-sentences-generated">
<h3>Think! 1.2: How does changing parameters the sentences generated?<a class="headerlink" href="#think-1-2-how-does-changing-parameters-the-sentences-generated" title="Permalink to this headline">¶</a></h3>
<p>Try and use a set of words but using a naive chain, and try a stochastic chain with a low value of k (i.e., 2), and a higher value (i.e., 5). How do these different configurations change the quality of the sequences produced? Below you have sample code to try these out.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">stochastic_chain_sequence</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">chain_length</span><span class="o">=...</span><span class="p">,</span> <span class="n">k</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<p>You should be able to use these matrices and the previous functions to be able to create the necessary configurations.</p>
</div>
</div>
<div class="section" id="section-1-3-what-is-a-hidden-markov-model">
<h2>Section 1.3: What is a Hidden Markov Model?<a class="headerlink" href="#section-1-3-what-is-a-hidden-markov-model" title="Permalink to this headline">¶</a></h2>
<p>A 1960s advance (by Leonard Baum and colleagues): Hidden Markov Models are:</p>
<ul class="simple">
<li><p>a Markov model in which the system modeled is assumed to be a Markov process/chain with unobservable (“hidden”) states.</p></li>
<li><p>HMM assumes there is another surrogate process whose behavior “depends” on the state–you learn about the state by observing the surrogate process.</p></li>
<li><p>HMMs have successfully been applied in fields where the goal is to recover a data sequence not immediately observable (but other data that depend on the sequence are).</p></li>
<li><p>The first dominant application: Speech and text processing (1970s)</p></li>
</ul>
<p>In this sub-section we will use the python library <a class="reference external" href="https://hmmlearn.readthedocs.io/en/latest/tutorial.html#training-hmm-parameters-and-inferring-the-hidden-states">hmmlearn</a>, which is part of the <em>scikit-learn</em> ecosystem. <a class="reference external" href="https://github.com/mfilej/nlg-with-hmmlearn">nlg-with-hmmlearn</a> offers useful code snippets to adapt <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> for text data. Because we are using a package that offers many out of the box implementations for HMMs, we don’t have to worry about the states, transition matrices,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load the data</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">brown</span><span class="o">.</span><span class="n">sents</span><span class="p">(</span><span class="n">categories</span><span class="o">=</span><span class="n">category</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
<span class="n">alphabet</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="c1"># Encode words</span>
<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">alphabet</span><span class="p">))</span>

<span class="c1"># Find word freqeuncies</span>
<span class="n">seq</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromiter</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">features</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">fd</span> <span class="o">=</span> <span class="n">FreqDist</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have our data setup, we can create our model. We use a multinomial HMM with 8 states, and can either do a random initialisation or use word frequences. We recommend trying both options!</p>
<div class="section" id="function-to-create-default-multinomial-hmm-model">
<h3>Function to create default Multinomial HMM model<a class="headerlink" href="#function-to-create-default-multinomial-hmm-model" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function to create default Multinomial HMM model</span>
<span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">num_states</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Initial parameter estimation using built-in method"</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">MultinomialHMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">num_states</span><span class="p">,</span> <span class="n">init_params</span><span class="o">=</span><span class="s1">'ste'</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="function-to-create-default-multinomial-hmm-model-information-of-relative-frequencies-of-words">
<h3>Function to create default Multinomial HMM model information of relative frequencies of words<a class="headerlink" href="#function-to-create-default-multinomial-hmm-model-information-of-relative-frequencies-of-words" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function to create default Multinomial HMM model information of relative frequencies of words</span>
<span class="k">def</span> <span class="nf">frequencies</span><span class="p">(</span><span class="n">num_states</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Initial parameter estimation using relative frequencies"</span><span class="p">)</span>

  <span class="n">frequencies</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromiter</span><span class="p">((</span><span class="n">fd</span><span class="o">.</span><span class="n">freq</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">alphabet</span><span class="p">))),</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
  <span class="n">emission_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">frequencies</span><span class="p">]</span><span class="o">*</span><span class="n">num_states</span><span class="p">)</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">hmm</span><span class="o">.</span><span class="n">MultinomialHMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">num_states</span><span class="p">,</span> <span class="n">init_params</span><span class="o">=</span><span class="s1">'st'</span><span class="p">)</span>
  <span class="n">model</span><span class="o">.</span><span class="n">emissionprob_</span> <span class="o">=</span> <span class="n">emission_prob</span>
  <span class="k">return</span> <span class="n">model</span>


<span class="nb">print</span><span class="p">(</span><span class="n">frequencies</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Note</strong>:</p>
<p>The following lines of code are commented out because they take a long time (~17 mins for default Brown corpus categories).</p>
<p>If you do not have that time, you can download the default model to try to generate text. You have to uncomment the appropriate lines.</p>
<p><strong>Note:</strong> Either you may want to uncomment Line 11 or Line 14, not both, as the output variable <code class="docutils literal notranslate"><span class="pre">model</span></code> will be overwritten.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Fitting a default multinomial HMM. This is lengthy (~17 mins)</span>
<span class="k">def</span> <span class="nf">run_model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">num_states</span><span class="p">):</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="n">num_states</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">model</span>


<span class="n">num_states</span> <span class="o">=</span> <span class="mi">8</span>
<span class="c1">## Uncomment, if you have time!</span>
<span class="c1"># model = run_model(features, lengths, num_states)</span>

<span class="c1">## Another way to get a model is to use default frequencies when initialising the model</span>
<span class="c1"># model = frequencies(num_states)</span>
</pre></div>
</div>
</div>
</div>
<p>Alternatively, you could use a saved model. Here is a <a class="reference external" href="https://drive.google.com/file/d/1IymcmcO48V6q3x-6dhf7-OU5NByo5W2F/view?usp=sharing">link</a> to the default model, which you can download and then upload into Colab.</p>
<p>Execute this cell to download the saved model.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute this cell to download the saved model.</span>
<span class="c1">## code to load the saved model</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">"https://osf.io/5k6cs/download"</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="s1">'model_w2d3_t1.pkl'</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">"model_w2d3_t1.pkl"</span><span class="p">,</span> <span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="function-to-generate-words-given-a-hmm-model">
<h3>Function to generate words given a hmm model<a class="headerlink" href="#function-to-generate-words-given-a-hmm-model" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Function to generate words given a hmm model</span>
<span class="k">def</span> <span class="nf">generate_text</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_lines</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">random_len</span><span class="o">=</span><span class="mi">15</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_lines</span><span class="p">):</span>
    <span class="n">set_seed</span><span class="p">(</span><span class="n">_i</span><span class="p">)</span>
    <span class="n">symbols</span><span class="p">,</span> <span class="n">_states</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">random_len</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">symbols</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">" "</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generate_text</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_lines</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_len</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We see that a hidden markov model also does well in generating text. We encourage you to try out different initialisations and hyperparameters to see how the model does.</p>
</div>
<div class="section" id="exercise-1-3-transition-probabilities">
<h3>Exercise 1.3: Transition probabilities<a class="headerlink" href="#exercise-1-3-transition-probabilities" title="Permalink to this headline">¶</a></h3>
<p>We have seen how we can use sequences of text to form probability chains, as well as how we can use out of the box models to generate text. In this exercise, you will be using your own data to generate sequences using <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> or any other implementation of a markov model. Explore the transition probabilities in your corpus and generate sentences. For example, one such exploration can be - how does using a model with the word frequencies incorporated in compare to using a default model?</p>
<p>Perform any one such comparison or exploration, and generate 3 sentences or 50 words using your model. You should be able to use all the existing functions defined for this exercise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load your own dataset and create a model using the frequencies based HMM model!</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="useful-links-for-markov-models-and-hmm">
<h3>Useful links for Markov Models and HMM:<a class="headerlink" href="#useful-links-for-markov-models-and-hmm" title="Permalink to this headline">¶</a></h3>
<p>Here are some useful links if you wish to explore this topic further.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6">Markov Chain Text</a></p></li>
<li><p><a class="reference external" href="https://python.quantecon.org/finite_markov.html">Python QuantEcon: Finite Markov Chains with Finance</a></p></li>
<li><p><a class="reference external" href="https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/">Markov Models from the ground up, with python</a></p></li>
<li><p><a class="reference external" href="https://github.com/nareshkumar66675/GenTex">GenTex</a></p></li>
<li><p><a class="reference external" href="https://hmmlearn.readthedocs.io/en/latest/tutorial.html">HMM learn</a></p></li>
</ul>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-word-embeddings">
<h1>Section 2: Word Embeddings<a class="headerlink" href="#section-2-word-embeddings" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-2-textual-dimension-reduction">
<h2>Video 2: Textual Dimension Reduction<a class="headerlink" href="#video-2-textual-dimension-reduction" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>Words or subword units such as morphemes are the basic units that we use to express meaning  in language. The technique of mapping words to vectors of real numbers is known as word embedding.</p>
<p>Word2vec is based on theories of distributional semantics - words that appear around each other are more likely to mean similar things than words that do not appear around each other. Keeping this in mind, our job is to create a high dimensional space where these semantic relations are preserved. The innovation in word2vec is the realisation that we can use unlabelled, running text in sentences as inputs for a supervised learning algorithm–as a self-supervision task. It is supervised because we use the words in a sentence to serve as positive and negative examples. Let’s break this down:</p>
<p>… “use the kitchen knife to chop the vegetables”…</p>
<p><strong>C1   C2   C3   T   C4   C5   C6   C7</strong></p>
<p>Here, the target word is knife, and the context words are the ones in its immediate (6-word) window.
The first word2vec method we’ll see is called skipgram, where the task is to assign a probability for how likely it is that the context window appears around the target word. In the training process, positive examples are samples of words and their context words, and negative examples are created by sampling from pairs of words that do not appear nearby one another.</p>
<p>This method of implementing word2vec is called skipgram with negative sampling. So while the algorithm tries to better learn which context words are likely to appear around a target word, it ends up pushing the embedded representations for every word so that they are located optimally (e.g., with minimal semantic distortion). In this process of adjusting embedding values, the algorithm brings semantically similar words close together in the resulting high dimensional space, and dissimilar words far away.</p>
<p>Another word2vec training method, Continuous Bag of Words (CBOW), works in a similar fashion, and tries to predict the target word, given context. This is converse of skipgram, which tries to predict the context, given the target word. Skip-gram represents rare words and phrases well, often requiring more data for stable representations, while CBOW is several times faster to train than the skip-gram, but with slightly better accuracy for the frequent words in its prediction task. The popular gensim implementation of word2vec has both the methods included.</p>
</div>
<div class="section" id="section-2-1-creating-word-embeddings">
<h2>Section 2.1: Creating Word Embeddings<a class="headerlink" href="#section-2-1-creating-word-embeddings" title="Permalink to this headline">¶</a></h2>
<p>We will create embeddings for a subset of categories in <a class="reference external" href="https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html">Brown corpus</a>.  In order to achieve this task we will use <a class="reference external" href="https://radimrehurek.com/gensim/">gensim</a> library to create word2vec embeddings. Gensim’s word2vec expects a sequence of sentences as its input. Each sentence is a list of words.
Calling <code class="docutils literal notranslate"><span class="pre">Word2Vec(sentences,</span> <span class="pre">iter=1)</span></code> will run two passes over the sentences iterator (or, in general iter+1 passes). The first pass collects words and their frequencies to build an internal dictionary tree structure. The second and subsequent passes train the neural model.
<code class="docutils literal notranslate"><span class="pre">Word2vec</span></code> accepts several parameters that affect both training speed and quality.</p>
<p>One of them is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them:</p>
<p><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Word2Vec(sentences,</span> <span class="pre">min_count=10)</span>  <span class="pre">#</span> <span class="pre">default</span> <span class="pre">value</span> <span class="pre">is</span> <span class="pre">5</span></code></p>
<p>A reasonable value for min_count is between 0-100, depending on the size of your dataset.</p>
<p>Another parameter is the size of the NN layers, which correspond to the “degrees” of freedom the training algorithm has:</p>
<p><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Word2Vec(sentences,</span> <span class="pre">size=200)</span>  <span class="pre">#</span> <span class="pre">default</span> <span class="pre">value</span> <span class="pre">is</span> <span class="pre">100</span></code></p>
<p>Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds.</p>
<p>The last of the major parameters (full list <a class="reference external" href="https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec">here</a>) is for training parallelization, to speed up training:</p>
<p><code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">=</span> <span class="pre">Word2Vec(sentences,</span> <span class="pre">workers=4)</span> <span class="pre">#</span> <span class="pre">default</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">worker</span> <span class="pre">=</span> <span class="pre">no</span> <span class="pre">parallelization</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">category</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'editorial'</span><span class="p">,</span> <span class="s1">'fiction'</span><span class="p">,</span> <span class="s1">'government'</span><span class="p">,</span> <span class="s1">'mystery'</span><span class="p">,</span> <span class="s1">'news'</span><span class="p">,</span> <span class="s1">'religion'</span><span class="p">,</span>
            <span class="s1">'reviews'</span><span class="p">,</span> <span class="s1">'romance'</span><span class="p">,</span> <span class="s1">'science_fiction'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_word2vec_model</span><span class="p">(</span><span class="n">category</span><span class="o">=</span><span class="s1">'news'</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="n">brown</span><span class="o">.</span><span class="n">sents</span><span class="p">(</span><span class="n">categories</span><span class="o">=</span><span class="n">category</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="n">sg</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">min_count</span><span class="p">)</span>

  <span class="k">except</span> <span class="p">(</span><span class="ne">AttributeError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s1">'Input variable "category" should be a string or list,'</span>
      <span class="s1">'"size", "sg", "min_count" should be integers'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">model_dictionary</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
  <span class="n">words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">words</span>

<span class="k">def</span> <span class="nf">get_embedding</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">all_categories</span> <span class="o">=</span> <span class="n">brown</span><span class="o">.</span><span class="n">categories</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">all_categories</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w2vmodel</span> <span class="o">=</span> <span class="n">create_word2vec_model</span><span class="p">(</span><span class="n">all_categories</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model_dictionary</span><span class="p">(</span><span class="n">w2vmodel</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">get_embedding</span><span class="p">(</span><span class="s1">'weather'</span><span class="p">,</span> <span class="n">w2vmodel</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="section-2-2-visualizing-word-embedding">
<h2>Section 2.2: Visualizing Word Embedding<a class="headerlink" href="#section-2-2-visualizing-word-embedding" title="Permalink to this headline">¶</a></h2>
<p>We can now obtain the word embeddings for any word in the dictionary using word2vec. Let’s visualize these embeddings to get an inuition of what these embeddings mean. The word embeddings obtained from word2vec model are in high dimensional space. We will use <code class="docutils literal notranslate"><span class="pre">tSNE</span></code> (t-distributed stochastic neighbor embedding), a statistical method for dimensionality deduction that allow us to visualize high-dimensional data in a 2D or 3D space. Here, we will use <code class="docutils literal notranslate"><span class="pre">tSNE</span></code> from [<code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>] module(https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) (if you are not familiar with this method, think about <code class="docutils literal notranslate"><span class="pre">PCA</span></code>) to project our high dimensional embeddings in the 2D space.</p>
<p>For each word in <code class="docutils literal notranslate"><span class="pre">keys</span></code>, we pick the top 10 similar words (using cosine similarity) and plot them.</p>
<p>What should be the arrangement of similar words?
What should be arrangement of the key clusters with respect to each other?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">keys</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'voters'</span><span class="p">,</span> <span class="s1">'magic'</span><span class="p">,</span> <span class="s1">'love'</span><span class="p">,</span> <span class="s1">'God'</span><span class="p">,</span> <span class="s1">'evidence'</span><span class="p">,</span> <span class="s1">'administration'</span><span class="p">,</span> <span class="s1">'governments'</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_cluster_embeddings</span><span class="p">(</span><span class="n">keys</span><span class="p">):</span>
  <span class="n">embedding_clusters</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">word_clusters</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># find closest words and add them to cluster</span>
  <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">:</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">w2vmodel</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">'The word '</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="s1">'is not in the dictionary'</span><span class="p">)</span>
      <span class="k">continue</span>

    <span class="k">for</span> <span class="n">similar_word</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">w2vmodel</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
      <span class="n">words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">similar_word</span><span class="p">)</span>
      <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w2vmodel</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">similar_word</span><span class="p">])</span>
    <span class="n">embedding_clusters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="n">word_clusters</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

  <span class="c1"># get embeddings for the words in clusers</span>
  <span class="n">embedding_clusters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embedding_clusters</span><span class="p">)</span>
  <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">embedding_clusters</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">tsne_model_en_2d</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">'pca'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">3500</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
  <span class="n">embeddings_en_2d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">tsne_model_en_2d</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embedding_clusters</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">)))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">embeddings_en_2d</span><span class="p">,</span> <span class="n">word_clusters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tsne_plot_similar_words</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">embedding_clusters</span><span class="p">,</span>
                            <span class="n">word_clusters</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
  <span class="n">colors</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)))</span>
  <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">embedding_clusters</span><span class="p">,</span> <span class="n">word_clusters</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">word</span><span class="p">,</span>
                   <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                   <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span>
                   <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                   <span class="n">textcoords</span><span class="o">=</span><span class="s1">'offset points'</span><span class="p">,</span>
                   <span class="n">ha</span><span class="o">=</span><span class="s1">'right'</span><span class="p">,</span>
                   <span class="n">va</span><span class="o">=</span><span class="s1">'bottom'</span><span class="p">,</span>
                   <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">"lower left"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">filename</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">'png'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="s1">'tight'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">embeddings_en_2d</span><span class="p">,</span> <span class="n">word_clusters</span> <span class="o">=</span> <span class="n">get_cluster_embeddings</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
<span class="n">tsne_plot_similar_words</span><span class="p">(</span><span class="s1">'Similar words from Brown Corpus'</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">embeddings_en_2d</span><span class="p">,</span> <span class="n">word_clusters</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="section-2-3-exploring-meaning-with-word-embeddings">
<h2>Section 2.3: Exploring meaning with word embeddings<a class="headerlink" href="#section-2-3-exploring-meaning-with-word-embeddings" title="Permalink to this headline">¶</a></h2>
<p>While word2vec was the method that started it all, research has since boomed, and we now have more sophisticated ways to represent words. One such method is FastText, developed at Facebook AI research, which breaks words into sub-words: such a technique also allows us to create embedding representations for unseen words. In this section, we will explore how semantics and meaning are captured using embedidngs, after downloading a pre-trained FastText model. Downloading pre-trained models is a way for us to plug in word embeddings and explore them without training them ourselves.</p>
<div class="section" id="download-fasttext-english-embeddings-of-dimension-100">
<h3>Download FastText English Embeddings of dimension 100<a class="headerlink" href="#download-fasttext-english-embeddings-of-dimension-100" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Download FastText English Embeddings of dimension 100</span>

<span class="n">fname</span> <span class="o">=</span> <span class="s1">'cc.en.100.bin.zip'</span>
<span class="n">download_str</span> <span class="o">=</span> <span class="s2">"Downloading"</span>
<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
  <span class="n">download_str</span> <span class="o">=</span> <span class="s2">"Redownloading"</span>
  <span class="o">!</span>rm -rf <span class="nv">$fname</span>

<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>
<span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">urlopen</span>
<span class="kn">from</span> <span class="nn">zipfile</span> <span class="kn">import</span> <span class="n">ZipFile</span>

<span class="n">zipurl</span> <span class="o">=</span> <span class="s1">'https://osf.io/w9sr7/download'</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">download_str</span><span class="si">}</span><span class="s2"> and unzipping the file... Please wait."</span><span class="p">)</span>
<span class="k">with</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">zipurl</span><span class="p">)</span> <span class="k">as</span> <span class="n">zipresp</span><span class="p">:</span>
  <span class="k">with</span> <span class="n">ZipFile</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">zipresp</span><span class="o">.</span><span class="n">read</span><span class="p">()))</span> <span class="k">as</span> <span class="n">zfile</span><span class="p">:</span>
    <span class="n">zfile</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="s1">'.'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Download completed!"</span><span class="p">)</span>

<span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="s1">'.'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load 100 dimension FastText Vectors using FastText library</span>
<span class="n">ft_en_vectors</span> <span class="o">=</span> <span class="n">fasttext</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s1">'cc.en.100.bin'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Length of the embedding is: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="s1">'king'</span><span class="p">))</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Embedding for the word King is: </span><span class="si">{</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="s1">'king'</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Cosine similarity is used for similarities between words. Similarity is a scalar between 0 and 1.</p>
<p>Now find the 10 most similar words to “King”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_nearest_neighbors</span><span class="p">(</span><span class="s2">"king"</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># Most similar by key</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="word-similarity">
<h3>Word Similarity<a class="headerlink" href="#word-similarity" title="Permalink to this headline">¶</a></h3>
<div class="section" id="video-3-semantic-measurements">
<h4>Video 3: Semantic Measurements<a class="headerlink" href="#video-3-semantic-measurements" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_remove-input docutils container">
</div>
<p>More on similarity between words. Let’s check how similar different pairs of word are. Feel free to play around.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">getSimilarity</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">):</span>
  <span class="n">v1</span> <span class="o">=</span> <span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="n">word1</span><span class="p">)</span>
  <span class="n">v2</span> <span class="o">=</span> <span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_word_vector</span><span class="p">(</span><span class="n">word2</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words King and Queen: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"king"</span><span class="p">,</span> <span class="s2">"queen"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words King and Knight: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"king"</span><span class="p">,</span> <span class="s2">"knight"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words King and Rock: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"king"</span><span class="p">,</span> <span class="s2">"rock"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words King and Twenty: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"king"</span><span class="p">,</span> <span class="s2">"twenty"</span><span class="p">))</span>

<span class="c1">## Try the same for two more pairs</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>

<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="homonym-words-dagger">
<h3>Homonym Words<span class="math notranslate nohighlight">\(^\dagger\)</span><a class="headerlink" href="#homonym-words-dagger" title="Permalink to this headline">¶</a></h3>
<p>Find the similarity for homonym words with their different meanings. The first one has been implemented for you.</p>
<p><span class="math notranslate nohighlight">\(^\dagger\)</span>: Two or more words having the same spelling or pronunciation but different meanings and origins are called <em>homonyms</em>. E.g.,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#######################     Words with multiple meanings     ##########################</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words Cricket and Insect: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"cricket"</span><span class="p">,</span> <span class="s2">"insect"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Similarity between the words Cricket and Sport: "</span><span class="p">,</span> <span class="n">getSimilarity</span><span class="p">(</span><span class="s2">"cricket"</span><span class="p">,</span> <span class="s2">"sport"</span><span class="p">))</span>

<span class="c1">## Try the same for two more pairs</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>

<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
<span class="c1"># print("Similarity between the words ___ and ___: ", getSimilarity(...))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="word-analogies">
<h3>Word Analogies<a class="headerlink" href="#word-analogies" title="Permalink to this headline">¶</a></h3>
<p>Embeddings can be used to find word analogies.
Let’s try it:</p>
<ol class="simple">
<li><p>Man : Woman  ::  King : _____</p></li>
<li><p>Germany: Berlin :: France : ______</p></li>
<li><p>Leaf : Tree  ::  Petal : _____</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Use get_analogies() funnction. The words have to be in the order Positive, negative,  Positve</span>

<span class="c1"># Man : Woman  ::  King : _____</span>
<span class="c1"># Positive=(woman, king), Negative=(man)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"woman"</span><span class="p">,</span> <span class="s2">"man"</span><span class="p">,</span> <span class="s2">"king"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Germany: Berlin :: France : ______</span>
<span class="c1"># Positive=(berlin, frannce), Negative=(germany)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"berlin"</span><span class="p">,</span> <span class="s2">"germany"</span><span class="p">,</span> <span class="s2">"france"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Leaf : Tree  ::  Petal : _____</span>
<span class="c1"># Positive=(tree, petal), Negative=(leaf)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"tree"</span><span class="p">,</span> <span class="s2">"leaf"</span><span class="p">,</span> <span class="s2">"petal"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Hammer : Nail  ::  Comb : _____</span>
<span class="c1"># Positive=(nail, comb), Negative=(hammer)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"nail"</span><span class="p">,</span> <span class="s2">"hammer"</span><span class="p">,</span> <span class="s2">"comb"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>But, does it always work?</p>
<ol class="simple">
<li><p>Poverty : Wealth  :: Sickness : _____</p></li>
<li><p>train : board :: horse : _____</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Poverty : Wealth  :: Sickness : _____</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"wealth"</span><span class="p">,</span> <span class="s2">"poverty"</span><span class="p">,</span> <span class="s2">"sickness"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># train : board :: horse : _____</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ft_en_vectors</span><span class="o">.</span><span class="n">get_analogies</span><span class="p">(</span><span class="s2">"board"</span><span class="p">,</span> <span class="s2">"train"</span><span class="p">,</span> <span class="s2">"horse"</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-neural-net-with-word-embeddings">
<h1>Section 3: Neural Net with word embeddings<a class="headerlink" href="#section-3-neural-net-with-word-embeddings" title="Permalink to this headline">¶</a></h1>
<p>Let’s use the pretrained FastText embeddings to train a neural network on the IMDB dataset.</p>
<p>To recap, the data consists of reviews and sentiments attached to it. It is a binary classification task. As a simple preview of the upcoming neural networks, we are going to introduce neural net with word embeddings. We’ll see detailed networks in the next tutorial.</p>
<div class="section" id="coding-exercise-3-1-simple-feed-forward-net">
<h2>Coding Exercise 3.1: Simple Feed Forward Net<a class="headerlink" href="#coding-exercise-3-1-simple-feed-forward-net" title="Permalink to this headline">¶</a></h2>
<p>This will load 300 dim FastText embeddings. It will take around 2-3 minutes.</p>
<p>Define a vanilla neural network with linear layers. Then average the word embeddings to get an embedding for the entire review.
The neural net will have one hidden layer of size 128.</p>
<div class="section" id="download-embeddings-and-clear-old-variables-to-clean-memory">
<h3>Download embeddings and clear old variables to clean memory.<a class="headerlink" href="#download-embeddings-and-clear-old-variables-to-clean-memory" title="Permalink to this headline">¶</a></h3>
<div class="section" id="execute-this-cell">
<h4>Execute this cell!<a class="headerlink" href="#execute-this-cell" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Download embeddings and clear old variables to clean memory.</span>
<span class="c1"># @markdown #### Execute this cell!</span>
<span class="k">if</span> <span class="s1">'ft_en_vectors'</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">():</span>
  <span class="k">del</span> <span class="n">ft_en_vectors</span>
<span class="k">if</span> <span class="s1">'w2vmodel'</span> <span class="ow">in</span> <span class="nb">locals</span><span class="p">():</span>
  <span class="k">del</span> <span class="n">w2vmodel</span>

<span class="n">embedding_fasttext</span> <span class="o">=</span> <span class="n">FastText</span><span class="p">(</span><span class="s1">'simple'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Load the Dataset</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Load the Dataset</span>
<span class="n">TEXT</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span> <span class="n">test_iter</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="n">embedding_fasttext</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_length</span><span class="p">,</span>
               <span class="n">word_embeddings</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_length</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">word_embeddings</span><span class="p">,</span>
                                               <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_length</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>


  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>

    <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_embeddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>  <span class="c1"># convert text to embeddings</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Fill in missing code below (...)</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Fill in the Neural Net"</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Average the word embeddings in a sentence</span>
    <span class="c1"># Use torch.nn.functional.avg_pool2d to compute the averages</span>
    <span class="n">pooled</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Pass the embeddings through the neural net</span>
    <span class="c1"># A fully-connected layer</span>
    <span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># ReLU activation</span>
    <span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Another fully-connected layer</span>
    <span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>


<span class="c1"># Uncomment to check your code</span>
<span class="c1"># nn_model = NeuralNet(2, 128, 100, 300, TEXT.vocab.vectors)</span>
<span class="c1"># print(nn_model)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D3_ModernRecurrentNeuralNetworks/solutions/W2D3_Tutorial1_Solution_e80c284c.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">NeuralNet</span><span class="p">(</span>
  <span class="p">(</span><span class="n">word_embeddings</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
  <span class="p">(</span><span class="n">fc1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="n">fc2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="training-and-testing-functions">
<h3>Training and Testing Functions<a class="headerlink" href="#training-and-testing-functions" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Training and Testing Functions</span>

<span class="c1"># @markdown #### `train(model, device, train_iter, valid_iter, epochs, learning_rate)`</span>
<span class="c1"># @markdown #### `test(model, device, test_iter)`</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">valid_iter</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
  <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

  <span class="n">train_loss</span><span class="p">,</span> <span class="n">validation_loss</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
  <span class="n">train_acc</span><span class="p">,</span> <span class="n">validation_acc</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># train</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">correct</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span>
      <span class="n">text</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
      <span class="n">text</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

      <span class="c1"># add micro for coding training loop</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
      <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

      <span class="c1"># get accuracy</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">))</span>
    <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Epoch: </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s1">, '</span>
          <span class="sa">f</span><span class="s1">'Training Loss: </span><span class="si">{</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, '</span>
          <span class="sa">f</span><span class="s1">'Training Accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="si">:</span><span class="s1"> .2f</span><span class="si">}</span><span class="s1">%'</span><span class="p">)</span>

    <span class="c1"># evaluate on validation data</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">correct</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">valid_iter</span><span class="p">):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="n">text</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># get accuracy</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">validation_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_iter</span><span class="p">))</span>
    <span class="n">validation_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="p">)</span>

    <span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s1">'Validation Loss: </span><span class="si">{</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_iter</span><span class="p">)</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, '</span>
           <span class="sa">f</span><span class="s1">'Validation Accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="si">:</span><span class="s1"> .2f</span><span class="si">}</span><span class="s1">%'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">validation_loss</span><span class="p">,</span> <span class="n">validation_acc</span>


<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">):</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_iter</span><span class="p">):</span>
      <span class="n">text</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">label</span>
      <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
      <span class="n">text</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
    <span class="k">return</span> <span class="n">acc</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model hyperparameters</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0003</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">embedding_length</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">word_embeddings</span> <span class="o">=</span> <span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">vectors</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">TEXT</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>

<span class="c1"># Model set-up</span>
<span class="n">nn_model</span> <span class="o">=</span> <span class="n">NeuralNet</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span>
                     <span class="n">hidden_size</span><span class="p">,</span>
                     <span class="n">vocab_size</span><span class="p">,</span>
                     <span class="n">embedding_length</span><span class="p">,</span>
                     <span class="n">word_embeddings</span><span class="p">)</span>
<span class="n">nn_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">nn_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">522</span><span class="p">)</span>
<span class="n">nn_train_loss</span><span class="p">,</span> <span class="n">nn_train_acc</span><span class="p">,</span> <span class="n">nn_validation_loss</span><span class="p">,</span> <span class="n">nn_validation_acc</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">nn_model</span><span class="p">,</span>
                                                                           <span class="n">DEVICE</span><span class="p">,</span>
                                                                           <span class="n">train_iter</span><span class="p">,</span>
                                                                           <span class="n">valid_iter</span><span class="p">,</span>
                                                                           <span class="n">epochs</span><span class="p">,</span>
                                                                           <span class="n">learning_rate</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"--- Time taken to train = </span><span class="si">{</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">nn_start_time</span><span class="p">)</span><span class="si">}</span><span class="s2"> seconds ---"</span><span class="p">)</span>
<span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">nn_model</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="se">\n\n</span><span class="s1">Test Accuracy: </span><span class="si">{</span><span class="n">test_accuracy</span><span class="si">}</span><span class="s1">%'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot accuracy curves</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">nn_train_acc</span><span class="p">,</span> <span class="n">nn_validation_acc</span><span class="p">,</span>
               <span class="s1">'train accuracy'</span><span class="p">,</span> <span class="s1">'val accuracy'</span><span class="p">,</span>
               <span class="s1">'Neural Net on IMDB text classification'</span><span class="p">,</span> <span class="s1">'accuracy'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C0'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">plot_train_val</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">),</span> <span class="n">nn_train_loss</span><span class="p">,</span>
               <span class="n">nn_validation_loss</span><span class="p">,</span>
               <span class="s1">'train loss'</span><span class="p">,</span> <span class="s1">'val loss'</span><span class="p">,</span>
               <span class="s1">''</span><span class="p">,</span>
               <span class="s1">'loss [a.u.]'</span><span class="p">,</span>
               <span class="n">color</span><span class="o">=</span><span class="s1">'C0'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">'upper left'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we explored two different concepts linked to sequences, and text in particular, that will be the conceptual foundation for Recurrent Neural Networks.</p>
<p>The first concept was that of sequences and probabilities. We saw how we can model language as sequences of text, and use this analogy to generate text. Such a setup is also used to classify text or identify parts of speech. We can either build chains manually using simple python and numerical computation, or use a package such as <code class="docutils literal notranslate"><span class="pre">hmmlearn</span></code> that allows us to train models a lot easier. These notions of sequences and probabilities (i.e, creating language models!) are key to the internals of a recurrent neural network as well.</p>
<p>The second concept is that of word embeddings, now a mainstay of natural language processing. By using a neural network to predict context of words, these neural networks learn internal representions of words that are a decent approximation of semantic meaning (i.e embeddings!). We saw how these embeddings can be visualised, as well as how they capture meaning. We finally saw how they can be integrated into neural networks to better classify text documents.</p>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W2D3_ModernRecurrentNeuralNetworks/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
<div class="prev-next-bottom">
<a class="left-prev" href="../chapter_title.html" id="prev-link" title="previous page">Modern Recurrent Neural Networks</a>
<a class="right-next" href="W2D3_Tutorial2.html" id="next-link" title="next page">Tutorial 2: Modern RNNs and their variants</a>
</div>
</div>
</div>
<footer class="footer mt-5 mt-md-0">
<div class="container">
<p>
        
          By Neuromatch<br/>
        
            © Copyright 2021.<br/>
</p>
</div>
</footer>
</main>
</div>
</div>
<script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>
</body>
</html>