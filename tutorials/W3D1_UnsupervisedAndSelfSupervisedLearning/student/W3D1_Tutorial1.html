
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 1: Un/Self-supervised learning methods — Neuromatch Academy: Deep Learning</title>
<link href="../../../_static/css/theme.css" rel="stylesheet"/>
<link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<link as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script src="../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
<script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
<script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-dl-logo-square-4xp.jpeg" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="../../W3D2_BasicReinforcementLearning/chapter_title.html" rel="next" title="Basic Reinforcement Learning"/>
<link href="../chapter_title.html" rel="prev" title="Unsupervised And Self Supervised Learning"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</link></link></link></link></head>
<body data-offset="80" data-spy="scroll" data-target="#bd-toc-nav">
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
<div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<img alt="logo" class="logo" src="../../../_static/nma-dl-logo-square-4xp.jpeg"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main navigation" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
   Introduction
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using Discord
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html">
     Tutorial 1: PyTorch
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html">
     Tutorial 3: Deep linear neural networks
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_Optimization/chapter_title.html">
   Optimization (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_Optimization/student/W1D4_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_Regularization/chapter_title.html">
   Regularization (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Doing more with fewer parameters
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/chapter_title.html">
   Convnets And Recurrent Neural Networks (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.html">
     Tutorial 2: Training loop of CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial3.html">
     Tutorial 3: Introduction to RNNs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial2.html">
     Tutorial 2: Facial recognition using modern convnets
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/chapter_title.html">
   Modern Recurrent Neural Networks (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.html">
     Tutorial 1: Modeling sequencies and encoding text
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html">
     Tutorial 2: Modern RNNs and their variants
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/student/W2D4_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D5_GenerativeModels/chapter_title.html">
   Generative Models (W2D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial2.html">
     Tutorial 2: Introduction to GANs and Density Ratio Estimation Perspective of GANs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial3.html">
     Tutorial 3: Conditional GANs and Implications of GAN Technology
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Advanced topics
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Unsupervised And Self Supervised Learning (W3D1)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 1: Un/Self-supervised learning methods
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/chapter_title.html">
   Basic Reinforcement Learning (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.html">
     Tutorial 1: Introduction to Reinforcement Learning
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/chapter_title.html">
   Reinforcement Learning For Games (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/student/W3D3_Tutorial1.html">
     Tutorial 1: Learn to play games with RL
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ContinualLearning/chapter_title.html">
   Continual Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Introduction to Continual Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Out-of-distribution (OOD) Learning
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction to projects
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through2_DL.html">
     Modeling Steps 1 - 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_3through4_DL.html">
     Modeling Steps 3 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through6_DL.html">
     Modeling Steps 5 - 6
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_7through9_DL.html">
     Modeling Steps 7 - 9
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_10_DL.html">
     Modeling Steps 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProjectDL.html">
     Example Data Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModelingProjectDL.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/Example_Deep_Learning_Project.html">
     Example Deep Learning Project
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/projects_overview.html">
   Project Templates
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ComputerVision/README.html">
     Computer Vision
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/em_synapses.html">
       Knowledge Extraction from a Convolutional Neural Network
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/spectrogram_analysis.html">
       Music classification and generation with spectrograms
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/screws.html">
       Setup matplotlib
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/image_alignment.html">
       Image Alignment
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/data_augmentation.html">
       Data Augmentation in image classification models
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/transfer_learning.html">
       Transfer Learning
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ReinforcementLearning/README.html">
     Reinforcement Learning
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/robolympics.html">
       NMA Robolympics: Controlling robots using reinforcement learning
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/lunar_lander.html">
       Performance Analysis of DQN Algorithm on the Lunar Lander task
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/human_rl.html">
<strong>
        ⚠️ This is a work in progress.
       </strong>
</a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/README.html">
     Natural Language Processing
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/sentiment_analysis.html">
       Twitter Sentiment Analysis
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/machine_translation.html">
       Machine Translation
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/Neuroscience/README.html">
     Neuroscience
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/pose_estimation.html">
       Animal Pose Estimation
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/cellular_segmentation.html">
       Segmentation and Denoising
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/algonauts_videos.html">
       Load algonauts videos
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/blurry_vision.html">
       Vision with Lost Glasses: Modelling how the brain deals with noisy input
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/finetuning_fmri.html">
       Moving beyond Labels: Finetuning CNNs on BOLD response
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/neuro_seq_to_seq.html">
       Focus on what matters: inferring low-dimensional dynamics from neural recordings
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/datasets_and_models.html">
   Models and Data sets
  </a>
</li>
</ul>
</div>
</nav> <!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
<div class="topbar container-xl fixed-top">
<div class="topbar-contents row">
<div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
<div class="col pl-md-4 topbar-main">
<button aria-controls="site-navigation" aria-expanded="true" aria-label="Toggle navigation" class="navbar-toggler ml-0" data-placement="left" data-target=".site-navigation" data-toggle="tooltip" id="navbar-toggler" title="Toggle navigation" type="button">
<i class="fas fa-bars"></i>
<i class="fas fa-arrow-left"></i>
<i class="fas fa-arrow-up"></i>
</button>
<div class="dropdown-buttons-trigger">
<button aria-label="Download this page" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fas fa-download"></i></button>
<div class="dropdown-buttons">
<!-- ipynb file if we had a myst markdown file -->
<!-- Download raw file -->
<a class="dropdown-buttons" href="../../../_sources/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.ipynb"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Download source file" type="button">.ipynb</button></a>
<!-- Download PDF via print -->
<button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" id="download-print" onclick="window.print()" title="Print to PDF" type="button">.pdf</button>
</div>
</div>
<!-- Source interaction buttons -->
<div class="dropdown-buttons-trigger">
<button aria-label="Connect with source repository" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fab fa-github"></i></button>
<div class="dropdown-buttons sourcebuttons">
<a class="repository-button" href="https://github.com/NeuromatchAcademy/course-content-dl"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Source repository" type="button"><i class="fab fa-github"></i>repository</button></a>
<a class="issues-button" href="https://github.com/NeuromatchAcademy/course-content-dl/issues/new?title=Issue%20on%20page%20%2Ftutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html&amp;body=Your%20issue%20content%20here."><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Open an issue" type="button"><i class="fas fa-lightbulb"></i>open issue</button></a>
</div>
</div>
<!-- Full screen (wrap in <a> to have style consistency -->
<a class="full-screen-button"><button aria-label="Fullscreen mode" class="btn btn-secondary topbarbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode" type="button"><i class="fas fa-expand"></i></button></a>
<!-- Launch buttons -->
</div>
<!-- Table of contents -->
<div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
            </div>
<nav id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 1: Un/Self-supervised learning methods
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-0-introduction">
     Video 0: Introduction
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-slides">
     Tutorial slides
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-dependencies">
     Install dependencies
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
     Set device (GPU or CPU). Execute
     <code class="docutils literal notranslate">
<span class="pre">
       set_device()
      </span>
</code>
</a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#pre-load-variables-allows-each-section-to-be-run-independently">
       Pre-load variables (allows each section to be run independently)
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-representations-are-important">
   Section 1: Representations are important
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-why-do-representations-matter">
     Video 1: Why do representations matter?
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-introducing-the-dsprites-dataset">
     Section 1.1: Introducing the dSprites dataset
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-1-1-1-exploring-the-dsprites-dataset">
       Interactive Demo 1.1.1: Exploring the dSprites dataset
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-training-a-classifier-with-and-without-representations">
     Section 1.2: Training a classifier with and without representations
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#encoder-network-schematic">
       Encoder network schematic
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-1-2-1-training-a-logistic-regression-classifier-directly-on-images">
       Interactive Demo 1.2.1: Training a logistic regression classifier directly on images.
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-2-1-training-a-logistic-regression-classifier-along-with-an-encoder">
       Coding Exercise 1.2.1: Training a logistic regression classifier along with an encoder.
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-supervised-learning-induces-invariant-representations">
   Section 2: Supervised learning induces invariant representations
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-supervised-learning-and-invariance">
     Video 2: Supervised Learning and Invariance
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-examining-representational-similarity-matrices-rsms">
     Section 2.1: Examining Representational Similarity Matrices (RSMs)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-1-1-complete-a-function-that-calculates-rsms">
       Coding Exercise 2.1.1: Complete a function that calculates RSMs.
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-2-1-1-plotting-the-supervised-network-encoder-rsm-along-different-latent-dimensions">
       Interactive Demo 2.1.1: Plotting the supervised network encoder RSM along different latent dimensions.
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#discussion-2-1-1-what-patterns-do-the-rsms-reveal-about-how-the-encoder-represents-different-images">
       Discussion 2.1.1: What patterns do the RSMs reveal about how the encoder represents different images?
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#supporting-images-for-discussion-response-examples-for-2-1-1">
       Supporting images for Discussion response examples for 2.1.1
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-random-projections-dont-work-as-well">
   Section 3: Random projections don’t work as well
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-random-representations">
     Video 3: Random Representations
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-examining-rsms-of-a-random-encoder">
     Section 3.1: Examining RSMs of a random encoder
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-1-1-plotting-a-random-network-encoder-rsm-along-different-latent-dimensions">
       Coding Exercise 3.1.1: Plotting a random network encoder RSM along different latent dimensions.
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#discussion-3-1-1-what-does-comparing-these-rsms-reveal-about-the-potential-value-of-trained-versus-random-encoder-representations">
       Discussion 3.1.1: What does comparing these RSMs reveal about the potential value of trained versus random encoder representations?
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#supporting-images-for-discussion-response-examples-for-3-1-1-all-random-encoder-rsms">
       Supporting images for Discussion response examples for 3.1.1: All random encoder RSMs
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-1-2-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-a-random-network-encoder">
       Coding Exercise 3.1.2: Evaluating the classification performance of a logistic regression trained on the representations produced by a random network encoder.
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#discussion-3-1-2-what-can-we-conclude-about-the-potential-consequences-of-using-random-projections-with-a-dataset-like-dsprites">
       Discussion 3.1.2: What can we conclude about the potential consequences of using random projections with a dataset like dSprites?
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-4-generative-approaches-to-representation-learning-can-fail">
   Section 4: Generative approaches to representation learning can fail
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-4-generative-models">
     Video 4: Generative models
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-4-1-examining-the-rsms-of-a-variational-autoencoder">
     Section 4.1: Examining the RSMs of a Variational Autoencoder
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-4-1-1-plotting-example-reconstructions-using-the-pre-trained-vae-encoder-and-decoder">
       Interactive Demo 4.1.1: Plotting example reconstructions using the pre-trained VAE encoder and decoder.
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#discussion-4-1-1-how-does-the-vae-perform-on-the-reconstruction-task">
       Discussion 4.1.1: How does the VAE perform on the reconstruction task?
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-4-1-2-visualizing-the-vae-encoder-rsms-organized-along-different-latent-dimensions">
       Interactive Demo 4.1.2: Visualizing the VAE encoder RSMs, organized along different latent dimensions.
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#discussion-4-1-2-what-can-we-conclude-about-the-the-ability-of-generative-models-like-vaes-to-construct-a-meaningful-representation-space">
       Discussion 4.1.2: What can we conclude about the the ability of generative models like VAEs to construct a meaningful representation space?
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#supporting-images-for-discussion-response-examples-for-4-1-2-all-vae-encoder-rsms">
       Supporting images for Discussion response examples for 4.1.2: All VAE encoder RSMs
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-4-1-2-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-the-pre-trained-vae-network-encoder">
       Coding Exercise 4.1.2: Evaluating the classification performance of a logistic regression trained on the representations produced by the pre-trained VAE network encoder.
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-5-the-modern-approach-to-self-supervised-training-for-invariance">
   Section 5: The modern approach to self-supervised training for invariance
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-5-modern-approach-in-self-supervised-learning">
     Video 5: Modern Approach in Self-supervised Learning
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-5-1-examining-different-options-for-learning-invariant-representations">
     Section 5.1: Examining different options for learning invariant representations.
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-5-1-1-visualizing-a-few-different-image-transformations-available-that-could-be-used-to-learn-invariance">
       Interactive Demo 5.1.1: Visualizing a few different image transformations available that could be used to learn invariance.
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-6-how-to-train-for-invariance-to-transformations-with-a-target-network">
   Section 6: How to train for invariance to transformations with a target network
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-6-data-transformations">
     Video 6: Data Transformations
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-6-1-using-image-transformations-to-learn-feature-invariant-representations-in-an-ssl-network">
     Section 6.1: Using image transformations to learn feature invariant representations in an SSL network.
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-6-1-1-complete-a-simclr-loss-function">
       Coding Exercise 6.1.1: Complete a SimCLR loss function.
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-6-1-1-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-a-simclr-network-encoder-that-was-pre-trained-using-different-image-transformations">
       Interactive Demo 6.1.1: Evaluating the classification performance of a logistic regression trained on the representations produced by a SimCLR network encoder that was pre-trained using different image transformations.
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-7-self-supervised-networks-learn-representation-invariance">
   Section 7: Self-supervised networks learn representation invariance
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-7-invariant-representations">
     Video 7: Invariant Representations
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-7-1-the-effects-of-using-data-transformations-on-invariance-in-simclr-network-representations">
     Section 7.1: The effects of using data transformations on invariance in SimCLR network representations.
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-7-1-1-visualizing-the-simclr-network-encoder-rsms-organized-along-different-latent-dimensions">
       Interactive Demo 7.1.1: Visualizing the SimCLR network encoder RSMs, organized along different latent dimensions.
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#discussion-7-1-1-what-can-we-conclude-about-the-ability-of-contrastive-models-like-simclr-to-construct-a-meaningful-representation-space">
       Discussion 7.1.1: What can we conclude about the ability of contrastive models like SimCLR to construct a meaningful representation space?
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#supporting-images-for-discussion-response-examples-for-7-1-1-all-simclr-encoder-rsms">
       Supporting images for Discussion response examples for 7.1.1: All SimCLR encoder RSMs
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-8-avoiding-representational-collapse">
   Section 8: Avoiding representational collapse
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-8-avoiding-representational-collapse">
     Video 8: Avoiding Representational Collapse
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-8-1-the-effects-of-reducing-the-number-of-negative-examples-used-in-the-simclr-contrastive-loss">
     Section 8.1: The effects of reducing the number of negative examples used in the SimCLR contrastive loss.
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-8-1-1-visualizing-the-network-encoder-rsms-organized-along-different-latent-dimensions-and-plotting-similarity-histograms">
       Coding Exercise 8.1.1: Visualizing the network encoder RSMs, organized along different latent dimensions, and plotting similarity histograms.
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#discussion-8-1-1-what-can-we-conclude-about-the-importance-of-negative-pairs-in-computing-the-contrastive-loss-for-models-like-simclr">
       Discussion 8.1.1: What can we conclude about the importance of negative pairs in computing the contrastive loss for models like SimCLR?
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#supporting-images-for-discussion-response-examples-for-8-1-1-all-simclr-encoder-2-neg-pairs-rsms">
       Supporting images for Discussion response examples for 8.1.1: All SimCLR encoder (2 neg. pairs) RSMs
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-8-1-1-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-a-simclr-network-encoder-pre-trained-with-only-a-few-negative-pairs">
       Interactive Demo 8.1.1: Evaluating the classification performance of a logistic regression trained on the representations produced by a SimCLR network encoder pre-trained with only a few negative pairs.
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-9-good-representations-enable-few-shot-learning">
   Section 9: Good representations enable few-shot learning.
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-9-few-shot-supervised-learning">
     Video 9: Few-shot Supervised Learning
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-9-1-the-benefits-of-pre-training-an-encoder-network-in-a-few-short-learning-scenario-i-e-when-only-few-labelled-examples-are-available">
     Section 9.1: The benefits of pre-training an encoder network in a few-short learning scenario, i.e., when only few labelled examples are available.
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-9-1-1-training-classifiers-on-different-encoders-using-only-a-fraction-of-the-full-labelled-dataset">
       Interactive Demo 9.1.1: Training classifiers on different encoders, using only a fraction of the full labelled dataset.
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#discussion-9-1-1-what-can-we-conclude-the-advantages-and-disadvantages-of-the-different-encoder-network-types-under-different-conditions">
       Discussion 9.1.1: What can we conclude the advantages and disadvantages of the different encoder network types under different conditions?
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#supporting-images-for-discussion-response-examples-for-9-1-1-classifier-performances-for-various-fractions-of-labelled-data">
       Supporting images for Discussion response examples for 9.1.1: Classifier performances for various fractions of labelled data
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-10-ethical-considerations-for-self-supervised-learning-from-biased-datasets">
   Section 10: Ethical considerations for self-supervised learning from biased datasets
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-10-un-self-supervised-learning">
     Video 10: Un/Self-Supervised Learning
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-10-1-the-consequences-of-training-models-on-biased-datasets">
     Section 10.1: The consequences of training models on biased datasets.
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#discussion-10-1-1-how-do-different-models-cope-with-a-biased-training-dataset">
       Discussion 10.1.1: How do different models cope with a biased training dataset?
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#discussion-10-1-2-how-do-these-principles-apply-more-generally">
       Discussion 10.1.2: How do these principles apply more generally?
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-11-conclusion">
   Section 11: Conclusion
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-11-conclusion">
     Video 11: Conclusion
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="row" id="main-content">
<div class="col-12 col-md-9 pl-md-3 pr-md-0">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<div class="section" id="tutorial-1-un-self-supervised-learning-methods">
<h1>Tutorial 1: Un/Self-supervised learning methods<a class="headerlink" href="#tutorial-1-un-self-supervised-learning-methods" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 3, Day 1: Unsupervised and self-supervised learning</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Arna Ghosh, Colleen Gillon, Tim Lillicrap, Blake Richards</p>
<p><strong>Content reviewers:</strong> Atnafu Lambebo, Hadi Vafaei, Khalid Almubarak, Melvin Selim Atay</p>
<p><strong>Content editors:</strong> Anoop Kulkarni, Spiros Chalvis</p>
<p><strong>Production editors:</strong> Deepak Raya, Spiros Chalvis</p>
<p><strong>Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs</strong></p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial Objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, you will learn about the importance of learning good representations of data.</p>
<p>Specific objectives for this tutorial:</p>
<ul class="simple">
<li><p>Train logistic regressions (A) directly on input data and (B) on representations learned from the data.</p></li>
<li><p>Compare the classification performances achieved by the different networks.</p></li>
<li><p>Compare the representations learned by the different networks.</p></li>
<li><p>Identify the advantages of self-supervised learning over supervised or traditional unsupervised methods.</p></li>
</ul>
<div class="section" id="video-0-introduction">
<h2>Video 0: Introduction<a class="headerlink" href="#video-0-introduction" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "2dbc55db84ae47da8265dde05faf754d"}
</script></div>
</div>
</div>
<div class="section" id="tutorial-slides">
<h2>Tutorial slides<a class="headerlink" href="#tutorial-slides" title="Permalink to this headline">¶</a></h2>
<p>These are the slides for the videos in this tutorial</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe allowfullscreen="" frameborder="0" height="480" src="https://mfr.ca-1.osf.io/render?url=https://osf.io/wvt34/?direct%26mode=render%26action=download%26mode=render" width="854"></iframe>
</div></div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="section" id="install-dependencies">
<h2>Install dependencies<a class="headerlink" href="#install-dependencies" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Install dependencies</span>

<span class="c1"># @markdown Download dataset, modules, and files needed for the tutorial from GitHub.</span>

<span class="c1"># @markdown Download from OSF. Original repo: https://github.com/colleenjg/neuromatch_ssl_tutorial.git</span>

<span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">sys</span><span class="o">,</span> <span class="nn">importlib</span>

<span class="n">REPO_PATH</span> <span class="o">=</span> <span class="s2">"neuromatch_ssl_tutorial"</span>
<span class="n">download_str</span> <span class="o">=</span> <span class="s2">"Downloading"</span>
<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">):</span>
  <span class="n">download_str</span> <span class="o">=</span> <span class="s2">"Redownloading"</span>
  <span class="o">!</span>rm -rf <span class="nv">$REPO_PATH</span>

<span class="c1"># download from github repo directly</span>
<span class="c1">#!git clone git://github.com/colleenjg/neuromatch_ssl_tutorial.git --quiet</span>

<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>
<span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">urlopen</span>
<span class="kn">from</span> <span class="nn">zipfile</span> <span class="kn">import</span> <span class="n">ZipFile</span>

<span class="n">zipurl</span> <span class="o">=</span> <span class="s1">'https://osf.io/smqvg/download'</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">download_str</span><span class="si">}</span><span class="s2"> and unzipping the file... Please wait."</span><span class="p">)</span>
<span class="k">with</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">zipurl</span><span class="p">)</span> <span class="k">as</span> <span class="n">zipresp</span><span class="p">:</span>
  <span class="k">with</span> <span class="n">ZipFile</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">zipresp</span><span class="o">.</span><span class="n">read</span><span class="p">()))</span> <span class="k">as</span> <span class="n">zfile</span><span class="p">:</span>
    <span class="n">zfile</span><span class="o">.</span><span class="n">extractall</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Download completed!"</span><span class="p">)</span>


<span class="c1"># @markdown Import modules designed for use in this tutorials</span>
<span class="kn">from</span> <span class="nn">neuromatch_ssl_tutorial.modules</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">load</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">plot_util</span>
<span class="kn">from</span> <span class="nn">neuromatch_ssl_tutorial.modules</span> <span class="kn">import</span> <span class="n">data</span><span class="p">,</span> <span class="n">load</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">plot_util</span>
<span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">data</span><span class="p">);</span>
<span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">load</span><span class="p">);</span>
<span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
<span class="n">importlib</span><span class="o">.</span><span class="n">reload</span><span class="p">(</span><span class="n">plot_util</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading and unzipping the file... Please wait.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Download completed!
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>       <span class="c1"># interactive display</span>
<span class="c1">#%matplotlib inline</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'axes'</span><span class="p">,</span> <span class="n">unicode_minus</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># to ensure negatives render correctly with xkcd style</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plotting-functions">
<h2>Plotting functions<a class="headerlink" href="#plotting-functions" title="Permalink to this headline">¶</a></h2>
<p>Function to plot a histogram of RSM values: <code class="docutils literal notranslate"><span class="pre">plot_rsm_histogram(rsms,</span> <span class="pre">colors)</span></code></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plotting functions</span>

<span class="c1"># @markdown Function to plot a histogram of RSM values: `plot_rsm_histogram(rsms, colors)`</span>
<span class="k">def</span> <span class="nf">plot_rsm_histogram</span><span class="p">(</span><span class="n">rsms</span><span class="p">,</span> <span class="n">colors</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Histogram of RSM values"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.05</span><span class="p">)</span>

  <span class="n">min_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">nanmin</span><span class="p">(</span><span class="n">rsm</span><span class="p">)</span> <span class="k">for</span> <span class="n">rsm</span> <span class="ow">in</span> <span class="n">rsms</span><span class="p">])</span>
  <span class="n">max_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">nanmax</span><span class="p">(</span><span class="n">rsm</span><span class="p">)</span> <span class="k">for</span> <span class="n">rsm</span> <span class="ow">in</span> <span class="n">rsms</span><span class="p">])</span>

  <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">,</span> <span class="n">nbins</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">labels</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">rsms</span><span class="p">)</span>
  <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rsms</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"If providing labels, must provide as many as RSMs."</span><span class="p">)</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rsms</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">colors</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Must provide as may colors as RSMs."</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">rsm</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rsms</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
        <span class="n">rsm</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">bins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">r</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">labels</span><span class="p">[</span><span class="n">r</span><span class="p">]</span>
        <span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">"dashed"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"k"</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Density"</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Similarity values"</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions">
<h2>Helper functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper functions</span>

<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Image</span> <span class="c1"># to visualize images</span>

<span class="c1"># @markdown Function to set test custom torch RSM function: `test_custom_torch_RSM_fct()`</span>
<span class="k">def</span> <span class="nf">test_custom_torch_RSM_fct</span><span class="p">(</span><span class="n">custom_torch_RSM_fct</span><span class="p">):</span>
  <span class="n">rand_feats</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
  <span class="n">RSM_custom</span> <span class="o">=</span> <span class="n">custom_torch_RSM_fct</span><span class="p">(</span><span class="n">rand_feats</span><span class="p">)</span>
  <span class="n">RSM_ground_truth</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">calculate_torch_RSM</span><span class="p">(</span><span class="n">rand_feats</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">RSM_custom</span><span class="p">,</span> <span class="n">RSM_ground_truth</span><span class="p">,</span> <span class="n">equal_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"custom_torch_RSM_fct() is correctly implemented."</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"custom_torch_RSM_fct() is NOT correctly implemented."</span><span class="p">)</span>


<span class="c1"># @markdown Function to set test custom contrastive loss function: `test_custom_contrastive_loss_fct()`</span>
<span class="k">def</span> <span class="nf">test_custom_contrastive_loss_fct</span><span class="p">(</span><span class="n">custom_simclr_contrastive_loss</span><span class="p">):</span>
  <span class="n">rand_proj_feat1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
  <span class="n">rand_proj_feat2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
  <span class="n">loss_custom</span> <span class="o">=</span> <span class="n">custom_simclr_contrastive_loss</span><span class="p">(</span><span class="n">rand_proj_feat1</span><span class="p">,</span> <span class="n">rand_proj_feat2</span><span class="p">)</span>
  <span class="n">loss_ground_truth</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">contrastive_loss</span><span class="p">(</span><span class="n">rand_proj_feat1</span><span class="p">,</span><span class="n">rand_proj_feat2</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">loss_custom</span><span class="p">,</span> <span class="n">loss_ground_truth</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"custom_simclr_contrastive_loss() is correctly implemented."</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"custom_simclr_contrastive_loss() is NOT correctly implemented."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-random-seed">
<h2>Set random seed<a class="headerlink" href="#set-random-seed" title="Permalink to this headline">¶</a></h2>
<p>Executing <code class="docutils literal notranslate"><span class="pre">set_seed(seed=seed)</span></code> you are setting the seed</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Set random seed</span>

<span class="c1">#@markdown Executing `set_seed(seed=seed)` you are setting the seed</span>

<span class="c1"># for DL its critical to set the random seed so that students can have a</span>
<span class="c1"># baseline to compare their results to expected results.</span>
<span class="c1"># Read more here: https://pytorch.org/docs/stable/notes/randomness.html</span>

<span class="c1"># Call `set_seed` function in the exercises to ensure reproducibility.</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Random seed </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1"> has been set.'</span><span class="p">)</span>


<span class="c1"># In case that `DataLoader` is used</span>
<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
  <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-device-gpu-or-cpu-execute-set-device">
<h2>Set device (GPU or CPU). Execute <code class="docutils literal notranslate"><span class="pre">set_device()</span></code><a class="headerlink" href="#set-device-gpu-or-cpu-execute-set-device" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Set device (GPU or CPU). Execute `set_device()`</span>
<span class="c1"># especially if torch modules used.</span>

<span class="c1"># inform the user if the notebook uses GPU or CPU.</span>

<span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: For this notebook to perform best, "</span>
        <span class="s2">"if possible, in the menu under `Runtime` -&gt; "</span>
        <span class="s2">"`Change runtime type.`  select `GPU` "</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook."</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set global variables</span>
<span class="n">SEED</span> <span class="o">=</span> <span class="mi">2021</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -&gt; `Change runtime type.`  select `GPU` 
</pre></div>
</div>
</div>
</div>
<div class="section" id="pre-load-variables-allows-each-section-to-be-run-independently">
<h3>Pre-load variables (allows each section to be run independently)<a class="headerlink" href="#pre-load-variables-allows-each-section-to-be-run-independently" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown ### Pre-load variables (allows each section to be run independently)</span>

<span class="c1"># Section 1</span>
<span class="n">dSprites</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dSpritesDataset</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span> <span class="s2">"dsprites"</span><span class="p">,</span> <span class="s2">"dsprites_subset.npz"</span><span class="p">)</span>
    <span class="p">)</span>

<span class="n">dSprites_torchdataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dSpritesTorchDataset</span><span class="p">(</span>
  <span class="n">dSprites</span><span class="p">,</span>
  <span class="n">target_latent</span><span class="o">=</span><span class="s2">"shape"</span>
  <span class="p">)</span>

<span class="n">train_sampler</span><span class="p">,</span> <span class="n">test_sampler</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">train_test_split_idx</span><span class="p">(</span>
  <span class="n">dSprites_torchdataset</span><span class="p">,</span>
  <span class="n">fraction_train</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
  <span class="n">randst</span><span class="o">=</span><span class="n">SEED</span>
  <span class="p">)</span>

<span class="n">supervised_encoder</span> <span class="o">=</span> <span class="n">load</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span>
                                       <span class="n">model_type</span><span class="o">=</span><span class="s2">"supervised"</span><span class="p">,</span>
                                       <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Section 2</span>
<span class="n">custom_torch_RSM_fct</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># default is used instead</span>

<span class="c1"># Section 3</span>
<span class="n">random_encoder</span> <span class="o">=</span> <span class="n">load</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span>
                                   <span class="n">model_type</span><span class="o">=</span><span class="s2">"random"</span><span class="p">,</span>
                                   <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Section 4</span>
<span class="n">vae_encoder</span> <span class="o">=</span> <span class="n">load</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span>
                                <span class="n">model_type</span><span class="o">=</span><span class="s2">"vae"</span><span class="p">,</span>
                                <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Section 5</span>
<span class="n">invariance_transforms</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">RandomAffine</span><span class="p">(</span>
    <span class="n">degrees</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span>
    <span class="n">translate</span><span class="o">=</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
    <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
    <span class="p">)</span>
<span class="n">dSprites_invariance_torchdataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dSpritesTorchDataset</span><span class="p">(</span>
    <span class="n">dSprites</span><span class="p">,</span>
    <span class="n">target_latent</span><span class="o">=</span><span class="s2">"shape"</span><span class="p">,</span>
    <span class="n">simclr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">simclr_transforms</span><span class="o">=</span><span class="n">invariance_transforms</span>
    <span class="p">)</span>

<span class="c1"># Section 6</span>
<span class="n">simclr_encoder</span> <span class="o">=</span> <span class="n">load</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span>
                                   <span class="n">model_type</span><span class="o">=</span><span class="s2">"simclr"</span><span class="p">,</span>
                                   <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-representations-are-important">
<h1>Section 1: Representations are important<a class="headerlink" href="#section-1-representations-are-important" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-1-why-do-representations-matter">
<h2>Video 1: Why do representations matter?<a class="headerlink" href="#video-1-why-do-representations-matter" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "0bff7e0d3c944c94bce6c89b6f67ba7b"}
</script></div>
</div>
</div>
<div class="section" id="section-1-1-introducing-the-dsprites-dataset">
<h2>Section 1.1: Introducing the dSprites dataset<a class="headerlink" href="#section-1-1-introducing-the-dsprites-dataset" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial, we will be using a subset of the openly available <strong>dSprites dataset</strong> to investigate the importance of learning good representations.</p>
<p><em><strong>Note on dataset:</strong> For convenience, we will be using a subset of the original, full dataset which is available <a class="reference external" href="https://github.com/deepmind/dsprites-dataset/">here</a>, on GitHub.</em></p>
<div class="section" id="interactive-demo-1-1-1-exploring-the-dsprites-dataset">
<h3>Interactive Demo 1.1.1: Exploring the dSprites dataset<a class="headerlink" href="#interactive-demo-1-1-1-exploring-the-dsprites-dataset" title="Permalink to this headline">¶</a></h3>
<p>In this first demo, we will get to know the <strong>dSprites dataset</strong>. This dataset is made up of black and white images (20,000 images total in the subset we are using).</p>
<p>The images in the dataset can be described using different combinations of <strong>latent dimension values</strong>, sampled from:</p>
<ul class="simple">
<li><p><strong>Shapes (3):</strong> square (1.0), oval (2.0) or heart (3.0)</p></li>
<li><p><strong>Scales (6):</strong> 0.5 to 1.0</p></li>
<li><p><strong>Orientations (40):</strong> 0 to 2<span class="math notranslate nohighlight">\(\pi\)</span></p></li>
<li><p><strong>Positions in X (32):</strong> 0 to 1 (left to right)</p></li>
<li><p><strong>Positions in Y (32):</strong> 0 to 1 (top to bottom)</p></li>
</ul>
<p>As a result, <strong>each image carries 5 labels.</strong> One for each of the latent dimensions.</p>
<p>We will first load the dataset into the <code class="docutils literal notranslate"><span class="pre">dSprites</span></code> object, which is an instance of the <code class="docutils literal notranslate"><span class="pre">data.dSpritesDataset</span></code> class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dSprites</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dSpritesDataset</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span> <span class="s2">"dsprites"</span><span class="p">,</span> <span class="s2">"dsprites_subset.npz"</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we use the <code class="docutils literal notranslate"><span class="pre">dSpritesDataset</span></code> class method <code class="docutils literal notranslate"><span class="pre">show_images()</span></code> to plot a few images from the dataset, with their latent dimension values printed below.</p>
<p><strong>Interactive Demo:</strong> View a different set of randomly sampled images by passing the random state argument <code class="docutils literal notranslate"><span class="pre">randst</span></code> any integer or the value <code class="docutils literal notranslate"><span class="pre">None</span></code>. (The original setting is <code class="docutils literal notranslate"><span class="pre">randst=SEED</span></code>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEMO: to view different images, set randst to any integer value.</span>
<span class="n">dSprites</span><span class="o">.</span><span class="n">show_images</span><span class="p">(</span><span class="n">num_images</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">randst</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D1_Tutorial1_35_0.png" src="../../../_images/W3D1_Tutorial1_35_0.png"/>
</div>
</div>
<p>To better understand the <code class="docutils literal notranslate"><span class="pre">posX</span></code> and <code class="docutils literal notranslate"><span class="pre">posY</span></code> latent dimensions (which will be most relevant in <strong>Section 10</strong>), we plot the images with some annotations. The annotations (in red) do not modify the actual images; they are added <strong>purely for visualization purposes</strong>, and show:</p>
<ul class="simple">
<li><p>the <strong>edges</strong> of the <code class="docutils literal notranslate"><span class="pre">posX</span></code> and <code class="docutils literal notranslate"><span class="pre">posY</span></code> spans, and</p></li>
<li><p>the <strong>center</strong>, i.e. <code class="docutils literal notranslate"><span class="pre">(posX,</span> <span class="pre">posY)</span></code>, for each shape.</p></li>
</ul>
<p><em><strong>Note on shape positions:</strong> Notice that all shape centers are positioned <strong>within the area marked by the red square</strong>. <code class="docutils literal notranslate"><span class="pre">posX</span></code> and <code class="docutils literal notranslate"><span class="pre">posY</span></code> actually describe the relative position of the center of a shape within this area: <code class="docutils literal notranslate"><span class="pre">posX=0</span></code> (left) to <code class="docutils literal notranslate"><span class="pre">posX=1</span></code> (right), and <code class="docutils literal notranslate"><span class="pre">posY=0</span></code> (top) to <code class="docutils literal notranslate"><span class="pre">posY=1</span></code> (bottom). No shape center appears outside, in the buffer area. This choice in the dSprites dataset design ensures that shapes of different scales and rotations <strong>all appear fully</strong>.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEMO: to view different images, set randst to any integer value.</span>
<span class="n">dSprites</span><span class="o">.</span><span class="n">show_images</span><span class="p">(</span><span class="n">num_images</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">randst</span><span class="o">=</span><span class="n">SEED</span><span class="p">,</span> <span class="n">annotations</span><span class="o">=</span><span class="s2">"pos"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D1_Tutorial1_37_0.png" src="../../../_images/W3D1_Tutorial1_37_0.png"/>
</div>
</div>
</div>
</div>
<div class="section" id="section-1-2-training-a-classifier-with-and-without-representations">
<h2>Section 1.2: Training a classifier with and without representations<a class="headerlink" href="#section-1-2-training-a-classifier-with-and-without-representations" title="Permalink to this headline">¶</a></h2>
<p>Now, we will investigate how 2 different types of classifiers perform when trained to decode the shape latent dimension of images in the <strong>dSprites dataset</strong>.</p>
<p>Specifically, we will train <strong>one classifier directly on the images</strong>, and <strong>another on the output of an encoder network</strong>.</p>
<p>The <strong>encoder network</strong> we will use here and throughout the tutorial is the multi-layer convolutional network, pictured below. It comprises 2 consecutive convolutional layers, followed by 3 fully connected layers, and uses average pooling and batch normalization between layers, as well as rectified linear units as non-linearities.</p>
<p>The <strong>classifier layer</strong> then takes the encoder features as input, predicting, for example, the shape latent dimension of encoded input images.</p>
<p><em><strong>Note on terminology:</strong> In this tutorial, both the terms <strong>representations</strong> and <strong>features</strong> are used to refer to the data embeddings learned in the final layer of the encoder network (of dimension 1x84, and indicated by a red dashed box) which are fed to the classifiers.</em></p>
<div class="section" id="encoder-network-schematic">
<h3>Encoder network schematic<a class="headerlink" href="#encoder-network-schematic" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown ### Encoder network schematic</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span> <span class="s2">"images"</span><span class="p">,</span> <span class="s2">"feat_encoder_schematic.png"</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1200</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D1_Tutorial1_40_0.png" src="../../../_images/W3D1_Tutorial1_40_0.png"/>
</div>
</div>
<p>The following code:</p>
<ul class="simple">
<li><p>seeds modules that will use random processes, to ensure the results are consistently reproducible, using the <code class="docutils literal notranslate"><span class="pre">seed_processes()</span></code> function,</p></li>
<li><p>collects the dSprites dataset into a torch dataset using the <code class="docutils literal notranslate"><span class="pre">data.dSpritesTorchDataset</span></code> class,</p></li>
<li><p>initializes a training and a test sampler to keep the two datasets separate using the <code class="docutils literal notranslate"><span class="pre">data.train_test_splix_idx()</span></code> function.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="c1"># initialize a torch dataset, specifying the target latent dimension for</span>
<span class="c1"># the classifier</span>
<span class="n">dSprites_torchdataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dSpritesTorchDataset</span><span class="p">(</span>
  <span class="n">dSprites</span><span class="p">,</span>
  <span class="n">target_latent</span><span class="o">=</span><span class="s2">"shape"</span>
  <span class="p">)</span>

<span class="c1"># initialize a train_sampler and a test_sampler to keep the two sets</span>
<span class="c1"># consistently separate</span>
<span class="n">train_sampler</span><span class="p">,</span> <span class="n">test_sampler</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">train_test_split_idx</span><span class="p">(</span>
  <span class="n">dSprites_torchdataset</span><span class="p">,</span>
  <span class="n">fraction_train</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>  <span class="c1"># 80:20 data split</span>
  <span class="n">randst</span><span class="o">=</span><span class="n">SEED</span>
  <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Dataset size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_sampler</span><span class="p">)</span><span class="si">}</span><span class="s2"> training, "</span>
      <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_sampler</span><span class="p">)</span><span class="si">}</span><span class="s2"> test images"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
Dataset size: 16000 training, 4000 test images
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="interactive-demo-1-2-1-training-a-logistic-regression-classifier-directly-on-images">
<h3>Interactive Demo 1.2.1: Training a logistic regression classifier directly on images.<a class="headerlink" href="#interactive-demo-1-2-1-training-a-logistic-regression-classifier-directly-on-images" title="Permalink to this headline">¶</a></h3>
<p>The following code:</p>
<ul class="simple">
<li><p>trains a logistic regression directly on the training set images to classify their shape, and assesses its performance on the test set images using the <code class="docutils literal notranslate"><span class="pre">models.train_classifier()</span></code> function.</p></li>
</ul>
<p><strong>Interactive Demo:</strong> Try a few different <code class="docutils literal notranslate"><span class="pre">num_epochs</span></code> settings to see whether performance improves with more training, e.g., between 1 and 50 epochs. (The original setting is <code class="docutils literal notranslate"><span class="pre">num_epochs=25</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">25</span>  <span class="c1"># DEMO: Try different numbers of training epochs</span>

<span class="c1"># train a classifier directly on the images</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Training a classifier directly on the images..."</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">train_classifier</span><span class="p">(</span>
  <span class="n">encoder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
  <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_torchdataset</span><span class="p">,</span>
  <span class="n">train_sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
  <span class="n">test_sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span>
  <span class="n">freeze_features</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># there is no feature encoder to train here, anyway</span>
  <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
  <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span> <span class="c1"># print results</span>
  <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
Training a classifier directly on the images...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "fd6f8f586c5d44a89172b8e74d3427fa"}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Network performance after 25 classifier training epochs (chance: 33.33%):
    Training accuracy: 51.81%
    Testing accuracy: 39.55%
</pre></div>
</div>
</div>
</div>
<p>As we can observe, the classifier trained directly on the images performs only a bit above chance (39.55%) on the test set, after 25 training epochs.</p>
<p><b>Shape classification results using different feature encoders:</b></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><em>Chance</em></p></th>
<th class="head"><p></p></th>
<th class="head"><p>None (raw data)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>33.33%</em></p></td>
<td><p></p></td>
<td><p>39.55%</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="coding-exercise-1-2-1-training-a-logistic-regression-classifier-along-with-an-encoder">
<h3>Coding Exercise 1.2.1: Training a logistic regression classifier along with an encoder.<a class="headerlink" href="#coding-exercise-1-2-1-training-a-logistic-regression-classifier-along-with-an-encoder" title="Permalink to this headline">¶</a></h3>
<p>The following code:</p>
<ul class="simple">
<li><p>uses the same dSprites torch dataset (<code class="docutils literal notranslate"><span class="pre">dSprites_torchdataset</span></code>) initialized above, as well as the training and test samplers (<code class="docutils literal notranslate"><span class="pre">train_sampler</span></code>, <code class="docutils literal notranslate"><span class="pre">test_sampler</span></code>),</p></li>
<li><p>again, seeds modules that will use random processes, to ensure the results are consistently reproducible,</p></li>
<li><p>initializes an encoder network to use in the supervised network using the <code class="docutils literal notranslate"><span class="pre">models.EncoderCore</span></code> class,</p></li>
<li><p>sets a proposed number of epochs to use when training the classifier and encoder (<code class="docutils literal notranslate"><span class="pre">num_epochs=10</span></code>).</p></li>
</ul>
<p><strong>Exercise:</strong> Train a classifier, along with the encoder, to classify the input images according to shape, using <code class="docutils literal notranslate"><span class="pre">models.train_classifier()</span></code>. How does it perform?</p>
<p><strong>Hints</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">models.train_classifier()</span></code>:</p>
<ul>
<li><p>is introduced in <strong>Interactive Demo 1.2.1</strong>.</p></li>
<li><p>takes <code class="docutils literal notranslate"><span class="pre">freeze_features</span></code> as an input argument:</p>
<ul>
<li><p>if set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the encoder is frozen, and so only the classifier layer is trained.</p></li>
<li><p>if set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the encoder is <strong>not</strong> frozen, and is trained along with the classifier layer.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_supervised_encoder</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
  <span class="c1"># call this before any dataset/network initializing or training,</span>
  <span class="c1"># to ensure reproducibility</span>
  <span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

  <span class="c1"># initialize a core encoder network on which the classifier will be added</span>
  <span class="n">supervised_encoder</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">EncoderCore</span><span class="p">()</span>

  <span class="c1">#################################################</span>
  <span class="c1"># Fill in missing code below (...),</span>
  <span class="c1"># then remove or comment the line below to test your implementation</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Exercise: Train a supervised encoder and classifier."</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># EXERCISE: Train an encoder and classifier on the images, using models.train_classifier()</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Training a supervised encoder and classifier..."</span><span class="p">)</span>
  <span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">train_classifier</span><span class="p">(</span>
      <span class="n">encoder</span><span class="o">=...</span><span class="p">,</span>
      <span class="n">dataset</span><span class="o">=...</span><span class="p">,</span>
      <span class="n">train_sampler</span><span class="o">=...</span><span class="p">,</span>
      <span class="n">test_sampler</span><span class="o">=...</span><span class="p">,</span>
      <span class="n">freeze_features</span><span class="o">=...</span><span class="p">,</span>
      <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
      <span class="n">verbose</span><span class="o">=...</span> <span class="c1"># print results</span>
      <span class="p">)</span>

  <span class="k">return</span> <span class="n">supervised_encoder</span>


<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Proposed number of training epochs</span>
<span class="c1">## Uncomment below to test your function</span>
<span class="c1"># supervised_encoder = train_supervised_encoder(num_epochs=num_epochs, seed=SEED)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_8475186f.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Network</span> <span class="n">performance</span> <span class="n">after</span> <span class="mi">10</span> <span class="n">encoder</span> <span class="ow">and</span> <span class="n">classifier</span> <span class="n">training</span> <span class="n">epochs</span> <span class="p">(</span><span class="n">chance</span><span class="p">:</span> <span class="mf">33.33</span><span class="o">%</span><span class="p">):</span>
    <span class="n">Training</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">100.00</span><span class="o">%</span>
    <span class="n">Testing</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">98.38</span><span class="o">%</span>
</pre></div>
</div>
<p>When the classifier is trained with an encoder network, however, it achieves very high classification accuracy (98.72%) on the test set, after only 10 training epochs.</p>
<p><b>Shape classification results using different feature encoders:</b></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><em>Chance</em></p></th>
<th class="head"><p></p></th>
<th class="head"><p>None (raw data)</p></th>
<th class="head"><p>Supervised</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>33.33%</em></p></td>
<td><p></p></td>
<td><p>39.55%</p></td>
<td><p>98.38%</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-supervised-learning-induces-invariant-representations">
<h1>Section 2: Supervised learning induces invariant representations<a class="headerlink" href="#section-2-supervised-learning-induces-invariant-representations" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-2-supervised-learning-and-invariance">
<h2>Video 2: Supervised Learning and Invariance<a class="headerlink" href="#video-2-supervised-learning-and-invariance" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "1ca065e514544aa5b8582199e6a4f8a4"}
</script></div>
</div>
</div>
<div class="section" id="section-2-1-examining-representational-similarity-matrices-rsms">
<h2>Section 2.1: Examining Representational Similarity Matrices (RSMs)<a class="headerlink" href="#section-2-1-examining-representational-similarity-matrices-rsms" title="Permalink to this headline">¶</a></h2>
<p>To examine the representations learned by the encoder network, we use <strong>Representational Similarity Matrices (RSMs)</strong>. In these matrices, the similarity between the encoder’s representations of each possible pair of images is plotted to reveal overall structure in representation space.</p>
<p><em><strong>Note on cosine similarity:</strong> Here, we use cosine similarity as a measure of representational similarity. Cosine similarity measures the angle between 2 vectors, and can be thought of as their normalized dot product.</em></p>
<div class="section" id="coding-exercise-2-1-1-complete-a-function-that-calculates-rsms">
<h3>Coding Exercise 2.1.1: Complete a function that calculates RSMs.<a class="headerlink" href="#coding-exercise-2-1-1-complete-a-function-that-calculates-rsms" title="Permalink to this headline">¶</a></h3>
<p>The following code:</p>
<ul class="simple">
<li><p>lays out the skeleton of a function <code class="docutils literal notranslate"><span class="pre">custom_torch_RSM_fct()</span></code> which calculates an RSM from features,</p></li>
<li><p>tests the custom function against the solution implementation.</p></li>
</ul>
<p><strong>Exercise:</strong> Complete the <code class="docutils literal notranslate"><span class="pre">custom_torch_RSM_fct()</span></code> implementation.</p>
<p><strong>Hints</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">custom_torch_RSM_fct()</span></code>:</p>
<ul>
<li><p>takes 1 input argument:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">features</span></code> (2D torch Tensor): feature matrix (nbr items x nbr features)</p></li>
</ul>
</li>
<li><p>returns 1 output:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">rsm</span></code> (2D torch Tensor): similarity matrix (nbr items x nbr items)</p></li>
</ul>
</li>
<li><p>uses <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.cosine_similarity()</span></code>.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.nn.functional.cosine_similarity()</span></code>:</p>
<ul>
<li><p>takes 3 arguments, in order:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">x1</span></code> (torch Tensor),</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x2</span></code> (torch Tensor),</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dim</span></code> (int)</p></li>
</ul>
</li>
<li><p>returns the similarity between <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">x2</span></code> along dimension <code class="docutils literal notranslate"><span class="pre">dim</span></code>.</p></li>
</ul>
</li>
</ul>
<p><strong>Detailed hint</strong>:</p>
<ul class="simple">
<li><p>To use <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.cosine_similarity()</span></code> to measure the similarity of <code class="docutils literal notranslate"><span class="pre">features</span></code> to <strong>itself</strong> for each possible <strong>pair of items</strong>:</p>
<ul>
<li><p>Pass 2 versions of <code class="docutils literal notranslate"><span class="pre">features</span></code> as <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">x2</span></code>, respectively.</p></li>
<li><p>Ensure that for <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">x2</span></code>, the <strong>features dimension is at the same position</strong> , and specify that dimension with <code class="docutils literal notranslate"><span class="pre">dim</span></code>.</p></li>
<li><p>To obtain the similarity between each possible pair of items, ensure that for <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">x2</span></code>, the <strong>items dimensions are orthogonal</strong> to one another (i.e., at different positions).</p></li>
<li><p>Don’t forget that to achieve this, singleton dimensions (i.e., dimensions of length 1) can be used.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">custom_torch_RSM_fct</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
  <span class="sd">"""</span>
<span class="sd">  custom_torch_RSM_fct(features)</span>

<span class="sd">  Custom function to calculate representational similarity matrix (RSM) of a feature</span>
<span class="sd">  matrix using pairwise cosine similarity.</span>

<span class="sd">  Complete the function below given the specific guidelines.</span>
<span class="sd">  Uses torch.nn.functional.cosine_similarity()</span>

<span class="sd">  Required args:</span>
<span class="sd">  - features (2D torch Tensor): feature matrix (nbr items x nbr features)</span>

<span class="sd">  Returns:</span>
<span class="sd">  - rsm (2D torch Tensor): similarity matrix</span>
<span class="sd">      (nbr items x nbr items)</span>
<span class="sd">  """</span>

  <span class="n">num_items</span><span class="p">,</span> <span class="n">num_features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span>

  <span class="c1">#################################################</span>
  <span class="c1"># Fill in missing code below (...),</span>
  <span class="c1"># then remove or comment the line below to test your function</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Exercise: Implement RSM calculation."</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># EXERCISE: Implement RSM calculation</span>
  <span class="n">rsm</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">rsm</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_items</span><span class="p">,</span> <span class="n">num_items</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">"RSM should be of shape (</span><span class="si">{</span><span class="n">num_items</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">num_items</span><span class="si">}</span><span class="s2">)"</span>
        <span class="p">)</span>

  <span class="k">return</span> <span class="n">rsm</span>


<span class="c1">## Test implementation by comparing output to solution implementation</span>
<span class="c1"># test_custom_torch_RSM_fct(custom_torch_RSM_fct)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_c6d022fc.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">custom_torch_RSM_fct</span><span class="p">()</span> <span class="ow">is</span> <span class="n">correctly</span> <span class="n">implemented</span><span class="o">.</span>
</pre></div>
</div>
</div>
<div class="section" id="interactive-demo-2-1-1-plotting-the-supervised-network-encoder-rsm-along-different-latent-dimensions">
<h3>Interactive Demo 2.1.1: Plotting the supervised network encoder RSM along different latent dimensions.<a class="headerlink" href="#interactive-demo-2-1-1-plotting-the-supervised-network-encoder-rsm-along-different-latent-dimensions" title="Permalink to this headline">¶</a></h3>
<p>In this demo, we calculate an RSM for representations of the test set images generated by the supervised network encoder.</p>
<p>The following code:</p>
<ul class="simple">
<li><p>calculates and plots the RSM for the test set, with rows and columns sorted by whichever latent dimension is specified (e.g., <code class="docutils literal notranslate"><span class="pre">sorting_latent="shape"</span></code>) using <code class="docutils literal notranslate"><span class="pre">models.plot_model_RSMs()</span></code>.</p></li>
</ul>
<p><strong>Interactive Demo:</strong> In the current example, the rows and columns of the RSM are organized along the <code class="docutils literal notranslate"><span class="pre">shape</span></code> latent dimension. Try organizing them along one of the other latent dimensions (<code class="docutils literal notranslate"><span class="pre">"scale"</span></code>, <code class="docutils literal notranslate"><span class="pre">"orientation"</span></code>, <code class="docutils literal notranslate"><span class="pre">"posX"</span></code> or <code class="docutils literal notranslate"><span class="pre">"posY"</span></code>) to see whether different patterns emerge. (The original setting is <code class="docutils literal notranslate"><span class="pre">sorting_latent="shape"</span></code>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorting_latent</span> <span class="o">=</span> <span class="s2">"shape"</span> <span class="c1"># DEMO: Try sorting by different latent dimensions</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Plotting RSMs..."</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">plot_model_RSMs</span><span class="p">(</span>
    <span class="n">encoders</span><span class="o">=</span><span class="p">[</span><span class="n">supervised_encoder</span><span class="p">],</span> <span class="c1"># we pass the trained supervised_encoder</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_torchdataset</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span> <span class="c1"># we want to see the representations on the held out test set</span>
    <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s2">"Supervised network encoder RSM"</span><span class="p">],</span> <span class="c1"># plot title</span>
    <span class="n">sorting_latent</span><span class="o">=</span><span class="n">sorting_latent</span><span class="p">,</span>
    <span class="n">RSM_fct</span><span class="o">=</span><span class="n">custom_torch_RSM_fct</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>

</div>
</div>
<div class="section" id="discussion-2-1-1-what-patterns-do-the-rsms-reveal-about-how-the-encoder-represents-different-images">
<h3>Discussion 2.1.1: What patterns do the RSMs reveal about how the encoder represents different images?<a class="headerlink" href="#discussion-2-1-1-what-patterns-do-the-rsms-reveal-about-how-the-encoder-represents-different-images" title="Permalink to this headline">¶</a></h3>
<p><strong>A.</strong> What does the yellow (maximal similarity color) diagonal, going from the top left to the bottom right, correspond to?<br/>
<strong>B.</strong> What pattern can be observed when comparing RSM values for pairs of images that share a similar latent value (e.g., 2 heart images) vs pairs of images that do not (e.g., a heart and a square image)?<br/>
<strong>C.</strong> Do some shapes appear to be encoded more similarly than others?<br/>
<strong>D.</strong> Do some latent dimensions show clearer RSM patterns than others? Why might that be so?</p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_f276340a.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="supporting-images-for-discussion-response-examples-for-2-1-1">
<h3>Supporting images for Discussion response examples for 2.1.1<a class="headerlink" href="#supporting-images-for-discussion-response-examples-for-2-1-1" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown ### Supporting images for Discussion response examples for 2.1.1</span>

<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span> <span class="s2">"images"</span><span class="p">,</span> <span class="s2">"rsms_supervised_encoder_10ep_bs1000_seed2021.png"</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1200</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D1_Tutorial1_66_0.png" src="../../../_images/W3D1_Tutorial1_66_0.png"/>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-random-projections-dont-work-as-well">
<h1>Section 3: Random projections don’t work as well<a class="headerlink" href="#section-3-random-projections-dont-work-as-well" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-3-random-representations">
<h2>Video 3: Random Representations<a class="headerlink" href="#video-3-random-representations" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "36a8574754d144ec8add8797fc8b91d4"}
</script></div>
</div>
</div>
<div class="section" id="section-3-1-examining-rsms-of-a-random-encoder">
<h2>Section 3.1: Examining RSMs of a random encoder<a class="headerlink" href="#section-3-1-examining-rsms-of-a-random-encoder" title="Permalink to this headline">¶</a></h2>
<p>To determine whether the patterns observed in the RSMs of the supervised network encoder are trivial, we investigate whether they also emerge from the <strong>random projections of an untrained encoder</strong>.</p>
<div class="section" id="coding-exercise-3-1-1-plotting-a-random-network-encoder-rsm-along-different-latent-dimensions">
<h3>Coding Exercise 3.1.1: Plotting a random network encoder RSM along different latent dimensions.<a class="headerlink" href="#coding-exercise-3-1-1-plotting-a-random-network-encoder-rsm-along-different-latent-dimensions" title="Permalink to this headline">¶</a></h3>
<p>In this exercise, we repeat the same analysis as in <strong>Section 2.1</strong>, but with a random encoder.</p>
<p>The following code:</p>
<ul class="simple">
<li><p>initializes an encoder network to use in the random network using the <code class="docutils literal notranslate"><span class="pre">models.EncoderCore</span></code> class,</p></li>
<li><p>proposes a latent dimension along which to sort the rows and columns (<code class="docutils literal notranslate"><span class="pre">sorting_latent</span> <span class="pre">=</span> <span class="pre">"shape"</span></code>).</p></li>
</ul>
<p><strong>Exercise:</strong></p>
<ul class="simple">
<li><p>Visualize the RSMs for the supervised and random network encoders, using <code class="docutils literal notranslate"><span class="pre">models.plot_model_RSMs()</span></code>.</p></li>
<li><p>Visualize the RSMs, organized along different latent dimensions (<code class="docutils literal notranslate"><span class="pre">"scale"</span></code>, <code class="docutils literal notranslate"><span class="pre">"orientation"</span></code>, <code class="docutils literal notranslate"><span class="pre">"posX"</span></code> or <code class="docutils literal notranslate"><span class="pre">"posY"</span></code>), and compare the patterns observed for the supervised versus the random encoder network.</p></li>
</ul>
<p><strong>Hint</strong>: <code class="docutils literal notranslate"><span class="pre">models.plot_model_RSMs()</span></code> is introduced in <strong>Interactive Demo 2.1.1</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_rsms</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
  <span class="c1"># call this before any dataset/network initializing or training,</span>
  <span class="c1"># to ensure reproducibility</span>
  <span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

  <span class="c1"># initialize a core encoder network that will not get trained</span>
  <span class="n">random_encoder</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">EncoderCore</span><span class="p">()</span>

  <span class="c1"># EXERCISE: Try sorting by different latent dimensions</span>
  <span class="n">sorting_latent</span> <span class="o">=</span> <span class="s2">"shape"</span>

  <span class="c1">#################################################</span>
  <span class="c1"># Fill in missing code below (...),</span>
  <span class="c1"># then remove or comment the line below to test your implementation</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Exercise: Plot RSMs."</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># EXERCISE: Plot RSMs</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Plotting RSMs..."</span><span class="p">)</span>
  <span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">plot_model_RSMs</span><span class="p">(</span>
      <span class="n">encoders</span><span class="o">=</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>  <span class="c1"># we pass both encoders</span>
      <span class="n">dataset</span><span class="o">=...</span><span class="p">,</span>
      <span class="n">sampler</span><span class="o">=...</span><span class="p">,</span>  <span class="c1"># we want to see the representations on the held out test set</span>
      <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s2">"Supervised network encoder RSM"</span><span class="p">,</span>
              <span class="s2">"Random network encoder RSM"</span><span class="p">],</span>  <span class="c1"># plot titles</span>
      <span class="n">sorting_latent</span><span class="o">=</span><span class="n">sorting_latent</span><span class="p">,</span>
      <span class="p">)</span>

  <span class="k">return</span> <span class="n">random_encoder</span>


<span class="c1">## Uncomment below to test your function</span>
<span class="c1"># random_encoder = plot_rsms(seed=SEED)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_73b1d41e.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_73b1d41e_1.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_73b1d41e_1.png" style="width: 1509.0px; height: 752.0px;"/></a>
</div>
<div class="section" id="discussion-3-1-1-what-does-comparing-these-rsms-reveal-about-the-potential-value-of-trained-versus-random-encoder-representations">
<h3>Discussion 3.1.1: What does comparing these RSMs reveal about the potential value of trained versus random encoder representations?<a class="headerlink" href="#discussion-3-1-1-what-does-comparing-these-rsms-reveal-about-the-potential-value-of-trained-versus-random-encoder-representations" title="Permalink to this headline">¶</a></h3>
<p><strong>A.</strong> What patterns, if any, are visible in the random network encoder RSM?<br/>
<strong>B.</strong> Which encoder network is most likely to produce meaningful representations?</p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_8b06362b.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="supporting-images-for-discussion-response-examples-for-3-1-1-all-random-encoder-rsms">
<h3>Supporting images for Discussion response examples for 3.1.1: All random encoder RSMs<a class="headerlink" href="#supporting-images-for-discussion-response-examples-for-3-1-1-all-random-encoder-rsms" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown ### Supporting images for Discussion response examples for 3.1.1: All random encoder RSMs</span>

<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span> <span class="s2">"images"</span><span class="p">,</span> <span class="s2">"rsms_random_encoder_0ep_bs0_seed2021.png"</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D1_Tutorial1_77_0.png" src="../../../_images/W3D1_Tutorial1_77_0.png"/>
</div>
</div>
</div>
<div class="section" id="coding-exercise-3-1-2-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-a-random-network-encoder">
<h3>Coding Exercise 3.1.2: Evaluating the classification performance of a logistic regression trained on the representations produced by a random network encoder.<a class="headerlink" href="#coding-exercise-3-1-2-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-a-random-network-encoder" title="Permalink to this headline">¶</a></h3>
<p>In this exercise, we repeat a similar analysis to <strong>Section 1.2.2</strong>, but with the random encoder network. Importantly, this time, the encoder parameters must stay frozen during training by setting <code class="docutils literal notranslate"><span class="pre">freeze_features=True</span></code>. Instead of being provided ahead of time a suggestion for a reasonable number of training epochs, we use the training loss array to select a good value.</p>
<p>The following code:</p>
<ul class="simple">
<li><p>trains a logistic regression on top of the random encoder network to classify images based on shape, and assesses its performance on the test set images using <code class="docutils literal notranslate"><span class="pre">models.train_classifier()</span></code> with <code class="docutils literal notranslate"><span class="pre">freeze_features=True</span></code> to ensure that the encoder is <strong>not</strong> trained, and only the classifier is.</p></li>
</ul>
<p><strong>Exercise:</strong></p>
<ul>
<li><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Set a number of epochs for which to train the classifier.  
</pre></div>
</div>
</li>
<li><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Plot the training loss array (`random_loss_array`, i.e. training loss at each epoch) returned when training the model.  
</pre></div>
</div>
</li>
<li><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Rerun the classifier if more training epochs are needed based on the progression of the training loss.
</pre></div>
</div>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_loss</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
  <span class="c1"># call this before any dataset/network initializing or training,</span>
  <span class="c1"># to ensure reproducibility</span>
  <span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

  <span class="c1"># train classifier on the randomly encoded images</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Training a classifier on the random encoder representations..."</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">random_loss_array</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">train_classifier</span><span class="p">(</span>
      <span class="n">encoder</span><span class="o">=</span><span class="n">random_encoder</span><span class="p">,</span>
      <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_torchdataset</span><span class="p">,</span>
      <span class="n">train_sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
      <span class="n">test_sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span>
      <span class="n">freeze_features</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># keep the encoder frozen while training the classifier</span>
      <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
      <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># print results</span>
      <span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># Fill in missing code below (...),</span>
  <span class="c1"># then remove or comment the line below to test your implementation</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Exercise: Plot loss array."</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># EXERCISE: Plot the loss array</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">random_loss_array</span>


<span class="c1"># EXERCISE: Set a reasonable number of training epochs</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">25</span>
<span class="c1">## Uncomment below to test your plot</span>
<span class="c1"># random_loss_array = plot_loss(num_epochs=num_epochs, seed=SEED)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_3f39cb7a.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_3f39cb7a_3.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_3f39cb7a_3.png" style="width: 1116.0px; height: 827.0px;"/></a>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Network</span> <span class="n">performance</span> <span class="n">after</span> <span class="mi">25</span> <span class="n">classifier</span> <span class="n">training</span> <span class="n">epochs</span> <span class="p">(</span><span class="n">chance</span><span class="p">:</span> <span class="mf">33.33</span><span class="o">%</span><span class="p">):</span>
    <span class="n">Training</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">46.02</span><span class="o">%</span>
    <span class="n">Testing</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">44.67</span><span class="o">%</span>
</pre></div>
</div>
<p>The network loss training is fairly stable by 25 epochs, at which point the classifier performs at 44.67% accuracy on the test dataset.</p>
<p><b>Shape classification results using different feature encoders:</b></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><em>Chance</em></p></th>
<th class="head"><p></p></th>
<th class="head"><p>None (raw data)</p></th>
<th class="head"><p>Supervised</p></th>
<th class="head"><p>Random</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>33.33%</em></p></td>
<td><p></p></td>
<td><p>39.55%</p></td>
<td><p>98.38%</p></td>
<td><p>44.67%</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="discussion-3-1-2-what-can-we-conclude-about-the-potential-consequences-of-using-random-projections-with-a-dataset-like-dsprites">
<h3>Discussion 3.1.2: What can we conclude about the potential consequences of using random projections with a dataset like dSprites?<a class="headerlink" href="#discussion-3-1-2-what-can-we-conclude-about-the-potential-consequences-of-using-random-projections-with-a-dataset-like-dsprites" title="Permalink to this headline">¶</a></h3>
<p><strong>A.</strong> How does the classifier performance compare to the classifier trained directly on the images?<br/>
<strong>B.</strong> How does the classifier performance compare to the classifier trained along with the encoder (supervised encoder)?<br/>
<strong>C.</strong> What explains these different performances?</p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_28e6be8a.py"><em>Click for solution</em></a></p>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-4-generative-approaches-to-representation-learning-can-fail">
<h1>Section 4: Generative approaches to representation learning can fail<a class="headerlink" href="#section-4-generative-approaches-to-representation-learning-can-fail" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-4-generative-models">
<h2>Video 4: Generative models<a class="headerlink" href="#video-4-generative-models" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "edac3a239f514c349e1cca8fa42914a1"}
</script></div>
</div>
</div>
<div class="section" id="section-4-1-examining-the-rsms-of-a-variational-autoencoder">
<h2>Section 4.1: Examining the RSMs of a Variational Autoencoder<a class="headerlink" href="#section-4-1-examining-the-rsms-of-a-variational-autoencoder" title="Permalink to this headline">¶</a></h2>
<p>We next ask what kind of representations a network can learn in the absence of labelled data. We first look at a <strong>generative model</strong>, namely the <strong>Variational Autoencoder (VAE)</strong>.</p>
<p>Given that generative models typically require more training than supervised models, instead of pre-training a network here, we will load one that was <strong>pre-trained for 300 epochs</strong>. Importantly, the <strong>encoder shares the same architecture</strong> as the one used for the supervised and random examples above.</p>
<p>The following code:</p>
<ul class="simple">
<li><p>loads the parameters of a full VAE network (encoder and decoder) pre-trained on the generative task of reconstructing the input images, under the Kullback–Leibler divergence (KLD) minimization constraint over the latent space that characterizes VAEs, using <code class="docutils literal notranslate"><span class="pre">load.load_encoder()</span></code> and <code class="docutils literal notranslate"><span class="pre">load.load_decoder()</span></code>,</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="c1"># Load VAE encoder and decoder pre-trained on the reconstruction and KLD tasks</span>
<span class="n">vae_encoder</span> <span class="o">=</span> <span class="n">load</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s2">"vae"</span><span class="p">)</span>
<span class="n">vae_decoder</span> <span class="o">=</span> <span class="n">load</span><span class="o">.</span><span class="n">load_vae_decoder</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
Loading VAE encoder from 'neuromatch_ssl_tutorial/checkpoints/vae_encoder_300ep_bs500_seed2021.pth'.
    =&gt; trained for 300 epochs (batch_size of 500) on the full dSprites subset dataset.
Loading VAE decoder from 'neuromatch_ssl_tutorial/checkpoints/vae_decoder_300ep_bs500_seed2021.pth'.
   =&gt; trained for 300 epochs (batch_size of 500) on the full dSprites subset dataset.
</pre></div>
</div>
</div>
</div>
<div class="section" id="interactive-demo-4-1-1-plotting-example-reconstructions-using-the-pre-trained-vae-encoder-and-decoder">
<h3>Interactive Demo 4.1.1: Plotting example reconstructions using the pre-trained VAE encoder and decoder.<a class="headerlink" href="#interactive-demo-4-1-1-plotting-example-reconstructions-using-the-pre-trained-vae-encoder-and-decoder" title="Permalink to this headline">¶</a></h3>
<p>In this demo, we sample images from the test set, and take a look at the quality of the reconstructions using <code class="docutils literal notranslate"><span class="pre">models.plot_vae_reconstructions()</span></code>.</p>
<p><strong>Interactive Demo:</strong> Try plotting different images from the test dataset by selecting different <code class="docutils literal notranslate"><span class="pre">test_sampler.indices</span></code> values. (Original setting is <code class="docutils literal notranslate"><span class="pre">indices=test_sampler.indices[:10]</span></code>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">models</span><span class="o">.</span><span class="n">plot_vae_reconstructions</span><span class="p">(</span>
    <span class="n">vae_encoder</span><span class="p">,</span>  <span class="c1"># pre-trained encoder</span>
    <span class="n">vae_decoder</span><span class="p">,</span>  <span class="c1"># pre-trained decoder</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_torchdataset</span><span class="p">,</span>
    <span class="n">indices</span><span class="o">=</span><span class="n">test_sampler</span><span class="o">.</span><span class="n">indices</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span>  <span class="c1"># DEMO: select different indices to plot from the test set</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"VAE test set image reconstructions"</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D1_Tutorial1_92_0.png" src="../../../_images/W3D1_Tutorial1_92_0.png"/>
</div>
</div>
</div>
<div class="section" id="discussion-4-1-1-how-does-the-vae-perform-on-the-reconstruction-task">
<h3>Discussion 4.1.1: How does the VAE perform on the reconstruction task?<a class="headerlink" href="#discussion-4-1-1-how-does-the-vae-perform-on-the-reconstruction-task" title="Permalink to this headline">¶</a></h3>
<p><strong>A.</strong> Which latent features does the network appear to preserve well, and which does it preserve less well?<br/>
<strong>B.</strong> Based on the reconstruction performance, what do you expect to see in the different RSMs?</p>
<p><em><strong>Note on reconstruction quality:</strong> This VAE network uses a basic VAE loss with a convolutional encoder (our core encoder network), and a deconvolutional decoder. This can lead to some blurriness in the reconstructed shapes which a more sophisticated VAE could overcome.</em></p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_b1fd9be0.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="interactive-demo-4-1-2-visualizing-the-vae-encoder-rsms-organized-along-different-latent-dimensions">
<h3>Interactive Demo 4.1.2: Visualizing the VAE encoder RSMs, organized along different latent dimensions.<a class="headerlink" href="#interactive-demo-4-1-2-visualizing-the-vae-encoder-rsms-organized-along-different-latent-dimensions" title="Permalink to this headline">¶</a></h3>
<p>We will now compare the pre-trained VAE encoder network RSM to the previously generated encoder RSMs.</p>
<p><strong>Interactive Demo:</strong> Visualize the RSMs, organized along different latent dimensions (<code class="docutils literal notranslate"><span class="pre">"scale"</span></code>, <code class="docutils literal notranslate"><span class="pre">"orientation"</span></code>, <code class="docutils literal notranslate"><span class="pre">"posX"</span></code> or <code class="docutils literal notranslate"><span class="pre">"posY"</span></code>), and compare the patterns observed for the different encoder networks. (The original setting is <code class="docutils literal notranslate"><span class="pre">sorting_latent="shape"</span></code>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorting_latent</span> <span class="o">=</span> <span class="s2">"shape"</span> <span class="c1"># DEMO: Try sorting by different latent dimensions</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Plotting RSMs..."</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">plot_model_RSMs</span><span class="p">(</span>
    <span class="n">encoders</span><span class="o">=</span><span class="p">[</span><span class="n">supervised_encoder</span><span class="p">,</span> <span class="n">random_encoder</span><span class="p">,</span> <span class="n">vae_encoder</span><span class="p">],</span> <span class="c1"># we pass all three encoders</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_torchdataset</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span> <span class="c1"># we want to see the representations on the held out test set</span>
    <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s2">"Supervised network encoder RSM"</span><span class="p">,</span> <span class="s2">"Random network encoder RSM"</span><span class="p">,</span>
            <span class="s2">"VAE network encoder RSM"</span><span class="p">],</span> <span class="c1"># plot titles</span>
    <span class="n">sorting_latent</span><span class="o">=</span><span class="n">sorting_latent</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Plotting RSMs...
</pre></div>
</div>
<img alt="../../../_images/W3D1_Tutorial1_96_1.png" src="../../../_images/W3D1_Tutorial1_96_1.png"/>
</div>
</div>
</div>
<div class="section" id="discussion-4-1-2-what-can-we-conclude-about-the-the-ability-of-generative-models-like-vaes-to-construct-a-meaningful-representation-space">
<h3>Discussion 4.1.2: What can we conclude about the the ability of generative models like VAEs to construct a meaningful representation space?<a class="headerlink" href="#discussion-4-1-2-what-can-we-conclude-about-the-the-ability-of-generative-models-like-vaes-to-construct-a-meaningful-representation-space" title="Permalink to this headline">¶</a></h3>
<p><strong>A.</strong> What structure can be observed in the pre-trained VAE encoder RSMs when sorted along the different latent dimensions, and what does that suggest about the feature space learned by the VAE encoder?<br/>
<strong>B.</strong> How do the pre-trained VAE encoder RSMs compare to the supervised and random encoder network RSMs?<br/>
<strong>C.</strong> What explains these different RSMs?<br/>
<strong>D.</strong> How well will the pre-trained VAE encoder likely perform on the shape classification task, as compared to the other encoder networks?<br/>
<strong>E.</strong> Might the pre-trained VAE encoder be better suited to predicting a different latent dimension?</p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_b7aba28a.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="supporting-images-for-discussion-response-examples-for-4-1-2-all-vae-encoder-rsms">
<h3>Supporting images for Discussion response examples for 4.1.2: All VAE encoder RSMs<a class="headerlink" href="#supporting-images-for-discussion-response-examples-for-4-1-2-all-vae-encoder-rsms" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown ### Supporting images for Discussion response examples for 4.1.2: All VAE encoder RSMs</span>

<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span> <span class="s2">"images"</span><span class="p">,</span> <span class="s2">"rsms_vae_encoder_300ep_bs500_seed2021.png"</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W3D1_Tutorial1_100_0.png" src="../../../_images/W3D1_Tutorial1_100_0.png"/>
</div>
</div>
</div>
<div class="section" id="coding-exercise-4-1-2-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-the-pre-trained-vae-network-encoder">
<h3>Coding Exercise 4.1.2: Evaluating the classification performance of a logistic regression trained on the representations produced by the pre-trained VAE network encoder.<a class="headerlink" href="#coding-exercise-4-1-2-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-the-pre-trained-vae-network-encoder" title="Permalink to this headline">¶</a></h3>
<p>For the pre-trained VAE encoder, as the encoder parameters have already been trained, they should be kept frozen while the classifier is trained by setting <code class="docutils literal notranslate"><span class="pre">freeze_features=True</span></code>.</p>
<p><strong>Exercise:</strong></p>
<ul>
<li><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Set a number of epochs for which to train the classifier.  
</pre></div>
</div>
</li>
<li><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Train a classifier, along with the encoder, to classify the input images according to shape, using `models.train_classifier()`.
</pre></div>
</div>
</li>
<li><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Plot the loss array returned when training the model, and update the number of training epochs, if needed. 
</pre></div>
</div>
</li>
</ul>
<p><strong>Hint</strong>: <code class="docutils literal notranslate"><span class="pre">models.train_classifier()</span></code> is introduced in <strong>Interactive Demo 1.2.1</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">vae_train_loss</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
  <span class="c1"># call this before any dataset/network initializing or training,</span>
  <span class="c1"># to ensure reproducibility</span>
  <span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># Fill in missing code below (...),</span>
  <span class="c1"># then remove or comment the line below to test your implementation</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Exercise: Train a classifer on the pre-trained VAE encoder representations."</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># EXERCISE: Train an encoder and classifier on the images, using models.train_classifier()</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Training a classifier on the pre-trained VAE encoder representations..."</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">vae_loss_array</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">train_classifier</span><span class="p">(</span>
      <span class="n">encoder</span><span class="o">=...</span><span class="p">,</span>
      <span class="n">dataset</span><span class="o">=...</span><span class="p">,</span>
      <span class="n">train_sampler</span><span class="o">=...</span><span class="p">,</span>
      <span class="n">test_sampler</span><span class="o">=...</span><span class="p">,</span>
      <span class="n">freeze_features</span><span class="o">=...</span><span class="p">,</span> <span class="c1"># keep the encoder frozen while training the classifier</span>
      <span class="n">num_epochs</span><span class="o">=...</span><span class="p">,</span>
      <span class="n">verbose</span><span class="o">=...</span> <span class="c1"># print results</span>
      <span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># Fill in missing code below (...),</span>
  <span class="c1"># then remove or comment the line below to test your implementation</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Exercise: Plot the VAE classifier training loss."</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># EXERCISE: Plot the VAE classifier training loss.</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">vae_loss_array</span>

<span class="c1"># Set a reasonable number of training epochs</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">25</span>
<span class="c1">## Uncomment below to test your function</span>
<span class="c1"># vae_loss_array = vae_train_loss(num_epochs=num_epochs, seed=SEED)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_bfa51e5a.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_bfa51e5a_3.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_bfa51e5a_3.png" style="width: 1116.0px; height: 827.0px;"/></a>
<p>The network loss training is fairly stable by 25 epochs, at which point the classifier performs at 45.75% accuracy on the test dataset.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Network</span> <span class="n">performance</span> <span class="n">after</span> <span class="mi">25</span> <span class="n">classifier</span> <span class="n">training</span> <span class="n">epochs</span> <span class="p">(</span><span class="n">chance</span><span class="p">:</span> <span class="mf">33.33</span><span class="o">%</span><span class="p">):</span>
    <span class="n">Training</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">46.48</span><span class="o">%</span>
    <span class="n">Testing</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">45.75</span><span class="o">%</span>
</pre></div>
</div>
<p><b>Shape classification results using different feature encoders:</b></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><em>Chance</em></p></th>
<th class="head"><p></p></th>
<th class="head"><p>None (raw data)</p></th>
<th class="head"><p>Supervised</p></th>
<th class="head"><p>Random</p></th>
<th class="head"><p>VAE</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>33.33%</em></p></td>
<td><p></p></td>
<td><p>39.55%</p></td>
<td><p>98.38%</p></td>
<td><p>44.67%</p></td>
<td><p>45.75%</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-5-the-modern-approach-to-self-supervised-training-for-invariance">
<h1>Section 5: The modern approach to self-supervised training for invariance<a class="headerlink" href="#section-5-the-modern-approach-to-self-supervised-training-for-invariance" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-5-modern-approach-in-self-supervised-learning">
<h2>Video 5: Modern Approach in Self-supervised Learning<a class="headerlink" href="#video-5-modern-approach-in-self-supervised-learning" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "48765f32078c4d36afd8033bb7cbccb6"}
</script></div>
</div>
</div>
<div class="section" id="section-5-1-examining-different-options-for-learning-invariant-representations">
<h2>Section 5.1: Examining different options for learning invariant representations.<a class="headerlink" href="#section-5-1-examining-different-options-for-learning-invariant-representations" title="Permalink to this headline">¶</a></h2>
<p>We now take a look at a few options for learning invariant shape representations for a dataset such as dSprites.</p>
<div class="section" id="interactive-demo-5-1-1-visualizing-a-few-different-image-transformations-available-that-could-be-used-to-learn-invariance">
<h3>Interactive Demo 5.1.1: Visualizing a few different image transformations available that could be used to learn invariance.<a class="headerlink" href="#interactive-demo-5-1-1-visualizing-a-few-different-image-transformations-available-that-could-be-used-to-learn-invariance" title="Permalink to this headline">¶</a></h3>
<p>The following code:</p>
<ul class="simple">
<li><p>initializes a set of transforms called <code class="docutils literal notranslate"><span class="pre">invariance_transforms</span></code> using the <code class="docutils literal notranslate"><span class="pre">torchvision.transforms.RandomAffine</span></code> class,</p></li>
<li><p>collects the dSprites dataset into a torch dataset <code class="docutils literal notranslate"><span class="pre">dSprites_invariance_torchdataset</span></code> which takes the <code class="docutils literal notranslate"><span class="pre">invariance_transforms</span></code> as input and deploys the transforms when it is called,</p></li>
<li><p>shows a few examples of images and their transformed versions using the <code class="docutils literal notranslate"><span class="pre">data.dSpritesTorchDataset</span></code> <code class="docutils literal notranslate"><span class="pre">show_images()</span></code> method.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">torchvision.transforms.RandomAffine</span></code> class enables us to predetermine which types and ranges of transforms will be sampled from when transforming the images, by setting the following arguments:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">degrees</span></code>: absolute maximum number of degrees to rotate</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">translate</span></code>: absolute maximum proportion of width to shift in x, and of height to shift in y</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scale</span></code>: minimum to maximum scaling factor</p></li>
</ul>
<p><strong>Interactive Demo:</strong> Try out a few combinations of the transformation parameters, and visualize the pairs of transformations of the same image. (The original settings are <code class="docutils literal notranslate"><span class="pre">degrees=90</span></code>, <code class="docutils literal notranslate"><span class="pre">translate=(0.2,</span> <span class="pre">0.2)</span></code>, <code class="docutils literal notranslate"><span class="pre">scale=(0.8,</span> <span class="pre">1.2)</span></code>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="c1"># DEMO: Try some random affine data augmentations combinations to apply to the images</span>
<span class="n">invariance_transforms</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">RandomAffine</span><span class="p">(</span>
    <span class="n">degrees</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span>
    <span class="n">translate</span><span class="o">=</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>  <span class="c1"># (in x, in y)</span>
    <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>   <span class="c1"># min to max scaling</span>
    <span class="p">)</span>

<span class="c1"># initialize a simclr-specific torch dataset</span>
<span class="n">dSprites_invariance_torchdataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dSpritesTorchDataset</span><span class="p">(</span>
    <span class="n">dSprites</span><span class="p">,</span>
    <span class="n">target_latent</span><span class="o">=</span><span class="s2">"shape"</span><span class="p">,</span>
    <span class="n">simclr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">simclr_transforms</span><span class="o">=</span><span class="n">invariance_transforms</span>
    <span class="p">)</span>

<span class="c1"># show a few example of pairs of image augmentations</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">dSprites_invariance_torchdataset</span><span class="o">.</span><span class="n">show_images</span><span class="p">(</span><span class="n">randst</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
</pre></div>
</div>
<img alt="../../../_images/W3D1_Tutorial1_112_1.png" src="../../../_images/W3D1_Tutorial1_112_1.png"/>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-6-how-to-train-for-invariance-to-transformations-with-a-target-network">
<h1>Section 6: How to train for invariance to transformations with a target network<a class="headerlink" href="#section-6-how-to-train-for-invariance-to-transformations-with-a-target-network" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-6-data-transformations">
<h2>Video 6: Data Transformations<a class="headerlink" href="#video-6-data-transformations" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "47d0ede3c69047dda8ec84c6e1fe7893"}
</script></div>
</div>
</div>
<div class="section" id="section-6-1-using-image-transformations-to-learn-feature-invariant-representations-in-an-ssl-network">
<h2>Section 6.1: Using image transformations to learn feature invariant representations in an SSL network.<a class="headerlink" href="#section-6-1-using-image-transformations-to-learn-feature-invariant-representations-in-an-ssl-network" title="Permalink to this headline">¶</a></h2>
<p>We will now investigate the effects of selecting certain transformations compared to others on the invariance learned by an encoder network trained with a <strong>specific type of SSL algorithm, namely SimCLR</strong>. Specifically, we will observe how pre-training an encoder network with SimCLR affects the performance of a classifier trained on the representations the network has learned.</p>
<div class="section" id="coding-exercise-6-1-1-complete-a-simclr-loss-function">
<h3>Coding Exercise 6.1.1: Complete a SimCLR loss function.<a class="headerlink" href="#coding-exercise-6-1-1-complete-a-simclr-loss-function" title="Permalink to this headline">¶</a></h3>
<p>The following code:</p>
<ul class="simple">
<li><p>lays out the skeleton of a function <code class="docutils literal notranslate"><span class="pre">custom_simclr_contrastive_loss()</span></code> which calculates the contrastive loss for a SimCLR network,</p></li>
<li><p>tests the custom function against the solution implementation,</p></li>
<li><p>trains SimCLR for a few epochs.</p></li>
</ul>
<p><strong>Exercise:</strong></p>
<ul class="simple">
<li><p>Complete the <code class="docutils literal notranslate"><span class="pre">custom_simclr_contrastive_loss()</span></code> implementation,</p></li>
<li><p>Plot the loss after training SimCLR with the custom loss function for a few epochs.</p></li>
</ul>
<p><strong>Detailed hint</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">custom_simclr_contrastive_loss()</span></code>:</p>
<ul>
<li><p>takes 2 input arguments:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">proj_feat1</span></code> (2D torch Tensor): projected features for first image augmentations (batch_size x feat_size)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">proj_feat2</span></code> (2D torch Tensor): projected features for second image augmentations (batch_size x feat_size)</p></li>
</ul>
</li>
<li><p>computes the <code class="docutils literal notranslate"><span class="pre">similarity_matrix</span></code> for all possible pairs of image augmentations.</p></li>
<li><p>identifies positive and negative sample indicators for indexing the <code class="docutils literal notranslate"><span class="pre">similarity_matrix</span></code>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">pos_sample_indicators</span></code> (2D torch Tensor): tensor indicating the positions of <strong>positive</strong> image pairs with 1s (and 0s in all other positions). (batch_size * 2 x batch_size * 2)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">neg_sample_indicators</span></code> (2D torch Tensor): tensor indicating the positions of <strong>negative</strong> image pairs with 1s (and 0s in all other positions). (batch_size * 2 x batch_size * 2)</p></li>
</ul>
</li>
<li><p>computes the 2 parts of the contrastive loss, retrieving the relevant values from the <code class="docutils literal notranslate"><span class="pre">similarity_matrix</span></code> using the indicators:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">numerator</span></code>: calculated from the <code class="docutils literal notranslate"><span class="pre">similarity_matrix</span></code> values for positive pairs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">denominator</span></code>: calculated from the <code class="docutils literal notranslate"><span class="pre">similarity_matrix</span></code> values for negative pairs.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">custom_simclr_contrastive_loss</span><span class="p">(</span><span class="n">proj_feat1</span><span class="p">,</span> <span class="n">proj_feat2</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>

  <span class="sd">"""</span>
<span class="sd">  custom_simclr_contrastive_loss(proj_feat1, proj_feat2)</span>
<span class="sd">  Returns contrastive loss, given sets of projected features, with positive</span>
<span class="sd">  pairs matched along the batch dimension.</span>
<span class="sd">  Required args:</span>
<span class="sd">  - proj_feat1 (2D torch Tensor): projected features for first image</span>
<span class="sd">      augmentations (batch_size x feat_size)</span>
<span class="sd">  - proj_feat2 (2D torch Tensor): projected features for second image</span>
<span class="sd">      augmentations (batch_size x feat_size)</span>

<span class="sd">  Optional args:</span>
<span class="sd">  - temperature (float): relaxation temperature. (default: 0.5)</span>
<span class="sd">  Returns:</span>
<span class="sd">  - loss (float): mean contrastive loss</span>
<span class="sd">  """</span>
  <span class="n">device</span> <span class="o">=</span> <span class="n">proj_feat1</span><span class="o">.</span><span class="n">device</span>

  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">proj_feat1</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">proj_feat2</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Batch dimension of proj_feat1 (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">proj_feat1</span><span class="p">)</span><span class="si">}</span><span class="s2">) "</span>
                     <span class="sa">f</span><span class="s2">"and proj_feat2 (</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">proj_feat2</span><span class="p">)</span><span class="si">}</span><span class="s2">) should be same"</span><span class="p">)</span>

  <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">proj_feat1</span><span class="p">)</span> <span class="c1"># N</span>
  <span class="n">z1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">proj_feat1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">z2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">proj_feat2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">proj_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 2N x projected feature dimension</span>
  <span class="n">similarity_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span>
      <span class="n">proj_features</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">proj_features</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span>
      <span class="p">)</span> <span class="c1"># dim: 2N x 2N</span>

  <span class="c1"># initialize arrays to identify sets of positive and negative examples, of</span>
  <span class="c1"># shape (batch_size * 2, batch_size * 2), and where</span>
  <span class="c1"># 0 indicates that 2 images are NOT a pair (either positive or negative, depending on the indicator type)</span>
  <span class="c1"># 1 indices that 2 images ARE a pair (either positive or negative, depending on the indicator type)</span>
  <span class="n">pos_sample_indicators</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">neg_sample_indicators</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

  <span class="c1">#################################################</span>
  <span class="c1"># Fill in missing code below (...),</span>
  <span class="c1"># then remove or comment the line below to test your function</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Exercise: Implement SimCLR loss."</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># EXERCISE: Implement the SimClr loss calculation</span>
  <span class="c1"># Calculate the numerator of the Loss expression by selecting the appropriate elements from similarity_matrix.</span>
  <span class="c1"># Use the pos_sample_indicators tensor</span>
  <span class="n">numerator</span> <span class="o">=</span> <span class="o">...</span>

  <span class="c1"># Calculate the denominator of the Loss expression by selecting the appropriate elements from similarity_matrix,</span>
  <span class="c1"># and summing over pairs for each item.</span>
  <span class="c1"># Use the neg_sample_indicators tensor</span>
  <span class="n">denominator</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">denominator</span> <span class="o">&lt;</span> <span class="mf">1e-8</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span> <span class="c1"># clamp to avoid division by 0</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">denominator</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">)</span>

  <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">loss</span>


<span class="c1">## Uncomment below to test your function</span>
<span class="c1"># test_custom_contrastive_loss_fct(custom_simclr_contrastive_loss)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_c98e8c66.py"><em>Click for solution</em></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">custom_simclr_contrastive_loss</span><span class="p">()</span> <span class="ow">is</span> <span class="n">correctly</span> <span class="n">implemented</span><span class="o">.</span>
</pre></div>
</div>
<p>We can now train the SimCLR encoder with the custom contrastive loss for a few epochs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="c1"># Train SimCLR for a few epochs</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Training a SimCLR encoder with the custom contrastive loss..."</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">_</span><span class="p">,</span> <span class="n">test_simclr_loss_array</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">train_simclr</span><span class="p">(</span>
    <span class="n">encoder</span><span class="o">=</span><span class="n">models</span><span class="o">.</span><span class="n">EncoderCore</span><span class="p">(),</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_invariance_torchdataset</span><span class="p">,</span>
    <span class="n">train_sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
    <span class="n">loss_fct</span><span class="o">=</span><span class="n">custom_simclr_contrastive_loss</span>
    <span class="p">)</span>

<span class="c1"># Plot SimCLR loss over a few epochs.</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_simclr_loss_array</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"SimCLR network loss"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Epoch number"</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Training loss"</span><span class="p">)</span>
</pre></div>
</div>
</div>

</div>
<p>Given that self-supervised models typically require more training than supervised models, instead of fully pre-training a network here, we will load one that was <strong>pre-trained for 60 epochs</strong>. Again, the <strong>encoder shares the same architecture</strong> as the one used for the supervised, random and VAE examples above.</p>
<p>The following code:</p>
<ul class="simple">
<li><p>loads the parameters of a SimCLR network pre-trained on the SimCLR contrastive task using <code class="docutils literal notranslate"><span class="pre">load.load_encoder()</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load SimCLR encoder pre-trained on the contrastive loss</span>
<span class="n">simclr_encoder</span> <span class="o">=</span> <span class="n">load</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s2">"simclr"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading SimCLR encoder from 'neuromatch_ssl_tutorial/checkpoints/simclr_encoder_60ep_bs1000_deg90_trans0-2_scale0-8to1-2_seed2021.pth'.
    =&gt; trained for 60 epochs (batch_size of 1000) on the full dSprites subset dataset
with the following random affine transforms:
	degrees=90
	translation=(0.2, 0.2)
	scale=(0.8, 1.2).
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="interactive-demo-6-1-1-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-a-simclr-network-encoder-that-was-pre-trained-using-different-image-transformations">
<h3>Interactive Demo 6.1.1: Evaluating the classification performance of a logistic regression trained on the representations produced by a SimCLR network encoder that was pre-trained using different image transformations.<a class="headerlink" href="#interactive-demo-6-1-1-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-a-simclr-network-encoder-that-was-pre-trained-using-different-image-transformations" title="Permalink to this headline">¶</a></h3>
<p>For the pre-trained SimCLR encoder, as with the VAE encoder, as the encoder parameters have already been trained, they should be kept frozen while the classifier is trained by setting <code class="docutils literal notranslate"><span class="pre">freeze_features=True</span></code>.</p>
<p>We train and test with <code class="docutils literal notranslate"><span class="pre">dSprites_torchdataset</span></code> instead of <code class="docutils literal notranslate"><span class="pre">dSprites_invariance_torchdataset</span></code>, as we are interested in the classifier performance on the real dSprites images, and not their augmentations.</p>
<p><strong>Interactive Demo:</strong> Try different numbers of epochs for which to train the classifier. (The original setting is <code class="docutils literal notranslate"><span class="pre">num_epochs=10</span></code>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Training a classifier on the pre-trained SimCLR encoder representations..."</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">simclr_loss_array</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">train_classifier</span><span class="p">(</span>
    <span class="n">encoder</span><span class="o">=</span><span class="n">simclr_encoder</span><span class="p">,</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_torchdataset</span><span class="p">,</span>
    <span class="n">train_sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
    <span class="n">test_sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span>
    <span class="n">freeze_features</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># keep the encoder frozen while training the classifier</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1"># DEMO: Try different numbers of epochs</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">simclr_loss_array</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Loss of classifier trained on a SimCLR encoder."</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Epoch number"</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Training loss"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
Training a classifier on the pre-trained SimCLR encoder representations...
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "ca4e6ba38dc849118b478158c6ae7f65"}
</script><div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_10472</span><span class="o">/</span><span class="mf">3517275132.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span>     <span class="n">freeze_features</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># keep the encoder frozen while training the classifier</span>
<span class="g g-Whitespace">     </span><span class="mi">12</span>     <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1"># DEMO: Try different numbers of epochs</span>
<span class="ne">---&gt; </span><span class="mi">13</span>     <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span>     <span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> 

<span class="nn">~/work/course-content-dl/course-content-dl/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/neuromatch_ssl_tutorial/modules/models.py</span> in <span class="ni">train_classifier</span><span class="nt">(encoder, dataset, train_sampler, test_sampler, num_epochs, fraction_of_labels, batch_size, freeze_features, subset_seed, use_cuda, progress_bar, verbose)</span>
<span class="g g-Whitespace">    </span><span class="mi">290</span> 
<span class="g g-Whitespace">    </span><span class="mi">291</span>             <span class="k">if</span> <span class="n">freeze_features</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">292</span>                 <span class="n">features</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">get_features</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">293</span>             <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">294</span>                 <span class="n">features</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="nn">~/work/course-content-dl/course-content-dl/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student/neuromatch_ssl_tutorial/modules/models.py</span> in <span class="ni">get_features</span><span class="nt">(self, X)</span>
<span class="g g-Whitespace">    </span><span class="mi">163</span>     <span class="k">def</span> <span class="nf">get_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">164</span>         <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
<span class="ne">--&gt; </span><span class="mi">165</span>             <span class="n">feats_extr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_extractor</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">166</span>             <span class="n">feats_flat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">feats_extr</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">167</span>             <span class="n">feats_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_projections</span><span class="p">(</span><span class="n">feats_flat</span><span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1049</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1050</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1051</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1052</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1053</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/nn/modules/container.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">137</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">138</span>         <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">139</span>             <span class="nb">input</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">140</span>         <span class="k">return</span> <span class="nb">input</span>
<span class="g g-Whitespace">    </span><span class="mi">141</span> 

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1049</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1050</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1051</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1052</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1053</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/nn/modules/pooling.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">614</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">615</span>         <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
<span class="ne">--&gt; </span><span class="mi">616</span>                             <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_include_pad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">divisor_override</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">617</span> 
<span class="g g-Whitespace">    </span><span class="mi">618</span> 

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Network</span> <span class="n">performance</span> <span class="n">after</span> <span class="mi">10</span> <span class="n">classifier</span> <span class="n">training</span> <span class="n">epochs</span> <span class="p">(</span><span class="n">chance</span><span class="p">:</span> <span class="mf">33.33</span><span class="o">%</span><span class="p">):</span>
    <span class="n">Training</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">97.83</span><span class="o">%</span>
    <span class="n">Testing</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">97.53</span><span class="o">%</span>
</pre></div>
</div>
<p>The network (using the transforms proposed above) performs at 97.53% accuracy on the test dataset, after 15 classifier training epochs.</p>
<p><b>Shape classification results using different feature encoders:</b></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><em>Chance</em></p></th>
<th class="head"><p></p></th>
<th class="head"><p>None (raw data)</p></th>
<th class="head"><p>Supervised</p></th>
<th class="head"><p>Random</p></th>
<th class="head"><p>VAE</p></th>
<th class="head"><p>SimCLR</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>33.33%</em></p></td>
<td><p></p></td>
<td><p>39.55%</p></td>
<td><p>98.38%</p></td>
<td><p>44.67%</p></td>
<td><p>45.75%</p></td>
<td><p>97.53%</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-7-self-supervised-networks-learn-representation-invariance">
<h1>Section 7: Self-supervised networks learn representation invariance<a class="headerlink" href="#section-7-self-supervised-networks-learn-representation-invariance" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-7-invariant-representations">
<h2>Video 7: Invariant Representations<a class="headerlink" href="#video-7-invariant-representations" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="section-7-1-the-effects-of-using-data-transformations-on-invariance-in-simclr-network-representations">
<h2>Section 7.1: The effects of using data transformations on invariance in SimCLR network representations.<a class="headerlink" href="#section-7-1-the-effects-of-using-data-transformations-on-invariance-in-simclr-network-representations" title="Permalink to this headline">¶</a></h2>
<p>We now observe the effects of adding our data transformations on the invariance learned by a pre-trained SimCLR network encoder.</p>
<div class="section" id="interactive-demo-7-1-1-visualizing-the-simclr-network-encoder-rsms-organized-along-different-latent-dimensions">
<h3>Interactive Demo 7.1.1: Visualizing the SimCLR network encoder RSMs, organized along different latent dimensions.<a class="headerlink" href="#interactive-demo-7-1-1-visualizing-the-simclr-network-encoder-rsms-organized-along-different-latent-dimensions" title="Permalink to this headline">¶</a></h3>
<p>We will now compare the pre-trained SimCLR encoder network RSM to the previously generated encoder RSMs.</p>
<p>Again, we pass <code class="docutils literal notranslate"><span class="pre">dSprites_torchdataset</span></code> instead of <code class="docutils literal notranslate"><span class="pre">dSprites_invariance_torchdataset</span></code>, as we are interested in the RSMs for the real dSprites images, and not their augmentations.</p>
<p><strong>Interactive Demo:</strong> Visualize the RSMs, organized along different latent dimensions (<code class="docutils literal notranslate"><span class="pre">"scale"</span></code>, <code class="docutils literal notranslate"><span class="pre">"orientation"</span></code>, <code class="docutils literal notranslate"><span class="pre">"posX"</span></code> or <code class="docutils literal notranslate"><span class="pre">"posY"</span></code>), and compare the patterns observed for the different encoder networks. (The original setting is <code class="docutils literal notranslate"><span class="pre">sorting_latent="shape"</span></code>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorting_latent</span> <span class="o">=</span> <span class="s2">"shape"</span> <span class="c1"># DEMO: Try sorting by different latent dimensions</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Plotting RSMs..."</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">plot_model_RSMs</span><span class="p">(</span>
    <span class="n">encoders</span><span class="o">=</span><span class="p">[</span><span class="n">supervised_encoder</span><span class="p">,</span> <span class="n">vae_encoder</span><span class="p">,</span> <span class="n">simclr_encoder</span><span class="p">],</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_torchdataset</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span> <span class="c1"># we want to see the representations on the held out test set</span>
    <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s2">"Supervised network encoder RSM"</span><span class="p">,</span> <span class="s2">"VAE network encoder RSM"</span><span class="p">,</span>
            <span class="s2">"SimCLR network encoder RSM"</span><span class="p">],</span> <span class="c1"># plot titles</span>
    <span class="n">sorting_latent</span><span class="o">=</span><span class="n">sorting_latent</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="discussion-7-1-1-what-can-we-conclude-about-the-ability-of-contrastive-models-like-simclr-to-construct-a-meaningful-representation-space">
<h3>Discussion 7.1.1: What can we conclude about the ability of contrastive models like SimCLR to construct a meaningful representation space?<a class="headerlink" href="#discussion-7-1-1-what-can-we-conclude-about-the-ability-of-contrastive-models-like-simclr-to-construct-a-meaningful-representation-space" title="Permalink to this headline">¶</a></h3>
<p><strong>A.</strong> How do the pre-trained SimCLR encoder RSMs (sorted along different latent dimensions) compare to the supervised and pre-trained VAE encoder network RSMs?<br/>
<strong>B.</strong>  What explains these different RSMs?<br/>
<strong>C.</strong>  What advantages might some encoders have over others?<br/>
<strong>D.</strong>  Does a good performance by the SimCLR encoder on a contrastive task guarantee good performance on a downstream classification task?<br/>
<strong>E.</strong>  How might one modify the SimCLR encoder pre-training, for example, if the downstream task were to predict <code class="docutils literal notranslate"><span class="pre">orientation</span></code> instead of <code class="docutils literal notranslate"><span class="pre">shape</span></code>?</p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_5d3ad579.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="supporting-images-for-discussion-response-examples-for-7-1-1-all-simclr-encoder-rsms">
<h3>Supporting images for Discussion response examples for 7.1.1: All SimCLR encoder RSMs<a class="headerlink" href="#supporting-images-for-discussion-response-examples-for-7-1-1-all-simclr-encoder-rsms" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown ### Supporting images for Discussion response examples for 7.1.1: All SimCLR encoder RSMs</span>

<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span> <span class="s2">"images"</span><span class="p">,</span> <span class="s2">"rsms_simclr_encoder_60ep_bs1000_deg90_trans0-2_scale0-8to1-2_seed2021.png"</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-8-avoiding-representational-collapse">
<h1>Section 8: Avoiding representational collapse<a class="headerlink" href="#section-8-avoiding-representational-collapse" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-8-avoiding-representational-collapse">
<h2>Video 8: Avoiding Representational Collapse<a class="headerlink" href="#video-8-avoiding-representational-collapse" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="section-8-1-the-effects-of-reducing-the-number-of-negative-examples-used-in-the-simclr-contrastive-loss">
<h2>Section 8.1: The effects of reducing the number of negative examples used in the SimCLR contrastive loss.<a class="headerlink" href="#section-8-1-the-effects-of-reducing-the-number-of-negative-examples-used-in-the-simclr-contrastive-loss" title="Permalink to this headline">¶</a></h2>
<p>As seen above in the contrastive loss implementation, a strategy used to train neural networks with contrastive losses is to use large batch sizes (here, we used 1,000 examples per batch), and to use the representations of different images in a batch as <strong>each other’s negative examples</strong>. So with a batch size of 1,000, each image has one positive paired image (its paired augmentation), and 999 negative paired images (every image but itself, including its own paired augmentation, again). This enables the contrastive loss to obtain a good estimate of the full representational similarity distribution.</p>
<p>To observe the consequences of sampling using fewer negative examples in the contrastive loss, we use a pre-trained SimCLR network again. However, this one was pre-trained with a parameter called <code class="docutils literal notranslate"><span class="pre">neg_pairs</span></code> set to <code class="docutils literal notranslate"><span class="pre">2</span></code>. Under the hood, this parameter affects only the contrastive loss calculation, allowing it to use <strong>only 2 of the total available negative pairs in a batch, for each image.</strong></p>
<p>The following code:</p>
<ul class="simple">
<li><p>loads the parameters of a SimCLR network pre-trained on the SimCLR contrastive task, but with only 2 negative pairs used per image in the loss calculation, using <code class="docutils literal notranslate"><span class="pre">load.load_encoder()</span></code>,</p></li>
<li><p>plots the RSMs of a few network encoders for comparison.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="c1"># Load SimCLR encoder pre-trained on the contrastive loss</span>
<span class="n">simclr_encoder_neg_pairs</span> <span class="o">=</span> <span class="n">load</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span>
    <span class="n">REPO_PATH</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s2">"simclr"</span><span class="p">,</span> <span class="n">neg_pairs</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-8-1-1-visualizing-the-network-encoder-rsms-organized-along-different-latent-dimensions-and-plotting-similarity-histograms">
<h3>Coding Exercise 8.1.1: Visualizing the network encoder RSMs, organized along different latent dimensions, and plotting similarity histograms.<a class="headerlink" href="#coding-exercise-8-1-1-visualizing-the-network-encoder-rsms-organized-along-different-latent-dimensions-and-plotting-similarity-histograms" title="Permalink to this headline">¶</a></h3>
<p>We will now compare the RSM for the pre-trained SimCLR encoder  trained with <strong>only 2 negative pairs</strong> to the normal pre-trained SimCLR network encoder and the random network encoder. To help us compare the representations learned by the normal and modified SimCLR encoders, we will plot a histogram of the values that make up both RSMs.</p>
<p><strong>Exercise:</strong></p>
<ul class="simple">
<li><p>Visualize the RSMs, organized along the <code class="docutils literal notranslate"><span class="pre">shape</span></code> latent dimension, and compare the patterns observed for the different encoder networks.</p></li>
<li><p>Plot a histogram of RSM values for the normal and 2-neg-pair SimCLR network encoders.</p></li>
</ul>
<p><strong>Hint</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">models.plot_model_RSMs()</span></code> returns the <strong>data matrices</strong> calculated for each encoder’s RSM, in order.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rsms_and_histogram_plot</span><span class="p">():</span>
  <span class="n">sorting_latent</span> <span class="o">=</span> <span class="s2">"shape"</span> <span class="c1"># Exercise: Try sorting by different latent dimensions</span>
  <span class="c1"># EXERCISE: Visualize RSMs for the normal SimCLR, 2-neg-pair SimCLR and random network encoders.</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Plotting RSMs..."</span><span class="p">)</span>
  <span class="n">simclr_rsm</span><span class="p">,</span> <span class="n">simclr_neg_pairs_rsm</span><span class="p">,</span> <span class="n">random_rsm</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">plot_model_RSMs</span><span class="p">(</span>
      <span class="n">encoders</span><span class="o">=</span><span class="p">[</span><span class="n">simclr_encoder</span><span class="p">,</span> <span class="n">simclr_encoder_neg_pairs</span><span class="p">,</span> <span class="n">random_encoder</span><span class="p">],</span>
      <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_torchdataset</span><span class="p">,</span>
      <span class="n">sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span> <span class="c1"># we want to see the representations on the held out test set</span>
      <span class="n">titles</span><span class="o">=</span><span class="p">[</span><span class="s2">"SimCLR network encoder RSM"</span><span class="p">,</span>
              <span class="sa">f</span><span class="s2">"SimCLR network encoder RSM</span><span class="se">\n</span><span class="s2">(2 negative pairs per image used in loss calc.)"</span><span class="p">,</span>
              <span class="s2">"Random network encoder RSM"</span><span class="p">],</span> <span class="c1"># plot titles</span>
      <span class="n">sorting_latent</span><span class="o">=</span><span class="n">sorting_latent</span>
      <span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># Fill in missing code below (...),</span>
  <span class="c1"># then remove or comment the line below to test your implementation</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Exercise: Plot histogram."</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># EXERCISE: Plot a histogram of RSM values for both SimCLR encoders.</span>
  <span class="n">plot_rsm_histogram</span><span class="p">(</span>
      <span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
      <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="o">...</span><span class="p">],</span>
      <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
      <span class="n">nbins</span><span class="o">=</span><span class="mi">100</span>
      <span class="p">)</span>


<span class="c1">## Uncomment below to test your code</span>
<span class="c1"># rsms_and_histogram_plot()</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_696d2dd7.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_696d2dd7_1.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_696d2dd7_1.png" style="width: 2258.0px; height: 759.0px;"/></a>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_696d2dd7_2.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/static/W3D1_Tutorial1_Solution_696d2dd7_2.png" style="width: 1120.0px; height: 833.0px;"/></a>
</div>
<div class="section" id="discussion-8-1-1-what-can-we-conclude-about-the-importance-of-negative-pairs-in-computing-the-contrastive-loss-for-models-like-simclr">
<h3>Discussion 8.1.1: What can we conclude about the importance of negative pairs in computing the contrastive loss for models like SimCLR?<a class="headerlink" href="#discussion-8-1-1-what-can-we-conclude-about-the-importance-of-negative-pairs-in-computing-the-contrastive-loss-for-models-like-simclr" title="Permalink to this headline">¶</a></h3>
<p><strong>A.</strong>  How does changing the number of negative pairs affect the networks’ RSMs?<br/>
<strong>B.</strong>  How is the shape classifier likely to perform when the encoder is pre-trained with very few negative pairs?<br/>
<strong>C.</strong>  What, intuitively, is the role of negative pairs in shaping the feature space that a contrastive model learns, and how does this role relate to the role of positive pairs?</p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_205ec8fe.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="supporting-images-for-discussion-response-examples-for-8-1-1-all-simclr-encoder-2-neg-pairs-rsms">
<h3>Supporting images for Discussion response examples for 8.1.1: All SimCLR encoder (2 neg. pairs) RSMs<a class="headerlink" href="#supporting-images-for-discussion-response-examples-for-8-1-1-all-simclr-encoder-2-neg-pairs-rsms" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown ### Supporting images for Discussion response examples for 8.1.1: All SimCLR encoder (2 neg. pairs) RSMs</span>

<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span> <span class="s2">"images"</span><span class="p">,</span> <span class="s2">"rsms_simclr_encoder_2neg_60ep_bs1000_deg90_trans0-2_scale0-8to1-2_seed2021.png"</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="interactive-demo-8-1-1-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-a-simclr-network-encoder-pre-trained-with-only-a-few-negative-pairs">
<h3>Interactive Demo 8.1.1: Evaluating the classification performance of a logistic regression trained on the representations produced by a SimCLR network encoder pre-trained with only a few negative pairs.<a class="headerlink" href="#interactive-demo-8-1-1-evaluating-the-classification-performance-of-a-logistic-regression-trained-on-the-representations-produced-by-a-simclr-network-encoder-pre-trained-with-only-a-few-negative-pairs" title="Permalink to this headline">¶</a></h3>
<p>For the 2-neg-pair SimCLR encoder, as the encoder parameters have already been trained, they should again be kept frozen while the classifier is trained by setting <code class="docutils literal notranslate"><span class="pre">freeze_features=True</span></code>.</p>
<p><strong>Interactive Demo:</strong> Try different numbers of epochs for which to train the classifier. (The original setting is <code class="docutils literal notranslate"><span class="pre">num_epochs=25</span></code>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Training a classifier on the representations learned by the SimCLR "</span>
      <span class="s2">"network encoder pre-trained</span><span class="se">\n</span><span class="s2">using only 2 negative pairs per image "</span>
      <span class="s2">"for the loss calculation..."</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">simclr_neg_pairs_loss_array</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">train_classifier</span><span class="p">(</span>
  <span class="n">encoder</span><span class="o">=</span><span class="n">simclr_encoder_neg_pairs</span><span class="p">,</span>
  <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_torchdataset</span><span class="p">,</span>
  <span class="n">train_sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
  <span class="n">test_sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span>
  <span class="n">freeze_features</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="c1"># keep the encoder frozen while training the classifier</span>
  <span class="n">num_epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="c1"># DEMO: try different numbers of epochs</span>
  <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>
  <span class="p">)</span>

<span class="c1"># Plot the loss array</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">simclr_neg_pairs_loss_array</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">((</span><span class="s2">"Loss of classifier trained on a SimCLR encoder</span><span class="se">\n</span><span class="s2">"</span>
<span class="s2">"trained with 2 negative pairs only."</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Epoch number"</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Training loss"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>After dropping the number of negative pairs used per image in pre-training a SimCLR encoder, classification accuracy drops to 66.75% on the test dataset, even after 50 classifier training epochs.</p>
<p><b>Shape classification results using different feature encoders:</b></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><em>Chance</em></p></th>
<th class="head"><p></p></th>
<th class="head"><p>None (raw data)</p></th>
<th class="head"><p>Supervised</p></th>
<th class="head"><p>Random</p></th>
<th class="head"><p>VAE</p></th>
<th class="head"><p>SimCLR</p></th>
<th class="head"><p>SimCLR (few neg.pairs)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><em>33.33%</em></p></td>
<td><p></p></td>
<td><p>39.55%</p></td>
<td><p>98.38%</p></td>
<td><p>44.67%</p></td>
<td><p>45.75%</p></td>
<td><p>97.53%</p></td>
<td><p>66.75%</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-9-good-representations-enable-few-shot-learning">
<h1>Section 9: Good representations enable few-shot learning.<a class="headerlink" href="#section-9-good-representations-enable-few-shot-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-9-few-shot-supervised-learning">
<h2>Video 9: Few-shot Supervised Learning<a class="headerlink" href="#video-9-few-shot-supervised-learning" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="section-9-1-the-benefits-of-pre-training-an-encoder-network-in-a-few-short-learning-scenario-i-e-when-only-few-labelled-examples-are-available">
<h2>Section 9.1: The benefits of pre-training an encoder network in a few-short learning scenario, i.e., when only few labelled examples are available.<a class="headerlink" href="#section-9-1-the-benefits-of-pre-training-an-encoder-network-in-a-few-short-learning-scenario-i-e-when-only-few-labelled-examples-are-available" title="Permalink to this headline">¶</a></h2>
<p>The toy dataset we have been using, <strong>dSprites</strong>, is thoroughly labelled along 5 different dimensions. However, this is not the case for many datasets. Some very large datasets may have few if any labels.</p>
<p>One of our last steps is to examine how each of our models perform in such a case when only few labelled images are available for training. In this scenario, we will train classifiers on different fractions of the training data (between 0.01 and 1.0), and see how they perform on the test set.</p>
<p>For the different types of encoder, this means:</p>
<ul class="simple">
<li><p><strong>Supervised encoder:</strong> As the supervised encoder can only be trained with labels, we will start from random encoders and train them end-to-end on the classification task with the fraction of labelled images allowed.<br/>
<em><strong>Note on * symbol:</strong> Given that that network is trained end-to-end, we will train it for more epochs, and mark it with “*” in the graphs.</em></p></li>
<li><p><strong>Random encoder:</strong> By definition, the random encoder is untrained.</p></li>
<li><p><strong>VAE encoder</strong>: As a generative model can be pre-trained on unlabelled data, we will use the VAE encoder pre-trained on the reconstruction task using the full dataset, before training the classifier layer with the fraction of labelled images allowed.</p></li>
<li><p><strong>SimCLR encoder</strong>: As an SSL model can be pre-trained on unlabelled data, we will use the SimCLR encoder pre-trained on the contrastive task using the full dataset, before training the classifier layer with the fraction of labelled images allowed.</p></li>
</ul>
<p><em><strong>Note on number of training epochs:</strong> The numbers of epochs are specified below for when the <strong>full training dataset</strong> is used. For each fraction of the dataset a classifier is trained on, the <strong>number of training epochs is scaled up</strong> to compensate for the drop in number of training examples. For example, if we specify 10 epochs for a model, the 0.1 fraction labelled classifier will be trained over ~30 epochs. Also, we use <strong>slightly fewer epochs</strong> than above, here, in the interest of time.</em></p>
<div class="section" id="interactive-demo-9-1-1-training-classifiers-on-different-encoders-using-only-a-fraction-of-the-full-labelled-dataset">
<h3>Interactive Demo 9.1.1: Training classifiers on different encoders, using only a fraction of the full labelled dataset.<a class="headerlink" href="#interactive-demo-9-1-1-training-classifiers-on-different-encoders-using-only-a-fraction-of-the-full-labelled-dataset" title="Permalink to this headline">¶</a></h3>
<p>In this demo, we select a few fractions (4 to 6) of the full labelled dataset with which to train the classifiers.</p>
<p><strong>Interactive Demo:</strong> Set <code class="docutils literal notranslate"><span class="pre">labelled_fractions</span></code> argument to a list of fractions (4 to 6 values between 0.01 and 1.0) with which to train classifiers for each encoder.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="n">new_supervised_encoder</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">EncoderCore</span><span class="p">()</span> <span class="c1"># new, random supervised encoder</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">train_encoder_clfs_by_fraction_labelled</span><span class="p">(</span>
    <span class="n">encoders</span><span class="o">=</span><span class="p">[</span><span class="n">new_supervised_encoder</span><span class="p">,</span> <span class="n">random_encoder</span><span class="p">,</span> <span class="n">vae_encoder</span><span class="p">,</span> <span class="n">simclr_encoder</span><span class="p">],</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_torchdataset</span><span class="p">,</span>
    <span class="n">train_sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
    <span class="n">test_sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span>
    <span class="n">labelled_fractions</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="p">],</span> <span class="c1"># DEMO: select 4-6 fractions to run</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="c1"># train the supervised network (end-to-end) for more epochs</span>
    <span class="n">freeze_features</span><span class="o">=</span><span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">],</span> <span class="c1"># only train new supervised network end-to-end</span>
    <span class="n">subset_seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">,</span>
    <span class="n">encoder_labels</span><span class="o">=</span><span class="p">[</span><span class="s2">"supervised"</span><span class="p">,</span> <span class="s2">"random"</span><span class="p">,</span> <span class="s2">"VAE"</span><span class="p">,</span> <span class="s2">"SimCLR"</span><span class="p">],</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Performance of classifiers trained</span><span class="se">\n</span><span class="s2">with different network encoders"</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="discussion-9-1-1-what-can-we-conclude-the-advantages-and-disadvantages-of-the-different-encoder-network-types-under-different-conditions">
<h3>Discussion 9.1.1: What can we conclude the advantages and disadvantages of the different encoder network types under different conditions?<a class="headerlink" href="#discussion-9-1-1-what-can-we-conclude-the-advantages-and-disadvantages-of-the-different-encoder-network-types-under-different-conditions" title="Permalink to this headline">¶</a></h3>
<p><strong>A.</strong> Which models are most and least affected by how much labelled data is available?<br/>
<strong>B.</strong> What might explain why different models are affected differently?</p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_e5b876cb.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="supporting-images-for-discussion-response-examples-for-9-1-1-classifier-performances-for-various-fractions-of-labelled-data">
<h3>Supporting images for Discussion response examples for 9.1.1: Classifier performances for various fractions of labelled data<a class="headerlink" href="#supporting-images-for-discussion-response-examples-for-9-1-1-classifier-performances-for-various-fractions-of-labelled-data" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown ### Supporting images for Discussion response examples for 9.1.1: Classifier performances for various fractions of labelled data</span>

<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">REPO_PATH</span><span class="p">,</span> <span class="s2">"images"</span><span class="p">,</span> <span class="s2">"labelled_fractions.png"</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-10-ethical-considerations-for-self-supervised-learning-from-biased-datasets">
<h1>Section 10: Ethical considerations for self-supervised learning from biased datasets<a class="headerlink" href="#section-10-ethical-considerations-for-self-supervised-learning-from-biased-datasets" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-10-un-self-supervised-learning">
<h2>Video 10: Un/Self-Supervised Learning<a class="headerlink" href="#video-10-un-self-supervised-learning" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="section-10-1-the-consequences-of-training-models-on-biased-datasets">
<h2>Section 10.1: The consequences of training models on biased datasets.<a class="headerlink" href="#section-10-1-the-consequences-of-training-models-on-biased-datasets" title="Permalink to this headline">¶</a></h2>
<p>If a model is trained on a biased dataset, it is likely to learn a representational encoding that reproduces these biases, impairing its ability to generalize properly and increasing the likelihood that it will propagate these biases forward.</p>
<p>Here, we investigate the effects of training the models on a biased subset of the training dataset. Specifically, we introduce a <code class="docutils literal notranslate"><span class="pre">train_sampler_biased</span></code>, a training dataset sampler that only samples:</p>
<ul class="simple">
<li><p><strong>squares</strong>, if they are centered on the <strong>lefthand</strong> side of an image <strong>(posX: 0 to 0.3)</strong>,</p></li>
<li><p><strong>ovals</strong>, if they are centered in the <strong>center</strong> of an image <strong>(posX: 0.35 to 0.65)</strong>,</p></li>
<li><p><strong>hearts</strong>, if they are centered on the <strong>righthand</strong> side of am image <strong>(posX: 0.7 to 1.0)</strong>.</p></li>
</ul>
<p>This sampling bias introduces a correlation between <code class="docutils literal notranslate"><span class="pre">shape</span></code> and <code class="docutils literal notranslate"><span class="pre">posX</span></code> that does not exist in the original dataset.</p>
<p>We then train each model as above on the dataset, and observe their performance when tested on an unbiased dataset.</p>
<p><em><strong>Note on dataset size:</strong> This biased sampling also significantly reduces the size of the training dataset available (approximately 6x). Thus, it would not be fair to compare our results here to those obtained previously in the tutorial, when we were using the full dataset. For this reason, <strong>as a control, we will also separately train the models with <code class="docutils literal notranslate"><span class="pre">train_sampler_bias_ctrl</span></code></strong>, a training dataset sampler that does not share the same sampling bias as <code class="docutils literal notranslate"><span class="pre">train_sampler_biased</span></code>, but can only sample as many samples as <code class="docutils literal notranslate"><span class="pre">train_sampler_biased</span></code> can.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="n">bias_type</span> <span class="o">=</span> <span class="s2">"shape_posX_spaced"</span> <span class="c1"># name of this bias</span>

<span class="c1"># initialize a biased training sampler and an unbiased test sampler</span>
<span class="n">train_sampler_biased</span><span class="p">,</span> <span class="n">test_sampler_for_biased</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">train_test_split_idx</span><span class="p">(</span>
    <span class="n">dSprites_torchdataset</span><span class="p">,</span>
    <span class="n">fraction_train</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="c1"># 95:5 split to partially compensate for loss of training examples due to bias</span>
    <span class="n">randst</span><span class="o">=</span><span class="n">SEED</span><span class="p">,</span>
    <span class="n">train_bias</span><span class="o">=</span><span class="n">bias_type</span>
    <span class="p">)</span>

<span class="c1"># initialize a control, unbiased training sampler and an unbiased test sampler</span>
<span class="n">train_sampler_bias_ctrl</span><span class="p">,</span> <span class="n">test_sampler_for_bias_ctrl</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">train_test_split_idx</span><span class="p">(</span>
    <span class="n">dSprites_torchdataset</span><span class="p">,</span>
    <span class="n">fraction_train</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
    <span class="n">randst</span><span class="o">=</span><span class="n">SEED</span><span class="p">,</span>
    <span class="n">train_bias</span><span class="o">=</span><span class="n">bias_type</span><span class="p">,</span>
    <span class="n">control</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Biased dataset: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_sampler_biased</span><span class="p">)</span><span class="si">}</span><span class="s2"> training, "</span>
      <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_sampler_for_biased</span><span class="p">)</span><span class="si">}</span><span class="s2"> test images"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Bias control dataset: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_sampler_bias_ctrl</span><span class="p">)</span><span class="si">}</span><span class="s2"> training, "</span>
      <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_sampler_for_bias_ctrl</span><span class="p">)</span><span class="si">}</span><span class="s2"> test images"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We plot some images sampled with <code class="docutils literal notranslate"><span class="pre">train_sampler_biased</span></code> to observe the pattern described above where <code class="docutils literal notranslate"><span class="pre">shape</span></code> and <code class="docutils literal notranslate"><span class="pre">posX</span></code> are now correlated.</p>
<p>To better visualize the bias introduced, we will plot them with annotations that show, in red:</p>
<ul class="simple">
<li><p>the <strong>edges</strong> of each of the 3 <code class="docutils literal notranslate"><span class="pre">posX</span></code> sections, and</p></li>
<li><p>the <strong>center</strong>, i.e. <code class="docutils literal notranslate"><span class="pre">(posX,</span> <span class="pre">posY)</span></code>, for each shape.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Plotting first 20 images from the biased training dataset.</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">dSprites</span><span class="o">.</span><span class="n">show_images</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">train_sampler_biased</span><span class="o">.</span><span class="n">indices</span><span class="p">[:</span><span class="mi">20</span><span class="p">],</span> <span class="n">annotations</span><span class="o">=</span><span class="s2">"posX_quadrants"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We also plot some images sampled with <code class="docutils literal notranslate"><span class="pre">train_sampler_bias_ctrl</span></code> to verify visually that this biased pattern does not appear in the control dataset.</p>
<p>Again, the annotations are added, <strong>purely for visualization purposes</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"Plotting sample images from the bias control training dataset.</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">dSprites</span><span class="o">.</span><span class="n">show_images</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="n">train_sampler_bias_ctrl</span><span class="o">.</span><span class="n">indices</span><span class="p">[:</span><span class="mi">20</span><span class="p">],</span> <span class="n">annotations</span><span class="o">=</span><span class="s2">"posX_quadrants"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown ### Function to run full training procedure</span>
<span class="c1"># @markdown (from initializing and pretraining encoders to training classifiers):</span>

<span class="c1"># @markdown `full_training_procedure(train_sampler, test_sampler)`</span>

<span class="k">def</span> <span class="nf">full_training_procedure</span><span class="p">(</span><span class="n">train_sampler</span><span class="p">,</span> <span class="n">test_sampler</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">dataset_type</span><span class="o">=</span><span class="s2">"biased"</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>

  <span class="k">if</span> <span class="n">dataset_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"biased"</span><span class="p">,</span> <span class="s2">"bias_ctrl"</span><span class="p">]:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">"Expected model_type to be 'biased' or 'bias_ctrl', "</span>
                     <span class="sa">f</span><span class="s2">"but found </span><span class="si">{</span><span class="n">model_type</span><span class="si">}</span><span class="s2">."</span><span class="p">)</span>

  <span class="n">supervised_encoder</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">EncoderCore</span><span class="p">()</span>
  <span class="n">random_encoder</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">EncoderCore</span><span class="p">()</span>

  <span class="c1"># Load pre-trained VAE encoder</span>
  <span class="n">vae_encoder</span> <span class="o">=</span> <span class="n">load</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span>
      <span class="n">REPO_PATH</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s2">"vae"</span><span class="p">,</span> <span class="n">dataset_type</span><span class="o">=</span><span class="n">dataset_type</span><span class="p">,</span>
      <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
      <span class="p">)</span>

  <span class="c1"># Load pre-trained SimCLR encoder</span>
  <span class="n">simclr_encoder</span> <span class="o">=</span> <span class="n">load</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span>
      <span class="n">REPO_PATH</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s2">"simclr"</span><span class="p">,</span> <span class="n">dataset_type</span><span class="o">=</span><span class="n">dataset_type</span><span class="p">,</span>
      <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
      <span class="p">)</span>

  <span class="n">encoders</span> <span class="o">=</span> <span class="p">[</span><span class="n">supervised_encoder</span><span class="p">,</span> <span class="n">random_encoder</span><span class="p">,</span> <span class="n">vae_encoder</span><span class="p">,</span> <span class="n">simclr_encoder</span><span class="p">]</span>
  <span class="n">freeze_features</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]</span>
  <span class="n">encoder_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"supervised"</span><span class="p">,</span> <span class="s2">"random"</span><span class="p">,</span> <span class="s2">"VAE"</span><span class="p">,</span> <span class="s2">"SimCLR"</span><span class="p">]</span>

  <span class="n">num_clf_epochs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">80</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="se">\n</span><span class="s2">Training supervised encoder and classifier for </span><span class="si">{</span><span class="n">num_clf_epochs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> "</span>
    <span class="sa">f</span><span class="s2">"epochs, and all other classifiers for </span><span class="si">{</span><span class="n">num_clf_epochs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> epochs each."</span><span class="p">)</span>
  <span class="n">_</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">train_encoder_clfs_by_fraction_labelled</span><span class="p">(</span>
      <span class="n">encoders</span><span class="o">=</span><span class="n">encoders</span><span class="p">,</span>
      <span class="n">dataset</span><span class="o">=</span><span class="n">dSprites_torchdataset</span><span class="p">,</span>
      <span class="n">train_sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
      <span class="n">test_sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span>
      <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_clf_epochs</span><span class="p">,</span>
      <span class="n">freeze_features</span><span class="o">=</span><span class="n">freeze_features</span><span class="p">,</span>
      <span class="n">subset_seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">,</span>
      <span class="n">encoder_labels</span><span class="o">=</span><span class="n">encoder_labels</span><span class="p">,</span>
      <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span>
      <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
      <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We now run the same analyses as in <strong>Section 9</strong>, but with our <strong>biased training data sampler</strong> (and unbiased control sampler) to observe how the different models perform. Because the dataset is much smaller, we increase the number of pre-trained and training epochs for the encoders and classifiers.</p>
<p>Let us start with our <strong>unbiased control sampler</strong>, to get a sense of the classification performance levels we should expect with a dataset this size.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Training all models using the control, unbiased training dataset</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">full_training_procedure</span><span class="p">(</span>
    <span class="n">train_sampler_bias_ctrl</span><span class="p">,</span> <span class="n">test_sampler_for_bias_ctrl</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Classifier performances with control, unbiased training dataset"</span><span class="p">,</span>
    <span class="n">dataset_type</span><span class="o">=</span><span class="s2">"bias_ctrl"</span> <span class="c1"># for loading correct pre-trained networks</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>A similar pattern is observed here as with the full dataset, though notably most performances are a bit weaker, likely due to us (A) using a smaller training dataset, and (B) training and pre-training for fewer iterations, considering the dataset size, for time-efficiency reasons.</p>
<p>Using the same parameters, we now repeat the analysis with the <strong>biased</strong> training data sampler.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># call this before any dataset/network initializing or training,</span>
<span class="c1"># to ensure reproducibility</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"Training all models using the biased training dataset</span><span class="se">\n</span><span class="s2">"</span><span class="p">)</span>
<span class="n">full_training_procedure</span><span class="p">(</span>
    <span class="n">train_sampler_biased</span><span class="p">,</span> <span class="n">test_sampler_for_biased</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Classifier performances with biased training dataset"</span><span class="p">,</span>
    <span class="n">dataset_type</span><span class="o">=</span><span class="s2">"biased"</span> <span class="c1"># for loading correct pre-trained networks</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Interestingly, the SimCLR network encoder is not only the only network to perform well, it even outperforms its control performance (which uses the same test dataset), at least with this particular dataset and biasing.</p>
<p><em><strong>Note on performance improvement:</strong> This improvement for the SimCLR encoder is reflected in the pre-training loss curves (not shown here), which show that the encoder trained with the biased dataset learns faster than the encoder trained with the unbiased training set. It is possible that the dataset biasing, by reducing the variability in the dataset, makes the contrastive task easier, thus enabling the network to learn a good feature space for the classification task in fewer epochs</em></p>
<div class="section" id="discussion-10-1-1-how-do-different-models-cope-with-a-biased-training-dataset">
<h3>Discussion 10.1.1: How do different models cope with a biased training dataset?<a class="headerlink" href="#discussion-10-1-1-how-do-different-models-cope-with-a-biased-training-dataset" title="Permalink to this headline">¶</a></h3>
<p><strong>A.</strong> Which models are most and least affected by the biased training dataset?<br/>
<strong>B.</strong> Which types of images in the test set are most likely causing the observed drop in performance?<br/>
<strong>C.</strong> Why are certain models more robust to the bias introduced here than others?<br/>
<strong>D.</strong> What are some methods we can employ to help mitigate the negative effects of biases in our training sets on our ability to learn good data representations with our models?</p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_ebd5812d.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="discussion-10-1-2-how-do-these-principles-apply-more-generally">
<h3>Discussion 10.1.2: How do these principles apply more generally?<a class="headerlink" href="#discussion-10-1-2-how-do-these-principles-apply-more-generally" title="Permalink to this headline">¶</a></h3>
<p>We have seen now how self-supervised learning (SSL) can improve a network’s ability to learn good representations of data. For the purposes of this tutorial, we presented examples with a <strong>simplified dataset</strong>: the dSprites dataset, where we know:<br/>
(1) the latent dimensions for all images,<br/>
(2) the joint probability distribution across latent dimensions for the full dataset, and<br/>
(3) the precise nature of the bias introduced into our biased dataset <strong>(Section 10)</strong>.</p>
<p>As a result, it is quite simple to design data augmentations that ensure that the pre-trained encoder will learn a good feature space for the downstream classification task.<br/>
<br/></p>
<p>In real-world applications, with more complex or difficult datasets,<br>
<strong>A.</strong> What principles can we draw on to successfully apply SSL to learn good data representations in feature space? For example,<br/>
<strong>B.</strong> What challenges might we face with new datasets, compared to applying SSL to dSprites?<br/>
<strong>C.</strong> What types of augmentations might we use when working with non visual datasets, e.g. a speech dataset.<br/>
<br/></br></p>
<p>In addition, we primarily discussed <strong>only one type of SSL, namely SimCLR</strong>. However, many different types of SSL exist, some of which do not use explicit data augmentations.<br>
<strong>D.</strong> What type of SSL task could be implemented for <strong>sequential or time series</strong> data. For example, you might wish to predict from electrical brain recordings what stage of sleep a person is in. How might you use the knowledge that sleep stages change slowly in time to construct a useful SSL task?</br></p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/solutions/W3D1_Tutorial1_Solution_118e3873.py"><em>Click for solution</em></a></p>
</div>
</div>
</div>
<div class="section" id="section-11-conclusion">
<h1>Section 11: Conclusion<a class="headerlink" href="#section-11-conclusion" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-11-conclusion">
<h2>Video 11: Conclusion<a class="headerlink" href="#video-11-conclusion" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W3D1_UnsupervisedAndSelfSupervisedLearning/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
<div class="prev-next-bottom">
<a class="left-prev" href="../chapter_title.html" id="prev-link" title="previous page">Unsupervised And Self Supervised Learning</a>
<a class="right-next" href="../../W3D2_BasicReinforcementLearning/chapter_title.html" id="next-link" title="next page">Basic Reinforcement Learning</a>
</div>
</div>
</div>
<footer class="footer mt-5 mt-md-0">
<div class="container">
<p>
        
          By Neuromatch<br/>
        
            © Copyright 2021.<br/>
</p>
</div>
</footer>
</main>
</div>
</div>
<script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>
</body>
</html>