
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 1: Introduction to Reinforcement Learning — Neuromatch Academy: Deep Learning</title>
<link href="../../../_static/css/theme.css" rel="stylesheet"/>
<link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<link as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script src="../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
<script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
<script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-dl-logo-square-4xp.jpeg" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="../../W3D3_ReinforcementLearningForGames/chapter_title.html" rel="next" title="Reinforcement Learning For Games"/>
<link href="../chapter_title.html" rel="prev" title="Basic Reinforcement Learning"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</link></link></link></link></head>
<body data-offset="80" data-spy="scroll" data-target="#bd-toc-nav">
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
<div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<img alt="logo" class="logo" src="../../../_static/nma-dl-logo-square-4xp.jpeg"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main navigation" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
   Introduction
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using Discord
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html">
     Tutorial 1: PyTorch
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html">
     Tutorial 3: Deep linear neural networks
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial Neural Networks
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_Optimization/chapter_title.html">
   Optimization (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_Optimization/student/W1D4_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_Regularization/chapter_title.html">
   Regularization (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/TheBasics.html">
   Deep Learning: The Basics Wrap-up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Doing More With Fewer Parameters
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/chapter_title.html">
   Convnets And Recurrent Neural Networks (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.html">
     Tutorial 2: Introduction to RNNs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial2.html">
     (Bonus) Tutorial 2: Facial recognition using modern convnets
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/chapter_title.html">
   Modern Recurrent Neural Networks (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.html">
     Tutorial 1: Modeling sequencies and encoding text
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html">
     Tutorial 2: Modern RNNs and their variants
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/student/W2D4_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D5_GenerativeModels/chapter_title.html">
   Generative Models (W2D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial2.html">
     Tutorial 2: Introduction to GANs and Density Ratio Estimation Perspective of GANs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial3.html">
     Tutorial 3: Conditional GANs and Implications of GAN Technology
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/DoingMoreWithFewerParameters.html">
   Deep Learning: Doing more with fewer parameters Wrap-up (Coming soon!)
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Advanced Topics
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/chapter_title.html">
   Unsupervised And Self Supervised Learning (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html">
     Tutorial 1: Un/Self-supervised learning methods
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Basic Reinforcement Learning (W3D2)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 1: Introduction to Reinforcement Learning
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/chapter_title.html">
   Reinforcement Learning For Games (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/student/W3D3_Tutorial1.html">
     Tutorial 1: Learn to play games with RL
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ContinualLearning/chapter_title.html">
   Continual Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Introduction to Continual Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Out-of-distribution (OOD) Learning
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/AdvancedTopics.html">
   Deep Learning: Advanced Topics Wrap-up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction to projects
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through2_DL.html">
     Modeling Steps 1 - 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_3through4_DL.html">
     Modeling Steps 3 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through6_DL.html">
     Modeling Steps 5 - 6
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_7through9_DL.html">
     Modeling Steps 7 - 9
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_10_DL.html">
     Modeling Steps 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProjectDL.html">
     Example Data Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModelingProjectDL.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/Example_Deep_Learning_Project.html">
     Example Deep Learning Project
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/projects_overview.html">
   Project Templates
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ComputerVision/README.html">
     Computer Vision
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/em_synapses.html">
       Knowledge Extraction from a Convolutional Neural Network
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/spectrogram_analysis.html">
       Music classification and generation with spectrograms
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/screws.html">
       Something Screwy - image recognition, detection, and classification of screws
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/image_alignment.html">
       Image Alignment
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/data_augmentation.html">
       Data Augmentation in image classification models
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/transfer_learning.html">
       Transfer Learning
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ReinforcementLearning/README.html">
     Reinforcement Learning
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/robolympics.html">
       NMA Robolympics: Controlling robots using reinforcement learning
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/lunar_lander.html">
       Performance Analysis of DQN Algorithm on the Lunar Lander task
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/human_rl.html">
       Using RL to Model Cognitive Tasks
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/README.html">
     Natural Language Processing
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/sentiment_analysis.html">
       Twitter Sentiment Analysis
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/machine_translation.html">
       Machine Translation
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/Neuroscience/README.html">
     Neuroscience
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/pose_estimation.html">
       Animal Pose Estimation
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/cellular_segmentation.html">
       Segmentation and Denoising
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/algonauts_videos.html">
       Load algonauts videos
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/blurry_vision.html">
       Vision with Lost Glasses: Modelling how the brain deals with noisy input
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/finetuning_fmri.html">
       Moving beyond Labels: Finetuning CNNs on BOLD response
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/neuro_seq_to_seq.html">
       Focus on what matters: inferring low-dimensional dynamics from neural recordings
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/datasets_and_models.html">
   Models and Data sets
  </a>
</li>
</ul>
</div>
</nav> <!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
<div class="topbar container-xl fixed-top">
<div class="topbar-contents row">
<div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
<div class="col pl-md-4 topbar-main">
<button aria-controls="site-navigation" aria-expanded="true" aria-label="Toggle navigation" class="navbar-toggler ml-0" data-placement="left" data-target=".site-navigation" data-toggle="tooltip" id="navbar-toggler" title="Toggle navigation" type="button">
<i class="fas fa-bars"></i>
<i class="fas fa-arrow-left"></i>
<i class="fas fa-arrow-up"></i>
</button>
<div class="dropdown-buttons-trigger">
<button aria-label="Download this page" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fas fa-download"></i></button>
<div class="dropdown-buttons">
<!-- ipynb file if we had a myst markdown file -->
<!-- Download raw file -->
<a class="dropdown-buttons" href="../../../_sources/tutorials/W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.ipynb"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Download source file" type="button">.ipynb</button></a>
<!-- Download PDF via print -->
<button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" id="download-print" onclick="window.print()" title="Print to PDF" type="button">.pdf</button>
</div>
</div>
<!-- Source interaction buttons -->
<div class="dropdown-buttons-trigger">
<button aria-label="Connect with source repository" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fab fa-github"></i></button>
<div class="dropdown-buttons sourcebuttons">
<a class="repository-button" href="https://github.com/NeuromatchAcademy/course-content-dl"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Source repository" type="button"><i class="fab fa-github"></i>repository</button></a>
<a class="issues-button" href="https://github.com/NeuromatchAcademy/course-content-dl/issues/new?title=Issue%20on%20page%20%2Ftutorials/W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.html&amp;body=Your%20issue%20content%20here."><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Open an issue" type="button"><i class="fas fa-lightbulb"></i>open issue</button></a>
</div>
</div>
<!-- Full screen (wrap in <a> to have style consistency -->
<a class="full-screen-button"><button aria-label="Fullscreen mode" class="btn btn-secondary topbarbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode" type="button"><i class="fas fa-expand"></i></button></a>
<!-- Launch buttons -->
</div>
<!-- Table of contents -->
<div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
            </div>
<nav id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 1: Introduction to Reinforcement Learning
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-slides">
     Tutorial slides
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-requirements">
     Install requirements
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
     Helper functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
     Set device (GPU or CPU). Execute
     <code class="docutils literal notranslate">
<span class="pre">
       set_device()
      </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-introduction-to-reinforcement-learning">
   Section 1: Introduction to Reinforcement Learning
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-introduction-to-rl">
     Video 1: Introduction to RL
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#acme-a-research-framework-for-reinforcement-learning">
     Acme: a research framework for reinforcement learning
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-general-formulation-of-rl-problems-and-gridworlds">
   Section 2: General Formulation of RL Problems and Gridworlds
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-general-formulation-and-mdps">
     Video 2: General Formulation and MDPs
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-the-environment">
     Section 2.1: The Environment
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#implement-gridworld-form-width-30">
       Implement GridWorld { form-width: “30%” }
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#look-at-environment-spec-form-width-30">
       Look at environment_spec { form-width: “30%” }
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#pick-an-action-and-see-the-state-changing">
       Pick an action and see the state changing
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-loop-form-width-30">
       Run loop  { form-width: “30%” }
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#implement-the-evaluation-loop-form-width-30">
       Implement the evaluation loop { form-width: “30%” }
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-the-agent">
     Section 2.2: The Agent
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#agent-interface">
       Agent interface
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-1-random-agent">
       Coding Exercise 2.1: Random Agent
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#visualisation-of-a-random-agent-in-gridworld-form-width-30">
         Visualisation of a random agent in GridWorld { form-width: “30%” }
        </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-the-bellman-equation">
   Section 3: The Bellman Equation
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-the-bellman-equation">
     Video 3: The Bellman Equation
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-4-policy-evaluation">
   Section 4: Policy Evaluation
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-4-policy-evaluation">
     Video 4: Policy Evaluation
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#lecture-footnotes">
       Lecture footnotes:
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-4-1-policy-evaluation-agent">
       Coding Exercise 4.1 Policy Evaluation Agent
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#perform-policy-evaluation-form-width-30">
         Perform policy evaluation { form-width: “30%” }
        </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-5-tabular-value-based-model-free-learning">
   Section 5: Tabular Value-Based Model-Free Learning
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-5-model-free-learning">
     Video 5: Model-Free Learning
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id2">
       Lecture footnotes:
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-5-1-on-policy-control-sarsa-agent">
     Section 5.1: On-policy control: SARSA Agent
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#sarsa-algorithm">
       SARSA Algorithm
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-5-1-implement-epsilon-greedy">
       Coding Exercise 5.1: Implement
       <span class="math notranslate nohighlight">
        \(\epsilon\)
       </span>
       -greedy
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#sample-action-from-epsilon-greedy-form-width-30">
         Sample action from
         <span class="math notranslate nohighlight">
          \(\epsilon\)
         </span>
         -greedy { form-width: “30%” }
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-5-2-run-your-sarsa-agent-on-the-obstacle-environment">
       Coding Exercise 5.2: Run your SARSA agent on the
       <code class="docutils literal notranslate">
<span class="pre">
         obstacle
        </span>
</code>
       environment
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-sarsa-agent-and-visualize-value-function">
         Run SARSA agent and visualize value function
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-5-3-implement-q-learning">
       Coding Exercise 5.3: Implement Q-Learning
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-your-q-learning-agent-on-the-obstacle-environment">
       Run your Q-learning agent on the
       <code class="docutils literal notranslate">
<span class="pre">
         obstacle
        </span>
</code>
       environment
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-your-q-learning">
         Run your Q-learning
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#experiment-with-different-levels-of-greediness">
       Experiment with different levels of greediness
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-the-cell">
         Run the cell
        </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-6-function-approximation">
   Section 6: Function Approximation
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-6-function-approximation">
     Video 6: Function approximation
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-6-1-replay-buffers">
     Section 6.1 Replay Buffers
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-6-2-nfq-agent">
     Section 6.2: NFQ Agent
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-6-1-implement-nfq">
       Coding Exercise 6.1: Implement NFQ
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#train-and-evaluate-the-nfq-agent">
       Train and Evaluate the NFQ Agent
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#training-the-nfq-agent">
         Training the NFQ Agent
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#evaluating-the-agent-set-epsilon-0">
         Evaluating the agent (set
         <span class="math notranslate nohighlight">
          \(\epsilon=0\)
         </span>
         )
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#visualise-the-learned-q-values">
         Visualise the learned
         <span class="math notranslate nohighlight">
          \(Q\)
         </span>
         values
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#compare-the-greedy-and-behaviour-epsilon-greedy-policies">
       Compare the greedy and behaviour (
       <span class="math notranslate nohighlight">
        \(\epsilon\)
       </span>
       -greedy) policies
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#compare-the-greedy-policy-with-the-agent-s-policy">
         Compare the greedy policy with the agent’s policy
        </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-7-dqn">
   Section 7: DQN
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-7-deep-q-networks-dqn">
     Video 7: Deep Q-Networks (DQN)
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-7-1-run-a-dqn-agent">
       Coding Exercise 7.1: Run a DQN Agent
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#train-and-evaluate-the-dqn-agent">
         Train and evaluate the DQN agent
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#id3">
         Visualise the learned
         <span class="math notranslate nohighlight">
          \(Q\)
         </span>
         values
        </a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#id4">
         Compare the greedy policy with the agent’s policy
        </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-8-beyond-value-based-model-free-methods">
   Section 8: Beyond Value Based Model-Free Methods
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-8-other-rl-methods">
     Video 8: Other RL Methods
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#cartpole-task">
     Cartpole task
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#make-a-cartpole-environment-gym-make-cartpole-v1">
       Make a CartPole environment,
       <code class="docutils literal notranslate">
<span class="pre">
         gym.make('CartPole-v1')
        </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-8-1-policy-gradient">
     Section 8.1: Policy gradient
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-the-hyperparameters-for-policy-gradient">
       Set the hyperparameters for Policy Gradient
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-8-1-creating-a-simple-neural-network">
       Coding Exercise 8.1: Creating a simple neural network
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#select-action">
       Select Action
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#update-policy">
       Update policy
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#reward-g-t">
         Reward
         <span class="math notranslate nohighlight">
          \(G_t\)
         </span>
</a>
</li>
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#update-policy-equation">
         Update Policy: equation
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#training">
       Training
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#run-the-model">
       Run the model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#plot-the-results">
       Plot the results
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#plot-the-training-performance-for-policy-gradient">
         Plot the training performance for policy gradient
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-8-1-explore-different-hyperparameters">
       Exercise 8.1: Explore different hyperparameters.
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-8-2-actor-critic">
     Section 8.2: Actor-critic
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-the-hyperparameters-for-actor-critic">
       Set the hyperparameters for Actor Critic
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#actor-critic-network">
       Actor Critic Network
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id5">
       Training
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id6">
       Run the model
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id7">
       Plot the results
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#plot-the-training-performance-for-actor-critic">
         Plot the training performance for Actor Critic
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-8-3-effect-of-episodes-on-performance">
       Exercise 8.3: Effect of episodes on performance
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-8-4-effect-of-learning-rate-on-performance">
       Exercise 8.4: Effect of learning rate on performance
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-9-rl-in-the-real-world">
   Section 9: RL in the real world
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-9-real-world-applications-and-ethics">
     Video 9: Real-world applications and ethics
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#exercise-9-group-discussion">
     Exercise 9: Group discussion
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-10-how-to-learn-more">
   Section 10: How to learn more
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-10-how-to-learn-more">
     Video 10: How to learn more
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#appendix-and-further-reading">
   Appendix and further reading
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="row" id="main-content">
<div class="col-12 col-md-9 pl-md-3 pr-md-0">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<div class="section" id="tutorial-1-introduction-to-reinforcement-learning">
<h1>Tutorial 1: Introduction to Reinforcement Learning<a class="headerlink" href="#tutorial-1-introduction-to-reinforcement-learning" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 3, Day 2: Basic Reinforcement Learning (RL)</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong>  Matthew Sargent, Anoop Kulkarni, Sowmya Parthiban, Feryal Behbahani, Jane Wang</p>
<p><strong>Content reviewers:</strong> Ezekiel Williams, Mehul Rastogi, Lily Cheng, Roberto Guidotti, Arush Tagade</p>
<p><strong>Content editors:</strong> Spiros Chavlis</p>
<p><strong>Production editors:</strong> Spiros Chavlis</p>
<p><strong>Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs</strong></p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial Objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p>By the end of the tutorial, you should be able to:</p>
<ol class="simple">
<li><p>Within the RL framework, be able to identify the different components: environment, agent, states, and actions.</p></li>
<li><p>Understand the Bellman equation and components involved.</p></li>
<li><p>Implement tabular value-based model-free learning (Q-learning and SARSA).</p></li>
<li><p>Run a DQN agent and experiment with different hyperparameters.</p></li>
<li><p>Have a high-level understanding of other (nonvalue-based) RL methods.</p></li>
<li><p>Discuss real-world applications and ethical issues of RL.</p></li>
</ol>
<p><strong>Note:</strong> There is an issue with some images not showing up if you’re using a Safari browser. Please switch to Chrome if this is the case.</p>
<div class="section" id="tutorial-slides">
<h2>Tutorial slides<a class="headerlink" href="#tutorial-slides" title="Permalink to this headline">¶</a></h2>
<p>These are the slides for the videos in this tutorial</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe allowfullscreen="" frameborder="0" height="480" src="https://mfr.ca-1.osf.io/render?url=https://osf.io/m3kqy/?direct%26mode=render%26action=download%26mode=render" width="854"></iframe>
</div></div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<p>Run the following 5 cells in order to set up needed functions. Don’t worry about the code for now!</p>
<div class="section" id="install-requirements">
<h2>Install requirements<a class="headerlink" href="#install-requirements" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Install requirements</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">clear_output</span>
<span class="c1"># @markdown we install the acme library, see [here](https://github.com/deepmind/acme) for more info</span>

<span class="c1"># @markdown WARNING: There may be errors and warnings reported during the installation.</span>
<span class="c1"># @markdown However, they should be ignored.</span>
<span class="o">!</span>apt-get install -y xvfb ffmpeg --quiet
<span class="o">!</span>pip install --upgrade pip --quiet
<span class="o">!</span>pip install imageio --quiet
<span class="o">!</span>pip install imageio-ffmpeg
<span class="o">!</span>pip install gym --quiet
<span class="o">!</span>pip install enum34 --quiet
<span class="o">!</span>pip install dm-env --quiet
<span class="o">!</span>pip install pandas --quiet
<span class="o">!</span>pip install keras-nightly<span class="o">==</span><span class="m">2</span>.5.0.dev2021020510 --quiet
<span class="o">!</span>pip install <span class="nv">grpcio</span><span class="o">==</span><span class="m">1</span>.34.0 --quiet
<span class="o">!</span>pip install tensorflow --quiet
<span class="o">!</span>pip install typing --quiet
<span class="o">!</span>pip install einops --quiet
<span class="o">!</span>pip install dm-acme --quiet
<span class="o">!</span>pip install dm-acme<span class="o">[</span>reverb<span class="o">]</span> --quiet
<span class="o">!</span>pip install dm-acme<span class="o">[</span>tf<span class="o">]</span> --quiet
<span class="o">!</span>pip install dm-acme<span class="o">[</span>envs<span class="o">]</span> --quiet
<span class="o">!</span>pip install dm-env --quiet

<span class="n">clear_output</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)
E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Requirement already satisfied: imageio-ffmpeg in /opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages (0.4.4)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>^C
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import modules</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">enum</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">acme</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">base64</span>
<span class="kn">import</span> <span class="nn">dm_env</span>
<span class="kn">import</span> <span class="nn">IPython</span>
<span class="kn">import</span> <span class="nn">imageio</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">collections</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">tensorflow.compat.v2</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">acme</span> <span class="kn">import</span> <span class="n">specs</span>
<span class="kn">from</span> <span class="nn">acme</span> <span class="kn">import</span> <span class="n">wrappers</span>
<span class="kn">from</span> <span class="nn">acme.utils</span> <span class="kn">import</span> <span class="n">tree_utils</span>
<span class="kn">from</span> <span class="nn">acme.utils</span> <span class="kn">import</span> <span class="n">loggers</span>
<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Sequence</span>

<span class="n">tf</span><span class="o">.</span><span class="n">enable_v2_behavior</span><span class="p">()</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">'ignore'</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>       <span class="c1"># interactive display</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle"</span><span class="p">)</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">'image'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'Blues'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions">
<h2>Helper Functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h2>
<p>Implement helpers for value visualisation</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper Functions</span>
<span class="c1"># @markdown Implement helpers for value visualisation</span>

<span class="n">map_from_action_to_subplot</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)[</span><span class="n">a</span><span class="p">]</span>
<span class="n">map_from_action_to_name</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="p">(</span><span class="s2">"up"</span><span class="p">,</span> <span class="s2">"right"</span><span class="p">,</span> <span class="s2">"down"</span><span class="p">,</span> <span class="s2">"left"</span><span class="p">)[</span><span class="n">a</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">plot_values</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">colormap</span><span class="o">=</span><span class="s1">'pink'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">"nearest"</span><span class="p">,</span>
             <span class="n">cmap</span><span class="o">=</span><span class="n">colormap</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="p">[</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">plot_state_value</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  <span class="n">q</span> <span class="o">=</span> <span class="n">action_values</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
  <span class="n">vmin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
  <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
  <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">plot_values</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">colormap</span><span class="o">=</span><span class="s1">'summer'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"$v(s)$"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_action_values</span><span class="p">(</span><span class="n">action_values</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
  <span class="n">q</span> <span class="o">=</span> <span class="n">action_values</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
  <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
  <span class="n">vmin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
  <span class="n">vmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">action_values</span><span class="p">)</span>
  <span class="n">dif</span> <span class="o">=</span> <span class="n">vmax</span> <span class="o">-</span> <span class="n">vmin</span>
  <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">map_from_action_to_subplot</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>

    <span class="n">plot_values</span><span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">a</span><span class="p">],</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span> <span class="o">-</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">dif</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span> <span class="o">+</span> <span class="mf">0.05</span><span class="o">*</span><span class="n">dif</span><span class="p">)</span>
    <span class="n">action_name</span> <span class="o">=</span> <span class="n">map_from_action_to_name</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">"$q(s, \mathrm{"</span> <span class="o">+</span> <span class="n">action_name</span> <span class="o">+</span> <span class="sa">r</span><span class="s2">"})$"</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
  <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">plot_values</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">colormap</span><span class="o">=</span><span class="s1">'summer'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">vmax</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"$v(s)$"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_stats</span><span class="p">(</span><span class="n">stats</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
  <span class="n">xline</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">episode_lengths</span><span class="p">),</span> <span class="n">window</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xline</span><span class="p">,</span> <span class="n">smooth</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">episode_lengths</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Episode Length'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Episode Count'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xline</span><span class="p">,</span> <span class="n">smooth</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">episode_rewards</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Episode Return'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Episode Count'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id1">
<h2>Helper functions<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper functions</span>
<span class="k">def</span> <span class="nf">smooth</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">x</span><span class="p">[:</span><span class="n">window</span><span class="o">*</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">//</span><span class="n">window</span><span class="p">)]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">//</span><span class="n">window</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-random-seed">
<h2>Set random seed<a class="headerlink" href="#set-random-seed" title="Permalink to this headline">¶</a></h2>
<p>Executing <code class="docutils literal notranslate"><span class="pre">set_seed(seed=seed)</span></code> you are setting the seed</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set random seed</span>

<span class="c1"># @markdown Executing `set_seed(seed=seed)` you are setting the seed</span>

<span class="c1"># for DL its critical to set the random seed so that students can have a</span>
<span class="c1"># baseline to compare their results to expected results.</span>
<span class="c1"># Read more here: https://pytorch.org/docs/stable/notes/randomness.html</span>

<span class="c1"># Call `set_seed` function in the exercises to ensure reproducibility.</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Random seed </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1"> has been set.'</span><span class="p">)</span>


<span class="c1"># In case that `DataLoader` is used</span>
<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
  <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-device-gpu-or-cpu-execute-set-device">
<h2>Set device (GPU or CPU). Execute <code class="docutils literal notranslate"><span class="pre">set_device()</span></code><a class="headerlink" href="#set-device-gpu-or-cpu-execute-set-device" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set device (GPU or CPU). Execute `set_device()`</span>
<span class="c1"># especially if torch modules used.</span>

<span class="c1"># inform the user if the notebook uses GPU or CPU.</span>

<span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: For this notebook to perform best, "</span>
        <span class="s2">"if possible, in the menu under `Runtime` -&gt; "</span>
        <span class="s2">"`Change runtime type.`  select `GPU` "</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook."</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SEED</span> <span class="o">=</span> <span class="mi">2021</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-introduction-to-reinforcement-learning">
<h1>Section 1: Introduction to Reinforcement Learning<a class="headerlink" href="#section-1-introduction-to-reinforcement-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-1-introduction-to-rl">
<h2>Video 1: Introduction to RL<a class="headerlink" href="#video-1-introduction-to-rl" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="acme-a-research-framework-for-reinforcement-learning">
<h2>Acme: a research framework for reinforcement learning<a class="headerlink" href="#acme-a-research-framework-for-reinforcement-learning" title="Permalink to this headline">¶</a></h2>
<p><strong>Acme</strong> is a library of reinforcement learning (RL) agents and agent building blocks by Google DeepMind. Acme strives to expose simple, efficient, and readable agents, that serve both as reference implementations of popular algorithms and as strong baselines, while still providing enough flexibility to do novel research. The design of Acme also attempts to provide multiple points of entry to the RL problem at differing levels of complexity.</p>
<p>For more information see <a class="reference external" href="https://github.com/deepmind/acme">github repository</a>.</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-general-formulation-of-rl-problems-and-gridworlds">
<h1>Section 2: General Formulation of RL Problems and Gridworlds<a class="headerlink" href="#section-2-general-formulation-of-rl-problems-and-gridworlds" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-2-general-formulation-and-mdps">
<h2>Video 2: General Formulation and MDPs<a class="headerlink" href="#video-2-general-formulation-and-mdps" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>The agent interacts with the environment in a loop corresponding to the following diagram. The environment defines a set of <font color="blue"><strong>actions</strong></font>  that an agent can take.  The agent takes an action informed by the <font color="redorange"><strong>observations</strong></font> it receives, and will get a <font color="green"><strong>reward</strong></font> from the environment after each action. The goal in RL is to find an agent whose actions maximize the total accumulation of rewards obtained from the environment.</p>
<center><img src="https://drive.google.com/uc?id=1KktLm5mdWx1ORotxeYCq1WcQHkXzRT4F" width="500"/></center>
</div>
<div class="section" id="section-2-1-the-environment">
<h2>Section 2.1: The Environment<a class="headerlink" href="#section-2-1-the-environment" title="Permalink to this headline">¶</a></h2>
<p>For this practical session we will focus on a <strong>simple grid world</strong> environment,which consists of a 9 x 10 grid of either wall or empty cells, depicted in black and white, respectively. The smiling agent starts from an initial location and needs to navigate to reach the goal square.</p>
<center>
<img src="https://drive.google.com/uc?id=163QdCqrPybJVVO0NhDxpun5O0YZmCnsI" width="500"/>
</center>
<p>Below you will find an implementation of this Gridworld as a <code class="docutils literal notranslate"><span class="pre">dm_env.Environment</span></code>.</p>
<p>There is no coding in this section, but if you want, you can look over the provided code so that you can familiarize yourself with an example of how to set up a <strong>grid world</strong> environment.</p>
<div class="section" id="implement-gridworld-form-width-30">
<h3>Implement GridWorld { form-width: “30%” }<a class="headerlink" href="#implement-gridworld-form-width-30" title="Permalink to this headline">¶</a></h3>
<p><em>Double-click</em> to inspect the contents of this cell.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Implement GridWorld { form-width: "30%" }</span>
<span class="c1"># @markdown *Double-click* to inspect the contents of this cell.</span>

<span class="k">class</span> <span class="nc">ObservationType</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">):</span>
  <span class="n">STATE_INDEX</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
  <span class="n">AGENT_ONEHOT</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
  <span class="n">GRID</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>
  <span class="n">AGENT_GOAL_POS</span> <span class="o">=</span> <span class="n">enum</span><span class="o">.</span><span class="n">auto</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">GridWorld</span><span class="p">(</span><span class="n">dm_env</span><span class="o">.</span><span class="n">Environment</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">layout</span><span class="p">,</span>
               <span class="n">start_state</span><span class="p">,</span>
               <span class="n">goal_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">STATE_INDEX</span><span class="p">,</span>
               <span class="n">discount</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
               <span class="n">penalty_for_walls</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span>
               <span class="n">reward_goal</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
               <span class="n">max_episode_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">randomize_goals</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">"""Build a grid environment.</span>

<span class="sd">    Simple gridworld defined by a map layout, a start and a goal state.</span>

<span class="sd">    Layout should be a NxN grid, containing:</span>
<span class="sd">      * 0: empty</span>
<span class="sd">      * -1: wall</span>
<span class="sd">      * Any other positive value: value indicates reward; episode will terminate</span>

<span class="sd">    Args:</span>
<span class="sd">      layout: NxN array of numbers, indicating the layout of the environment.</span>
<span class="sd">      start_state: Tuple (y, x) of starting location.</span>
<span class="sd">      goal_state: Optional tuple (y, x) of goal location. Will be randomly</span>
<span class="sd">        sampled once if None.</span>
<span class="sd">      observation_type: Enum observation type to use. One of:</span>
<span class="sd">        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.</span>
<span class="sd">        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the</span>
<span class="sd">          agent is and 0 elsewhere.</span>
<span class="sd">        * ObservationType.GRID: NxNx3 float32 grid of feature channels.</span>
<span class="sd">          First channel contains walls (1 if wall, 0 otherwise), second the</span>
<span class="sd">          agent position (1 if agent, 0 otherwise) and third goal position</span>
<span class="sd">          (1 if goal, 0 otherwise)</span>
<span class="sd">        * ObservationType.AGENT_GOAL_POS: float32 tuple with</span>
<span class="sd">          (agent_y, agent_x, goal_y, goal_x)</span>
<span class="sd">      discount: Discounting factor included in all Timesteps.</span>
<span class="sd">      penalty_for_walls: Reward added when hitting a wall (should be negative).</span>
<span class="sd">      reward_goal: Reward added when finding the goal (should be positive).</span>
<span class="sd">      max_episode_length: If set, will terminate an episode after this many</span>
<span class="sd">        steps.</span>
<span class="sd">      randomize_goals: If true, randomize goal at every episode.</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="n">observation_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ObservationType</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'observation_type should be a ObservationType instace.'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">layout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span> <span class="o">=</span> <span class="n">start_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_states</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_discount</span> <span class="o">=</span> <span class="n">discount</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_penalty_for_walls</span> <span class="o">=</span> <span class="n">penalty_for_walls</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reward_goal</span> <span class="o">=</span> <span class="n">reward_goal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="o">=</span> <span class="n">observation_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layout_dims</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_max_episode_length</span> <span class="o">=</span> <span class="n">max_episode_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_episode_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_randomize_goals</span> <span class="o">=</span> <span class="n">randomize_goals</span>
    <span class="k">if</span> <span class="n">goal_state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="c1"># Randomly sample goal_state if not provided</span>
      <span class="n">goal_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_goal</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="n">goal_state</span>

  <span class="k">def</span> <span class="nf">_sample_goal</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Randomly sample reachable non-starting state."""</span>
    <span class="c1"># Sample a new goal</span>
    <span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">max_tries</span> <span class="o">=</span> <span class="mf">1e5</span>
    <span class="k">while</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="n">max_tries</span><span class="p">:</span>
      <span class="n">goal_state</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">goal_state</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">goal_state</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># Reachable state found!</span>
        <span class="k">return</span> <span class="n">goal_state</span>
      <span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'Failed to sample a goal state.'</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">layout</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">number_of_states</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_states</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">goal_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">start_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>

  <span class="k">def</span> <span class="nf">set_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

  <span class="nd">@goal_state</span><span class="o">.</span><span class="n">setter</span>
  <span class="k">def</span> <span class="nf">goal_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_goal</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">new_goal</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">new_goal</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">'This is not a valid goal!'</span><span class="p">)</span>
    <span class="c1"># Zero out any other goal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Setup new goal location</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">new_goal</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_goal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span> <span class="o">=</span> <span class="n">new_goal</span>

  <span class="k">def</span> <span class="nf">observation_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">AGENT_ONEHOT</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">specs</span><span class="o">.</span><span class="n">Array</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">,</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">'observation_agent_onehot'</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">GRID</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">specs</span><span class="o">.</span><span class="n">Array</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout_dims</span> <span class="o">+</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span>
          <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
          <span class="n">name</span><span class="o">=</span><span class="s1">'observation_grid'</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">AGENT_GOAL_POS</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">specs</span><span class="o">.</span><span class="n">Array</span><span class="p">(</span>
          <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'observation_agent_goal_pos'</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">STATE_INDEX</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">specs</span><span class="o">.</span><span class="n">DiscreteArray</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_states</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'observation_state_index'</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">action_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">specs</span><span class="o">.</span><span class="n">DiscreteArray</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'action'</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_obs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">AGENT_ONEHOT</span><span class="p">:</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="c1"># Place agent</span>
      <span class="n">obs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_state</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">obs</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">GRID</span><span class="p">:</span>
      <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">3</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
      <span class="n">obs</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">&lt;</span> <span class="mi">0</span>
      <span class="n">obs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="n">obs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="k">return</span> <span class="n">obs</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">AGENT_GOAL_POS</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_type</span> <span class="ow">is</span> <span class="n">ObservationType</span><span class="o">.</span><span class="n">STATE_INDEX</span><span class="p">:</span>
      <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>
      <span class="k">return</span> <span class="n">y</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x</span>

  <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_episode_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_randomize_goals</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_goal</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">(</span>
        <span class="n">step_type</span><span class="o">=</span><span class="n">dm_env</span><span class="o">.</span><span class="n">StepType</span><span class="o">.</span><span class="n">FIRST</span><span class="p">,</span>
        <span class="n">reward</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">discount</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_obs</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>

    <span class="k">if</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># up</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># right</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1"># down</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">action</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>  <span class="c1"># left</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="s1">'Invalid action: </span><span class="si">{}</span><span class="s1"> is not 0, 1, 2, or 3.'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">action</span><span class="p">))</span>

    <span class="n">new_y</span><span class="p">,</span> <span class="n">new_x</span> <span class="o">=</span> <span class="n">new_state</span>
    <span class="n">step_type</span> <span class="o">=</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">StepType</span><span class="o">.</span><span class="n">MID</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">new_y</span><span class="p">,</span> <span class="n">new_x</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>  <span class="c1"># wall</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_penalty_for_walls</span>
      <span class="n">discount</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_discount</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">new_y</span><span class="p">,</span> <span class="n">new_x</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># empty cell</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="mf">0.</span>
      <span class="n">discount</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_discount</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># a goal</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="p">[</span><span class="n">new_y</span><span class="p">,</span> <span class="n">new_x</span><span class="p">]</span>
      <span class="n">discount</span> <span class="o">=</span> <span class="mf">0.</span>
      <span class="n">new_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span>
      <span class="n">step_type</span> <span class="o">=</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">StepType</span><span class="o">.</span><span class="n">LAST</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="n">new_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_episode_steps</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_episode_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_episode_steps</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_episode_length</span><span class="p">):</span>
      <span class="n">step_type</span> <span class="o">=</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">StepType</span><span class="o">.</span><span class="n">LAST</span>
    <span class="k">return</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">(</span>
        <span class="n">step_type</span><span class="o">=</span><span class="n">step_type</span><span class="p">,</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">reward</span><span class="p">),</span>
        <span class="n">discount</span><span class="o">=</span><span class="n">discount</span><span class="p">,</span>
        <span class="n">observation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_obs</span><span class="p">())</span>

  <span class="k">def</span> <span class="nf">plot_grid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">add_start</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layout</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">'nearest'</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="c1"># Add start/goal</span>
    <span class="k">if</span> <span class="n">add_start</span><span class="p">:</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_start_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
          <span class="sa">r</span><span class="s1">'$\mathbf</span><span class="si">{S}</span><span class="s1">$'</span><span class="p">,</span>
          <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
          <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
          <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="sa">r</span><span class="s1">'$\mathbf</span><span class="si">{G}</span><span class="s1">$'</span><span class="p">,</span>
        <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
        <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>
    <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">w</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="n">y</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s1">'-w'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">h</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">],</span> <span class="s1">'-w'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">plot_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">return_rgb</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">plot_grid</span><span class="p">(</span><span class="n">add_start</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Add the agent location</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="sa">u</span><span class="s1">'😃'</span><span class="p">,</span>
        <span class="c1"># fontname='symbola',</span>
        <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span>
        <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
        <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">return_rgb</span><span class="p">:</span>
      <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'tight'</span><span class="p">)</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
      <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromstring</span><span class="p">(</span><span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">tostring_rgb</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">''</span><span class="p">)</span>
      <span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">get_width_height</span><span class="p">()</span>
      <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
      <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">data</span>

  <span class="k">def</span> <span class="nf">plot_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
    <span class="n">action_names</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sa">r</span><span class="s1">'$\uparrow$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$\rightarrow$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$\downarrow$'</span><span class="p">,</span> <span class="sa">r</span><span class="s1">'$\leftarrow$'</span>
    <span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">plot_grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Policy Visualization'</span><span class="p">)</span>
    <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
        <span class="c1"># if ((y, x) != self._start_state) and ((y, x) != self._goal_state):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_goal_state</span><span class="p">:</span>
          <span class="n">action_name</span> <span class="o">=</span> <span class="n">action_names</span><span class="p">[</span><span class="n">policy</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">]]</span>
          <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">action_name</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">'center'</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">'center'</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">plot_greedy_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="n">greedy_actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">plot_policy</span><span class="p">(</span><span class="n">greedy_actions</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">build_gridworld_task</span><span class="p">(</span><span class="n">task</span><span class="p">,</span>
                         <span class="n">discount</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                         <span class="n">penalty_for_walls</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span>
                         <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">STATE_INDEX</span><span class="p">,</span>
                         <span class="n">max_episode_length</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
  <span class="sd">"""Construct a particular Gridworld layout with start/goal states.</span>

<span class="sd">  Args:</span>
<span class="sd">      task: string name of the task to use. One of {'simple', 'obstacle',</span>
<span class="sd">        'random_goal'}.</span>
<span class="sd">      discount: Discounting factor included in all Timesteps.</span>
<span class="sd">      penalty_for_walls: Reward added when hitting a wall (should be negative).</span>
<span class="sd">      observation_type: Enum observation type to use. One of:</span>
<span class="sd">        * ObservationType.STATE_INDEX: int32 index of agent occupied tile.</span>
<span class="sd">        * ObservationType.AGENT_ONEHOT: NxN float32 grid, with a 1 where the</span>
<span class="sd">          agent is and 0 elsewhere.</span>
<span class="sd">        * ObservationType.GRID: NxNx3 float32 grid of feature channels.</span>
<span class="sd">          First channel contains walls (1 if wall, 0 otherwise), second the</span>
<span class="sd">          agent position (1 if agent, 0 otherwise) and third goal position</span>
<span class="sd">          (1 if goal, 0 otherwise)</span>
<span class="sd">        * ObservationType.AGENT_GOAL_POS: float32 tuple with</span>
<span class="sd">          (agent_y, agent_x, goal_y, goal_x).</span>
<span class="sd">      max_episode_length: If set, will terminate an episode after this many</span>
<span class="sd">        steps.</span>
<span class="sd">  """</span>
  <span class="n">tasks_specifications</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s1">'simple'</span><span class="p">:</span> <span class="p">{</span>
          <span class="s1">'layout'</span><span class="p">:</span> <span class="p">[</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
          <span class="p">],</span>
          <span class="s1">'start_state'</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
          <span class="s1">'goal_state'</span><span class="p">:</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
      <span class="p">},</span>
      <span class="s1">'obstacle'</span><span class="p">:</span> <span class="p">{</span>
          <span class="s1">'layout'</span><span class="p">:</span> <span class="p">[</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
          <span class="p">],</span>
          <span class="s1">'start_state'</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
          <span class="s1">'goal_state'</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
      <span class="p">},</span>
      <span class="s1">'random_goal'</span><span class="p">:</span> <span class="p">{</span>
          <span class="s1">'layout'</span><span class="p">:</span> <span class="p">[</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
          <span class="p">],</span>
          <span class="s1">'start_state'</span><span class="p">:</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
          <span class="c1"># 'randomize_goals': True</span>
      <span class="p">},</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">GridWorld</span><span class="p">(</span>
      <span class="n">discount</span><span class="o">=</span><span class="n">discount</span><span class="p">,</span>
      <span class="n">penalty_for_walls</span><span class="o">=</span><span class="n">penalty_for_walls</span><span class="p">,</span>
      <span class="n">observation_type</span><span class="o">=</span><span class="n">observation_type</span><span class="p">,</span>
      <span class="n">max_episode_length</span><span class="o">=</span><span class="n">max_episode_length</span><span class="p">,</span>
      <span class="o">**</span><span class="n">tasks_specifications</span><span class="p">[</span><span class="n">task</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">setup_environment</span><span class="p">(</span><span class="n">environment</span><span class="p">):</span>
  <span class="sd">"""Returns the environment and its spec."""</span>

  <span class="c1"># Make sure the environment outputs single-precision floats.</span>
  <span class="n">environment</span> <span class="o">=</span> <span class="n">wrappers</span><span class="o">.</span><span class="n">SinglePrecisionWrapper</span><span class="p">(</span><span class="n">environment</span><span class="p">)</span>

  <span class="c1"># Grab the spec of the environment.</span>
  <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">specs</span><span class="o">.</span><span class="n">make_environment_spec</span><span class="p">(</span><span class="n">environment</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span>
</pre></div>
</div>
</div>
</div>
<p>We will use two distinct tabular GridWorlds:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">simple</span></code> where the goal is at the bottom left of the grid, little navigation required.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">obstacle</span></code> where the goal is behind an obstacle the agent must avoid.</p></li>
</ul>
<p>You can visualize the grid worlds by running the cell below.</p>
<p>Note that <strong>S</strong> indicates the start state and <strong>G</strong> indicates the goal.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualise GridWorlds</span>

<span class="c1"># Instantiate two tabular environments, a simple task, and one that involves</span>
<span class="c1"># the avoidance of an obstacle.</span>
<span class="n">simple_grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s1">'simple'</span><span class="p">,</span> <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">GRID</span><span class="p">)</span>
<span class="n">obstacle_grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s1">'obstacle'</span><span class="p">,</span> <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">GRID</span><span class="p">)</span>

<span class="c1"># Plot them.</span>
<span class="n">simple_grid</span><span class="o">.</span><span class="n">plot_grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Simple'</span><span class="p">)</span>

<span class="n">obstacle_grid</span><span class="o">.</span><span class="n">plot_grid</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Obstacle'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In this environment, the agent has four possible  <font color="blue"><strong>actions</strong></font>: <code class="docutils literal notranslate"><span class="pre">up</span></code>, <code class="docutils literal notranslate"><span class="pre">right</span></code>, <code class="docutils literal notranslate"><span class="pre">down</span></code>, and <code class="docutils literal notranslate"><span class="pre">left</span></code>.  The <font color="green"><strong>reward</strong></font> is <code class="docutils literal notranslate"><span class="pre">-5</span></code> for bumping into a wall, <code class="docutils literal notranslate"><span class="pre">+10</span></code> for reaching the goal, and <code class="docutils literal notranslate"><span class="pre">0</span></code> otherwise. The episode ends when the agent reaches the goal, and otherwise continues. The <strong>discount</strong> on continuing steps, is <span class="math notranslate nohighlight">\(\gamma = 0.9\)</span>.</p>
<p>Before we start building an agent to interact with this environment, let’s first look at the types of objects the environment either returns (e.g., <font color="redorange"><strong>observations</strong></font>) or consumes (e.g., <font color="blue"><strong>actions</strong></font>). The <code class="docutils literal notranslate"><span class="pre">environment_spec</span></code> will show you the form of the <font color="redorange"><strong>observations</strong></font>, <font color="green"><strong>rewards</strong></font> and <strong>discounts</strong> that the environment exposes and the form of the <font color="blue"><strong>actions</strong></font> that can be taken.</p>
</div>
<div class="section" id="look-at-environment-spec-form-width-30">
<h3>Look at environment_spec { form-width: “30%” }<a class="headerlink" href="#look-at-environment-spec-form-width-30" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Look at environment_spec { form-width: "30%" }</span>

<span class="c1"># Note: setup_environment is implemented in the same cell as GridWorld.</span>
<span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">setup_environment</span><span class="p">(</span><span class="n">simple_grid</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'actions:</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'observations:</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">observations</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'rewards:</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">rewards</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'discounts:</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">discounts</span><span class="p">,</span> <span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We first set the environment to its initial state by calling the <code class="docutils literal notranslate"><span class="pre">reset()</span></code> method which returns the first observation and resets the agent to the starting location.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">environment</span><span class="o">.</span><span class="n">plot_state</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now we want to take an action to interact with the environment. We do this by passing a valid action to the <code class="docutils literal notranslate"><span class="pre">dm_env.Environment.step()</span></code> method which returns a <code class="docutils literal notranslate"><span class="pre">dm_env.TimeStep</span></code> namedtuple with fields <code class="docutils literal notranslate"><span class="pre">(step_type,</span> <span class="pre">reward,</span> <span class="pre">discount,</span> <span class="pre">observation)</span></code>.</p>
<p>Let’s take an action and visualise the resulting state of the grid-world. (You’ll need to rerun the cell if you pick a new action.)</p>
<p><strong>Note for kaggle users:</strong> As kaggle does not render the forms automatically the students should be careful to notice the various instructions and manually play around with the values for the variables</p>
</div>
<div class="section" id="pick-an-action-and-see-the-state-changing">
<h3>Pick an action and see the state changing<a class="headerlink" href="#pick-an-action-and-see-the-state-changing" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Pick an action and see the state changing</span>
<span class="n">action</span> <span class="o">=</span> <span class="s2">"left"</span> <span class="c1">#@param ["up", "right", "down", "left"] {type:"string"}</span>

<span class="n">action_int</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'up'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
              <span class="s1">'right'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
              <span class="s1">'down'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
              <span class="s1">'left'</span><span class="p">:</span><span class="mi">3</span> <span class="p">}</span>
<span class="n">action</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">action_int</span><span class="p">[</span><span class="n">action</span><span class="p">])</span>
<span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>  <span class="c1"># pytype: dm_env.TimeStep</span>
<span class="n">environment</span><span class="o">.</span><span class="n">plot_state</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="run-loop-form-width-30">
<h3>Run loop  { form-width: “30%” }<a class="headerlink" href="#run-loop-form-width-30" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Run loop  { form-width: "30%" }</span>
<span class="c1"># @markdown This function runs an agent in the environment for a number of</span>
<span class="c1"># @markdown episodes, allowing it to learn.</span>

<span class="c1"># @markdown *Double-click* to inspect the `run_loop` function.</span>


<span class="k">def</span> <span class="nf">run_loop</span><span class="p">(</span><span class="n">environment</span><span class="p">,</span>
             <span class="n">agent</span><span class="p">,</span>
             <span class="n">num_episodes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">num_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">logger_time_delta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
             <span class="n">label</span><span class="o">=</span><span class="s1">'training_loop'</span><span class="p">,</span>
             <span class="n">log_loss</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="p">):</span>
  <span class="sd">"""Perform the run loop.</span>

<span class="sd">  We are following the Acme run loop.</span>

<span class="sd">  Run the environment loop for `num_episodes` episodes. Each episode is itself</span>
<span class="sd">  a loop which interacts first with the environment to get an observation and</span>
<span class="sd">  then give that observation to the agent in order to retrieve an action. Upon</span>
<span class="sd">  termination of an episode a new episode will be started. If the number of</span>
<span class="sd">  episodes is not given then this will interact with the environment</span>
<span class="sd">  infinitely.</span>

<span class="sd">  Args:</span>
<span class="sd">    environment: dm_env used to generate trajectories.</span>
<span class="sd">    agent: acme.Actor for selecting actions in the run loop.</span>
<span class="sd">    num_steps: number of steps to run the loop for. If `None` (default), runs</span>
<span class="sd">      without limit.</span>
<span class="sd">    num_episodes: number of episodes to run the loop for. If `None` (default),</span>
<span class="sd">      runs without limit.</span>
<span class="sd">    logger_time_delta: time interval (in seconds) between consecutive logging</span>
<span class="sd">      steps.</span>
<span class="sd">    label: optional label used at logging steps.</span>
<span class="sd">  """</span>
  <span class="n">logger</span> <span class="o">=</span> <span class="n">loggers</span><span class="o">.</span><span class="n">TerminalLogger</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">time_delta</span><span class="o">=</span><span class="n">logger_time_delta</span><span class="p">)</span>
  <span class="n">iterator</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">)</span> <span class="k">if</span> <span class="n">num_episodes</span> <span class="k">else</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
  <span class="n">all_returns</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="n">num_total_steps</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>
    <span class="c1"># Reset any counts and start the environment.</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">episode_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">episode_return</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">episode_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="c1"># Make the first observation.</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">observe_first</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span>

    <span class="c1"># Run an episode.</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">timestep</span><span class="o">.</span><span class="n">last</span><span class="p">():</span>
      <span class="c1"># Generate an action from the agent's policy and step the environment.</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">timestep</span><span class="o">.</span><span class="n">observation</span><span class="p">)</span>
      <span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

      <span class="c1"># Have the agent observe the timestep and let the agent update itself.</span>
      <span class="n">agent</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">)</span>
      <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

      <span class="c1"># Book-keeping.</span>
      <span class="n">episode_steps</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">num_total_steps</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">episode_return</span> <span class="o">+=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">reward</span>

      <span class="k">if</span> <span class="n">log_loss</span><span class="p">:</span>
        <span class="n">episode_loss</span> <span class="o">+=</span> <span class="n">agent</span><span class="o">.</span><span class="n">last_loss</span>

      <span class="k">if</span> <span class="n">num_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_total_steps</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># Collect the results and combine with counts.</span>
    <span class="n">steps_per_second</span> <span class="o">=</span> <span class="n">episode_steps</span> <span class="o">/</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">'episode'</span><span class="p">:</span> <span class="n">episode</span><span class="p">,</span>
        <span class="s1">'episode_length'</span><span class="p">:</span> <span class="n">episode_steps</span><span class="p">,</span>
        <span class="s1">'episode_return'</span><span class="p">:</span> <span class="n">episode_return</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">log_loss</span><span class="p">:</span>
      <span class="n">result</span><span class="p">[</span><span class="s1">'loss_avg'</span><span class="p">]</span> <span class="o">=</span> <span class="n">episode_loss</span><span class="o">/</span><span class="n">episode_steps</span>

    <span class="n">all_returns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode_return</span><span class="p">)</span>

    <span class="c1"># Log the given results.</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">num_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">num_total_steps</span> <span class="o">&gt;=</span> <span class="n">num_steps</span><span class="p">:</span>
      <span class="k">break</span>
  <span class="k">return</span> <span class="n">all_returns</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="implement-the-evaluation-loop-form-width-30">
<h3>Implement the evaluation loop { form-width: “30%” }<a class="headerlink" href="#implement-the-evaluation-loop-form-width-30" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Implement the evaluation loop { form-width: "30%" }</span>
<span class="c1"># @markdown This function runs the agent in the environment for a number of</span>
<span class="c1"># @markdown episodes, without allowing it to learn, in order to evaluate it.</span>

<span class="c1"># @markdown *Double-click* to inspect the `evaluate` function.</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">environment</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">Environment</span><span class="p">,</span>
             <span class="n">agent</span><span class="p">:</span> <span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">,</span>
             <span class="n">evaluation_episodes</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
  <span class="n">frames</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">evaluation_episodes</span><span class="p">):</span>
    <span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">episode_return</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">timestep</span><span class="o">.</span><span class="n">last</span><span class="p">():</span>
      <span class="n">frames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">environment</span><span class="o">.</span><span class="n">plot_state</span><span class="p">(</span><span class="n">return_rgb</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

      <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">timestep</span><span class="o">.</span><span class="n">observation</span><span class="p">)</span>
      <span class="n">timestep</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
      <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">episode_return</span> <span class="o">+=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">reward</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s1">'Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s1"> ended with reward </span><span class="si">{</span><span class="n">episode_return</span><span class="si">}</span><span class="s1"> in </span><span class="si">{</span><span class="n">steps</span><span class="si">}</span><span class="s1"> steps'</span>
    <span class="p">)</span>
  <span class="k">return</span> <span class="n">frames</span>

<span class="k">def</span> <span class="nf">display_video</span><span class="p">(</span><span class="n">frames</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
                  <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">'temp.mp4'</span><span class="p">,</span>
                  <span class="n">frame_rate</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">):</span>
  <span class="sd">"""Save and display video."""</span>
  <span class="c1"># Write the frames to a video.</span>
  <span class="k">with</span> <span class="n">imageio</span><span class="o">.</span><span class="n">get_writer</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="n">frame_rate</span><span class="p">)</span> <span class="k">as</span> <span class="n">video</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">frames</span><span class="p">:</span>
      <span class="n">video</span><span class="o">.</span><span class="n">append_data</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>

  <span class="c1"># Read video and display the video.</span>
  <span class="n">video</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
  <span class="n">b64_video</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">video</span><span class="p">)</span>
  <span class="n">video_tag</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'&lt;video  width="320" height="240" controls alt="test" '</span>
               <span class="s1">'src="data:video/mp4;base64,</span><span class="si">{0}</span><span class="s1">"&gt;'</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">b64_video</span><span class="o">.</span><span class="n">decode</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="n">video_tag</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-2-2-the-agent">
<h2>Section 2.2: The Agent<a class="headerlink" href="#section-2-2-the-agent" title="Permalink to this headline">¶</a></h2>
<p>We will be implementing Tabular &amp; Function Approximation agents. Tabular agents are purely in Python.</p>
<p>All agents will share the same interface from the Acme <code class="docutils literal notranslate"><span class="pre">Actor</span></code>. Here we borrow a figure from Acme to show how this interaction occurs:</p>
<div class="section" id="agent-interface">
<h3>Agent interface<a class="headerlink" href="#agent-interface" title="Permalink to this headline">¶</a></h3>
<center><img src="https://drive.google.com/uc?id=1T7FTpA9RgDYFkciDFZK4brNyURZN_ZGp" width="500"/></center>
<p>Each agent implements the following functions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">(</span><span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">number_of_actions</span><span class="p">,</span> <span class="n">number_of_states</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
    <span class="sd">"""Provides the agent the number of actions and number of states."""</span>

  <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="sd">"""Generates actions from observations."""</span>

  <span class="k">def</span> <span class="nf">observe_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">):</span>
    <span class="sd">"""Records the initial timestep in a trajectory."""</span>
  
  <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">):</span>
    <span class="sd">"""Records the transition which occurred from taking an action."""</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Updates the agent's internals to potentially change its behavior."""</span>
</pre></div>
</div>
<p>Remarks on the <code class="docutils literal notranslate"><span class="pre">observe()</span></code> function:</p>
<ol class="simple">
<li><p>In the last method, the <code class="docutils literal notranslate"><span class="pre">next_timestep</span></code> provides the <code class="docutils literal notranslate"><span class="pre">reward</span></code>, <code class="docutils literal notranslate"><span class="pre">discount</span></code>, and <code class="docutils literal notranslate"><span class="pre">observation</span></code> that resulted from selecting <code class="docutils literal notranslate"><span class="pre">action</span></code>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">next_timestep.step_type</span></code> will be either <code class="docutils literal notranslate"><span class="pre">MID</span></code> or <code class="docutils literal notranslate"><span class="pre">LAST</span></code> and should be used to determine whether this is the last observation in the episode.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">next_timestep.step_type</span></code> cannot be <code class="docutils literal notranslate"><span class="pre">FIRST</span></code>; such a timestep should only ever be given to <code class="docutils literal notranslate"><span class="pre">observe_first()</span></code>.</p></li>
</ol>
</div>
<div class="section" id="coding-exercise-2-1-random-agent">
<h3>Coding Exercise 2.1: Random Agent<a class="headerlink" href="#coding-exercise-2-1-random-agent" title="Permalink to this headline">¶</a></h3>
<p>Below is a partially complete implemention of an agent that follows a random (non-learning) policy. Fill in the <code class="docutils literal notranslate"><span class="pre">select_action</span></code> method.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">select_action</span></code> method should return a random <strong>integer</strong> between 0 and <code class="docutils literal notranslate"><span class="pre">self._num_actions</span></code> (not a tensor or an array!)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RandomAgent</span><span class="p">(</span><span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">environment_spec</span><span class="p">):</span>
    <span class="sd">"""Gets the number of available actions from the environment spec."""</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span>

  <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="sd">"""Selects an action uniformly at random."""</span>
    <span class="c1">#################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your implementation</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete the select action method"</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># TODO return a random integer beween 0 and self._num_actions.</span>
    <span class="c1"># HINT: see the reference for how to sample a random integer in numpy:</span>
    <span class="c1">#   https://numpy.org/doc/1.16/reference/routines.random.html</span>
    <span class="n">action</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">return</span> <span class="n">action</span>

  <span class="k">def</span> <span class="nf">observe_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">):</span>
    <span class="sd">"""Does not record as the RandomAgent has no use for data."""</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">):</span>
    <span class="sd">"""Does not record as the RandomAgent has no use for data."""</span>
    <span class="k">pass</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">"""Does not update as the RandomAgent does not learn from data."""</span>
    <span class="k">pass</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_7eaa84d6.py"><em>Click for solution</em></a></p>
<div class="section" id="visualisation-of-a-random-agent-in-gridworld-form-width-30">
<h4>Visualisation of a random agent in GridWorld { form-width: “30%” }<a class="headerlink" href="#visualisation-of-a-random-agent-in-gridworld-form-width-30" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Visualisation of a random agent in GridWorld { form-width: "30%" }</span>

<span class="c1"># Create the agent by giving it the action space specification.</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">RandomAgent</span><span class="p">(</span><span class="n">environment_spec</span><span class="p">)</span>

<span class="c1"># Run the agent in the evaluation loop, which returns the frames.</span>
<span class="n">frames</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">environment</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">evaluation_episodes</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Visualize the random agent's episode.</span>
<span class="n">display_video</span><span class="p">(</span><span class="n">frames</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-the-bellman-equation">
<h1>Section 3: The Bellman Equation<a class="headerlink" href="#section-3-the-bellman-equation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-3-the-bellman-equation">
<h2>Video 3: The Bellman Equation<a class="headerlink" href="#video-3-the-bellman-equation" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>In this tutorial we focus mainly on <font color="green"><strong>value based methods</strong></font>, where agents maintain a value for all state-action pairs and use those estimates to choose actions that maximize that <font color="green"><strong>value</strong></font> (instead of maintaining a policy directly, like in <font color="blue"><strong>policy gradient methods</strong></font>).</p>
<p>We represent the <font color="green"><strong>action-value function</strong></font> (otherwise known as <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-function associated with following/employing a policy <span class="math notranslate nohighlight">\(\pi\)</span> in a given MDP as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8ab3701b-0158-4be4-a04c-f7458123e67b">
<span class="eqno">(94)<a class="headerlink" href="#equation-8ab3701b-0158-4be4-a04c-f7458123e67b" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{green}Q^{\color{blue}{\pi}}(\color{red}{s},\color{blue}{a}) = \mathbb{E}_{\tau \sim P^{\color{blue}{\pi}}} \left[ \sum_t \gamma^t \color{green}{r_t}| s_0=\color{red}s,a_0=\color{blue}{a} \right]
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\tau = \{\color{red}{s_0}, \color{blue}{a_0}, \color{green}{r_0}, \color{red}{s_1}, \color{blue}{a_1}, \color{green}{r_1}, \cdots \}\)</span></p>
<p>Recall that efficient value estimations are based on the famous <strong><em>Bellman Expectation Equation</em></strong>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9ea29d03-d523-417a-8c02-c764fb08656f">
<span class="eqno">(95)<a class="headerlink" href="#equation-9ea29d03-d523-417a-8c02-c764fb08656f" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{green}Q^\color{blue}{\pi}(\color{red}{s},\color{blue}{a}) =    \sum_{\color{red}{s'}\in \color{red}{\mathcal{S}}} 
\color{purple}P(\color{red}{s'} |\color{red}{s},\color{blue}{a})
\left(
  \color{green}{R}(\color{red}{s},\color{blue}{a}, \color{red}{s'}) 
  + \gamma \color{green}V^\color{blue}{\pi}(\color{red}{s'}) 
  \right)
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\color{green}V^\color{blue}{\pi}\)</span> is the expected <span class="math notranslate nohighlight">\(\color{green}Q^\color{blue}{\pi}\)</span> value for a particular state, i.e. <span class="math notranslate nohighlight">\(\color{green}V^\color{blue}{\pi}(\color{red}{s}) = \sum_{\color{blue}{a} \in \color{blue}{\mathcal{A}}} \color{blue}{\pi}(\color{blue}{a} |\color{red}{s}) \color{green}Q^\color{blue}{\pi}(\color{red}{s},\color{blue}{a})\)</span>.</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-4-policy-evaluation">
<h1>Section 4: Policy Evaluation<a class="headerlink" href="#section-4-policy-evaluation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-4-policy-evaluation">
<h2>Video 4: Policy Evaluation<a class="headerlink" href="#video-4-policy-evaluation" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<div class="section" id="lecture-footnotes">
<h3>Lecture footnotes:<a class="headerlink" href="#lecture-footnotes" title="Permalink to this headline">¶</a></h3>
<p><strong>Episodic vs non-episodic environments:</strong> Up until now, we’ve mainly been talking about episodic environments, or environments that terminate and reset (resampled) after a finite number of steps. However, there are also <em>non-episodic</em> environments, in which an agent cannot count on the environment resetting. Thus, they are forced to learn in a <em>continual</em> fashion.</p>
<p><strong>Policy iteration vs value iteration:</strong> Compare the two equations below, noting that the only difference is that in value iteration, the second sum is replaced by a max.</p>
<p><em>Policy iteration (using Bellman expectation equation)</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-091238c4-4955-407b-a59d-67b6474d9a80">
<span class="eqno">(96)<a class="headerlink" href="#equation-091238c4-4955-407b-a59d-67b6474d9a80" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{green}Q_\color{green}{k}(\color{red}{s},\color{blue}{a}) \leftarrow  \color{green}{R}(\color{red}{s},\color{blue}{a}) +\gamma \sum_{\color{red}{s'}\in \color{red}{\mathcal{S}}} 
\color{purple}P(\color{red}{s'} |\color{red}{s},\color{blue}{a})
\sum_{\color{blue}{a'} \in \color{blue}{\mathcal{A}}} \color{blue}{\pi_{k-1}}(\color{blue}{a'} |\color{red}{s'}) \color{green}{Q_{k-1}}(\color{red}{s'},\color{blue}{a'})
\end{equation}\]</div>
<p><em>Value iteration (using Bellman optimality equation)</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-b04c7e16-4a2e-4cc3-afb6-8708a2b30e8d">
<span class="eqno">(97)<a class="headerlink" href="#equation-b04c7e16-4a2e-4cc3-afb6-8708a2b30e8d" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{green}Q_\color{green}{k}(\color{red}{s},\color{blue}{a}) \leftarrow  \color{green}{R}(\color{red}{s},\color{blue}{a}) +\gamma \sum_{\color{red}{s'}\in \color{red}{\mathcal{S}}} 
\color{purple}P(\color{red}{s'} |\color{red}{s},\color{blue}{a})
\max_{\color{blue}{a'}} \color{green}{Q_{k-1}}(\color{red}{s'},\color{blue}{a'})
\end{equation}\]</div>
</div>
<div class="section" id="coding-exercise-4-1-policy-evaluation-agent">
<h3>Coding Exercise 4.1 Policy Evaluation Agent<a class="headerlink" href="#coding-exercise-4-1-policy-evaluation-agent" title="Permalink to this headline">¶</a></h3>
<p>Tabular agents implement a function <code class="docutils literal notranslate"><span class="pre">q_values()</span></code> returning a matrix of Q values
of shape: (<code class="docutils literal notranslate"><span class="pre">number_of_states</span></code>, <code class="docutils literal notranslate"><span class="pre">number_of_actions</span></code>)</p>
<p>In this section, we will implement a <code class="docutils literal notranslate"><span class="pre">PolicyEvalAgent</span></code> as an ACME actor: given an <code class="docutils literal notranslate"><span class="pre">evaluation_policy</span></code> <span class="math notranslate nohighlight">\(\pi_e\)</span> and a <code class="docutils literal notranslate"><span class="pre">behaviour_policy</span></code> <span class="math notranslate nohighlight">\(\pi_b\)</span>, it will use the <code class="docutils literal notranslate"><span class="pre">behaviour_policy</span></code> to choose actions, and it will use the corresponding trajectory data to evaluate the <code class="docutils literal notranslate"><span class="pre">evaluation_policy</span></code> (i.e. compute the Q-values as if you were following the <code class="docutils literal notranslate"><span class="pre">evaluation_policy</span></code>).</p>
<p>Algorithm:</p>
<p><strong>Initialize</strong> <span class="math notranslate nohighlight">\(Q(\color{red}{s}, \color{blue}{a})\)</span> for all <span class="math notranslate nohighlight">\(\color{red}{s}\)</span> ∈ <span class="math notranslate nohighlight">\(\mathcal{\color{red}S}\)</span> and <span class="math notranslate nohighlight">\(\color{blue}a\)</span> ∈ <span class="math notranslate nohighlight">\(\mathcal{\color{blue}A}(\color{red}s)\)</span></p>
<p><strong>Loop forever</strong>:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\color{red}{s} \gets{}\)</span>current (nonterminal) state</p></li>
<li><p><span class="math notranslate nohighlight">\(\color{blue}{a} \gets{} \text{behaviour_policy }\pi_b(\color{red}s)\)</span></p></li>
<li><p>Take action <span class="math notranslate nohighlight">\(\color{blue}{a}\)</span>; observe resulting reward <span class="math notranslate nohighlight">\(\color{green}{r}\)</span>, discount <span class="math notranslate nohighlight">\(\gamma\)</span>, and state, <span class="math notranslate nohighlight">\(\color{red}{s'}\)</span></p></li>
<li><p>Compute TD-error: <span class="math notranslate nohighlight">\(\delta = \color{green}R + \gamma Q(\color{red}{s'}, \underbrace{\pi_e(\color{red}{s'}}_{\color{blue}{a'}})) − Q(\color{red}s, \color{blue}a)\)</span></p></li>
<li><p>Update Q-value with a small <span class="math notranslate nohighlight">\(\alpha\)</span> step: <span class="math notranslate nohighlight">\(Q(\color{red}s, \color{blue}a) \gets Q(\color{red}s, \color{blue}a) + \alpha \delta\)</span></p></li>
</ol>
<p>We will use a uniform <code class="docutils literal notranslate"><span class="pre">random</span> <span class="pre">policy</span></code> as our <code class="docutils literal notranslate"><span class="pre">evaluation</span> <span class="pre">policy</span></code> here, but you could replace this with any policy you want, such as a greedy one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Uniform random policy</span>
<span class="k">def</span> <span class="nf">random_policy</span><span class="p">(</span><span class="n">q</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PolicyEvalAgent</span><span class="p">(</span><span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">environment_spec</span><span class="p">,</span> <span class="n">evaluated_policy</span><span class="p">,</span>
               <span class="n">behaviour_policy</span><span class="o">=</span><span class="n">random_policy</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Get number of states and actions from the environment spec.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_states</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">observations</span><span class="o">.</span><span class="n">num_values</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_actions</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_step_size</span> <span class="o">=</span> <span class="n">step_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_behaviour_policy</span> <span class="o">=</span> <span class="n">behaviour_policy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_evaluated_policy</span> <span class="o">=</span> <span class="n">evaluated_policy</span>
    <span class="c1">#################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your implementation</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Initialize your Q-values!"</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># TODO Initialize the Q-values to be all zeros.</span>
    <span class="c1"># (Note: can also be random, but we use zeros here for reproducibility)</span>
    <span class="c1"># HINT: This is a table of state and action pairs, so needs to be a 2-D</span>
    <span class="c1">#   array. See the reference for how to create this in numpy:</span>
    <span class="c1">#   https://numpy.org/doc/stable/reference/generated/numpy.zeros.html</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q</span> <span class="o">=</span> <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_action</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_next_state</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">q_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># return the Q values</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q</span>

  <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="c1"># Select an action</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_behaviour_policy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="p">[</span><span class="n">observation</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">observe_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">observation</span>

  <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">action</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">next_timestep</span><span class="o">.</span><span class="n">reward</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">next_timestep</span><span class="o">.</span><span class="n">discount</span>
    <span class="n">next_s</span> <span class="o">=</span> <span class="n">next_timestep</span><span class="o">.</span><span class="n">observation</span>

    <span class="c1"># Compute TD-Error.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_action</span> <span class="o">=</span> <span class="n">a</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_next_state</span> <span class="o">=</span> <span class="n">next_s</span>
    <span class="c1">#################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your implementation</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Need to select the next action"</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># TODO Select the next action from the evaluation policy</span>
    <span class="c1"># HINT: Refer to step 4 of the algorithm above.</span>
    <span class="n">next_a</span> <span class="o">=</span> <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_td_error</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">g</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="p">[</span><span class="n">next_s</span><span class="p">,</span> <span class="n">next_a</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Updates</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>
    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_action</span>
    <span class="c1"># Q-value table update.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_td_error</span>
    <span class="c1"># Update the state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_state</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_7b3f830c.py"><em>Click for solution</em></a></p>
<div class="section" id="perform-policy-evaluation-form-width-30">
<h4>Perform policy evaluation { form-width: “30%” }<a class="headerlink" href="#perform-policy-evaluation-form-width-30" title="Permalink to this headline">¶</a></h4>
<p>Here you can visualize the state value and action-value functions for the “simple” task.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Perform policy evaluation { form-width: "30%" }</span>
<span class="c1"># @markdown Here you can visualize the state value and action-value functions for the "simple" task.</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mf">1e3</span>

<span class="c1"># Create the environment</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">'simple'</span><span class="p">)</span>
<span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">setup_environment</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>

<span class="c1"># Create the policy evaluation agent to evaluate a random policy.</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">PolicyEvalAgent</span><span class="p">(</span><span class="n">environment_spec</span><span class="p">,</span> <span class="n">evaluated_policy</span><span class="o">=</span><span class="n">random_policy</span><span class="p">)</span>

<span class="c1"># run experiment and get the value functions from agent</span>
<span class="n">returns</span> <span class="o">=</span> <span class="n">run_loop</span><span class="p">(</span><span class="n">environment</span><span class="o">=</span><span class="n">environment</span><span class="p">,</span> <span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">num_steps</span><span class="p">))</span>

<span class="c1"># get the q-values</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">q_values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>

<span class="c1"># visualize value functions</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'AFTER </span><span class="si">{}</span><span class="s1"> STEPS ...'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_steps</span><span class="p">))</span>
<span class="n">plot_action_values</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-5-tabular-value-based-model-free-learning">
<h1>Section 5: Tabular Value-Based Model-Free Learning<a class="headerlink" href="#section-5-tabular-value-based-model-free-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-5-model-free-learning">
<h2>Video 5: Model-Free Learning<a class="headerlink" href="#video-5-model-free-learning" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<div class="section" id="id2">
<h3>Lecture footnotes:<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p><strong>On-policy (SARSA) vs off-policy (Q-learning) TD control:</strong> Compare the two equations below and see that the only difference is that for Q-learning, the update is performed assuming that a greedy policy is followed, which is not the one used to collect the data, hence the name <em>off-policy</em>.</p>
<p><em>SARSA</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-6a449cba-f5e7-43cc-8c43-83b7ade266cc">
<span class="eqno">(98)<a class="headerlink" href="#equation-6a449cba-f5e7-43cc-8c43-83b7ade266cc" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{green}Q(\color{red}{s},\color{blue}{a}) \leftarrow  \color{green}Q(\color{red}{s},\color{blue}{a}) +\alpha(\color{green}{r} + \gamma\color{green}{Q}(\color{red}{s'},\color{blue}{a'}) - \color{green}{Q}(\color{red}{s},\color{blue}{a}))
\end{equation}\]</div>
<p><em>Q-learning</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-4aca6cd4-a18d-4207-9a73-6fda97959544">
<span class="eqno">(99)<a class="headerlink" href="#equation-4aca6cd4-a18d-4207-9a73-6fda97959544" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{green}Q(\color{red}{s},\color{blue}{a}) \leftarrow  \color{green}Q(\color{red}{s},\color{blue}{a}) +\alpha(\color{green}{r} + \gamma\max_{\color{blue}{a'}} \color{green}{Q}(\color{red}{s'},\color{blue}{a'}) - \color{green}{Q}(\color{red}{s},\color{blue}{a}))
\end{equation}\]</div>
</div>
</div>
<div class="section" id="section-5-1-on-policy-control-sarsa-agent">
<h2>Section 5.1: On-policy control: SARSA Agent<a class="headerlink" href="#section-5-1-on-policy-control-sarsa-agent" title="Permalink to this headline">¶</a></h2>
<p>In this section, we are focusing on control RL algorithms, which perform the <strong>evaluation</strong> and <strong>improvement</strong> of the policy synchronously. That is, the policy that is being evaluated improves as the agent is using it to interact with the environent.</p>
<p>The first algorithm we are going to be looking at is SARSA. This is an <strong>on-policy algorithm</strong> – i.e: the data collection is done by leveraging the policy we’re trying to optimize.</p>
<p>As discussed during lectures, a greedy policy with respect to a given <span class="math notranslate nohighlight">\(\color{Green}Q\)</span> fails to explore the environment as needed; we will use instead an <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy policy with respect to <span class="math notranslate nohighlight">\(\color{Green}Q\)</span>.</p>
<div class="section" id="sarsa-algorithm">
<h3>SARSA Algorithm<a class="headerlink" href="#sarsa-algorithm" title="Permalink to this headline">¶</a></h3>
<p><strong>Input:</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon \in (0, 1)\)</span> the probability of taking a random action , and</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> the step size, also known as learning rate.</p></li>
</ul>
<p><strong>Initialize:</strong> <span class="math notranslate nohighlight">\(\color{green}Q(\color{red}{s}, \color{blue}{a})\)</span> for all <span class="math notranslate nohighlight">\(\color{red}{s}\)</span> ∈ <span class="math notranslate nohighlight">\(\mathcal{\color{red}S}\)</span> and <span class="math notranslate nohighlight">\(\color{blue}a\)</span> ∈ <span class="math notranslate nohighlight">\(\mathcal{\color{blue}A}\)</span></p>
<p><strong>Loop forever:</strong></p>
<ol class="simple">
<li><p>Get <span class="math notranslate nohighlight">\(\color{red}s \gets{}\)</span>current (non-terminal) state</p></li>
<li><p>Select <span class="math notranslate nohighlight">\(\color{blue}a \gets{} \text{epsilon_greedy}(\color{green}Q(\color{red}s, \cdot))\)</span></p></li>
<li><p>Step in the environment by passing the selected action <span class="math notranslate nohighlight">\(\color{blue}a\)</span></p></li>
<li><p>Observe resulting reward <span class="math notranslate nohighlight">\(\color{green}r\)</span>, discount <span class="math notranslate nohighlight">\(\gamma\)</span>, and state <span class="math notranslate nohighlight">\(\color{red}{s'}\)</span></p></li>
<li><p>Compute TD error: <span class="math notranslate nohighlight">\(\Delta \color{green}Q \gets 
\color{green}r + \gamma \color{green}Q(\color{red}{s'}, \color{blue}{a'}) − \color{green}Q(\color{red}s, \color{blue}a)\)</span>, <br/> where <span class="math notranslate nohighlight">\(\color{blue}{a'} \gets \text{epsilon_greedy}(\color{green}Q(\color{red}{s'}, \cdot))\)</span></p></li>
<li><p>Update <span class="math notranslate nohighlight">\(\color{green}Q(\color{red}s, \color{blue}a) \gets \color{green}Q(\color{red}s, \color{blue}a) + \alpha \Delta \color{green}Q\)</span></p></li>
</ol>
</div>
<div class="section" id="coding-exercise-5-1-implement-epsilon-greedy">
<h3>Coding Exercise 5.1: Implement <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy<a class="headerlink" href="#coding-exercise-5-1-implement-epsilon-greedy" title="Permalink to this headline">¶</a></h3>
<p>Below you will find incomplete code for sampling from an <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy policy, to be used later when we implement an agent that learns values according to the SARSA algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">epsilon_greedy</span><span class="p">(</span>
    <span class="n">q_values_at_s</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>  <span class="c1"># Q-values in state s: Q(s, a).</span>
    <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Probability of taking a random action.</span>
    <span class="p">):</span>
  <span class="sd">"""Return an epsilon-greedy action sample."""</span>
  <span class="c1">#################################################</span>
  <span class="c1"># Fill in missing code below (...),</span>
  <span class="c1"># then remove or comment the line below to test your implementation</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete epsilon greedy policy function"</span><span class="p">)</span>
  <span class="c1">#################################################</span>
  <span class="c1"># TODO generate a uniform random number and compare it to epsilon to decide if</span>
  <span class="c1"># the action should be greedy or not</span>
  <span class="c1"># HINT: Use np.random.random() to generate a random float from 0 to 1.</span>
  <span class="k">if</span> <span class="o">...</span><span class="p">:</span>
    <span class="c1">#TODO Greedy: Pick action with the largest Q-value.</span>
    <span class="n">action</span> <span class="o">=</span> <span class="o">...</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="c1"># Get the number of actions from the size of the given vector of Q-values.</span>
    <span class="n">num_actions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">q_values_at_s</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># TODO else return a random action</span>
    <span class="c1"># HINT: Use np.random.randint() to generate a random integer.</span>
    <span class="n">action</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_524ce08f.py"><em>Click for solution</em></a></p>
<div class="section" id="sample-action-from-epsilon-greedy-form-width-30">
<h4>Sample action from <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy { form-width: “30%” }<a class="headerlink" href="#sample-action-from-epsilon-greedy-form-width-30" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Sample action from $\epsilon$-greedy { form-width: "30%" }</span>
<span class="c1"># @markdown With $\epsilon=0.5$, you should see that about half the time, you will get back the optimal</span>
<span class="c1"># @markdown action 3, but half the time, it will be random.</span>

<span class="c1"># Create fake q-values</span>
<span class="n">q_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="c1"># Set epsilon = 0.5</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-5-2-run-your-sarsa-agent-on-the-obstacle-environment">
<h3>Coding Exercise 5.2: Run your SARSA agent on the <code class="docutils literal notranslate"><span class="pre">obstacle</span></code> environment<a class="headerlink" href="#coding-exercise-5-2-run-your-sarsa-agent-on-the-obstacle-environment" title="Permalink to this headline">¶</a></h3>
<p>This environment is similar to the Cliff-walking example from <a class="reference external" href="http://incompleteideas.net/book/RLbook2018.pdf">Sutton &amp; Barto</a> and allows us to see the different policies learned by on-policy vs off-policy methods. Try varying the number of steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SarsaAgent</span><span class="p">(</span><span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">environment_spec</span><span class="p">:</span> <span class="n">specs</span><span class="o">.</span><span class="n">EnvironmentSpec</span><span class="p">,</span>
               <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
               <span class="n">step_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
               <span class="p">):</span>

    <span class="c1"># Get number of states and actions from the environment spec.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_states</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">observations</span><span class="o">.</span><span class="n">num_values</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span>

    <span class="c1"># Create the table of Q-values, all initialized at zero.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span><span class="p">))</span>

    <span class="c1"># Store algorithm hyper-parameters.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_step_size</span> <span class="o">=</span> <span class="n">step_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>

    <span class="c1"># Containers you may find useful.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_action</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_next_state</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">q_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q</span>

  <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="p">[</span><span class="n">observation</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">observe_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">):</span>
    <span class="c1"># Set current state.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">observation</span>

  <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">):</span>
    <span class="c1"># Unpacking the timestep to lighten notation.</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">action</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">next_timestep</span><span class="o">.</span><span class="n">reward</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">next_timestep</span><span class="o">.</span><span class="n">discount</span>
    <span class="n">next_s</span> <span class="o">=</span> <span class="n">next_timestep</span><span class="o">.</span><span class="n">observation</span>
    <span class="c1"># Compute the action that would be taken from the next state.</span>
    <span class="n">next_a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">next_s</span><span class="p">)</span>
    <span class="c1"># Compute the on-policy Q-value update.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_action</span> <span class="o">=</span> <span class="n">a</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_next_state</span> <span class="o">=</span> <span class="n">next_s</span>
    <span class="c1">#################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your implementation</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete the on-policy Q-value update"</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># TODO complete the line below to compute the temporal difference error</span>
    <span class="c1"># HINT: see step 5 in the pseudocode above.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_td_error</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Optional unpacking to lighten notation.</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>
    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_action</span>
    <span class="c1">#################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your implementation</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete value update"</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># Update the Q-value table value at (s, a).</span>
    <span class="c1"># TODO: Update the Q-value table value at (s, a).</span>
    <span class="c1"># HINT: see step 6 in the pseudocode above, remember that alpha = step_size!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="o">...</span>
    <span class="c1"># Update the current state.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_state</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_4f341a18.py"><em>Click for solution</em></a></p>
<div class="section" id="run-sarsa-agent-and-visualize-value-function">
<h4>Run SARSA agent and visualize value function<a class="headerlink" href="#run-sarsa-agent-and-visualize-value-function" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Run SARSA agent and visualize value function</span>

<span class="n">num_steps</span> <span class="o">=</span> <span class="mf">1e5</span> <span class="c1"># @param {type:"number"}</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_steps</span><span class="p">)</span>

<span class="c1"># Create the environment.</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">'obstacle'</span><span class="p">)</span>
<span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">setup_environment</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>

<span class="c1"># Create the agent.</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">SarsaAgent</span><span class="p">(</span><span class="n">environment_spec</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Run the experiment and get the value functions from agent</span>
<span class="n">returns</span> <span class="o">=</span> <span class="n">run_loop</span><span class="p">(</span><span class="n">environment</span><span class="o">=</span><span class="n">environment</span><span class="p">,</span> <span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'AFTER </span><span class="si">{0:,}</span><span class="s1"> STEPS ...'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_steps</span><span class="p">))</span>

<span class="c1"># Get the Q-values and reshape them to recover grid-like structure of states.</span>
<span class="n">q_values</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">q_values</span>
<span class="n">grid_shape</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">shape</span>
<span class="n">q_values</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="o">*</span><span class="n">grid_shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Visualize the value and Q-value tables.</span>
<span class="n">plot_action_values</span><span class="p">(</span><span class="n">q_values</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>

<span class="c1"># Visualize the greedy policy.</span>
<span class="n">environment</span><span class="o">.</span><span class="n">plot_greedy_policy</span><span class="p">(</span><span class="n">q_values</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>##Section 5.2 Off-policy control: Q-learning Agent
Reminder: <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-learning is a very powerful and general algorithm, that enables control (figuring out the optimal policy/value function) both on and off-policy.</p>
<p><strong>Initialize</strong> <span class="math notranslate nohighlight">\(\color{green}Q(\color{red}{s}, \color{blue}{a})\)</span> for all <span class="math notranslate nohighlight">\(\color{red}{s} \in \color{red}{\mathcal{S}}\)</span> and <span class="math notranslate nohighlight">\(\color{blue}{a} \in \color{blue}{\mathcal{A}}\)</span></p>
<p><strong>Loop forever</strong>:</p>
<ol class="simple">
<li><p>Get <span class="math notranslate nohighlight">\(\color{red}{s} \gets{}\)</span>current (non-terminal) state</p></li>
<li><p>Select <span class="math notranslate nohighlight">\(\color{blue}{a} \gets{} \text{behaviour_policy}(\color{red}{s})\)</span></p></li>
<li><p>Step in the environment by passing the selected action <span class="math notranslate nohighlight">\(\color{blue}{a}\)</span></p></li>
<li><p>Observe resulting reward <span class="math notranslate nohighlight">\(\color{green}{r}\)</span>, discount <span class="math notranslate nohighlight">\(\gamma\)</span>, and state, <span class="math notranslate nohighlight">\(\color{red}{s'}\)</span></p></li>
<li><p>Compute the TD error: <span class="math notranslate nohighlight">\(\Delta \color{green}Q \gets \color{green}{r} + \gamma \color{green}Q(\color{red}{s'}, \color{blue}{a'}) − \color{green}Q(\color{red}{s}, \color{blue}{a})\)</span>, <br/>
where <span class="math notranslate nohighlight">\(\color{blue}{a'} \gets \arg\max_{\color{blue}{\mathcal A}} \color{green}Q(\color{red}{s'}, \cdot)\)</span></p></li>
<li><p>Update <span class="math notranslate nohighlight">\(\color{green}Q(\color{red}{s}, \color{blue}{a}) \gets \color{green}Q(\color{red}{s}, \color{blue}{a}) + \alpha \Delta \color{green}Q\)</span></p></li>
</ol>
<p>Notice that the actions <span class="math notranslate nohighlight">\(\color{blue}{a}\)</span> and <span class="math notranslate nohighlight">\(\color{blue}{a'}\)</span> are not selected using the same policy, hence this algorithm being <strong>off-policy</strong>.</p>
</div>
</div>
<div class="section" id="coding-exercise-5-3-implement-q-learning">
<h3>Coding Exercise 5.3: Implement Q-Learning<a class="headerlink" href="#coding-exercise-5-3-implement-q-learning" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">QValues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
<span class="n">Action</span> <span class="o">=</span> <span class="nb">int</span>
<span class="c1"># A value-based policy takes the Q-values at a state and returns an action.</span>
<span class="n">ValueBasedPolicy</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">QValues</span><span class="p">],</span> <span class="n">Action</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">QLearningAgent</span><span class="p">(</span><span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">environment_spec</span><span class="p">:</span> <span class="n">specs</span><span class="o">.</span><span class="n">EnvironmentSpec</span><span class="p">,</span>
               <span class="n">behaviour_policy</span><span class="p">:</span> <span class="n">ValueBasedPolicy</span><span class="p">,</span>
               <span class="n">step_size</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>

    <span class="c1"># Get number of states and actions from the environment spec.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_states</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">observations</span><span class="o">.</span><span class="n">num_values</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span>

    <span class="c1"># Create the table of Q-values, all initialized at zero.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_states</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span><span class="p">))</span>

    <span class="c1"># Store algorithm hyper-parameters.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_step_size</span> <span class="o">=</span> <span class="n">step_size</span>

    <span class="c1"># Store behavior policy.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_behaviour_policy</span> <span class="o">=</span> <span class="n">behaviour_policy</span>

    <span class="c1"># Containers you may find useful.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_action</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_next_state</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">q_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q</span>

  <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_behaviour_policy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="p">[</span><span class="n">observation</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">observe_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">):</span>
    <span class="c1"># Set current state.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">observation</span>

  <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">):</span>
    <span class="c1"># Unpacking the timestep to lighten notation.</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">action</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">next_timestep</span><span class="o">.</span><span class="n">reward</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">next_timestep</span><span class="o">.</span><span class="n">discount</span>
    <span class="n">next_s</span> <span class="o">=</span> <span class="n">next_timestep</span><span class="o">.</span><span class="n">observation</span>

    <span class="c1"># Compute the TD error.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_action</span> <span class="o">=</span> <span class="n">a</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_next_state</span> <span class="o">=</span> <span class="n">next_s</span>
    <span class="c1">#################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your implementation</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete the off-policy Q-value update"</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># TODO complete the line below to compute the temporal difference error</span>
    <span class="c1"># HINT: This is very similar to what we did for SARSA, except keep in mind</span>
    <span class="c1"># that we're now taking a max over the q-values (see lecture footnotes above).</span>
    <span class="c1"># You will find the function np.max() useful.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_td_error</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Optional unpacking to lighten notation.</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state</span>
    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_action</span>
    <span class="c1">#################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your implementation</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete value update"</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># Update the Q-value table value at (s, a).</span>
    <span class="c1"># TODO: Update the Q-value table value at (s, a).</span>
    <span class="c1"># HINT: see step 6 in the pseudocode above, remember that alpha = step_size!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">+=</span> <span class="o">...</span>
    <span class="c1"># Update the current state.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_state</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_0f0ff9d8.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="run-your-q-learning-agent-on-the-obstacle-environment">
<h3>Run your Q-learning agent on the <code class="docutils literal notranslate"><span class="pre">obstacle</span></code> environment<a class="headerlink" href="#run-your-q-learning-agent-on-the-obstacle-environment" title="Permalink to this headline">¶</a></h3>
<div class="section" id="run-your-q-learning">
<h4>Run your Q-learning<a class="headerlink" href="#run-your-q-learning" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Run your Q-learning</span>
<span class="n">epsilon</span> <span class="o">=</span>   <span class="mf">1.</span> <span class="c1"># @param {type:"number"}</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mf">1e5</span>  <span class="c1"># @param {type:"number"}</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_steps</span><span class="p">)</span>

<span class="c1"># environment</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">'obstacle'</span><span class="p">)</span>
<span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">setup_environment</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>

<span class="c1"># behavior policy</span>
<span class="n">behavior_policy</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">qval</span><span class="p">:</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">qval</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>

<span class="c1"># agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">QLearningAgent</span><span class="p">(</span><span class="n">environment_spec</span><span class="p">,</span> <span class="n">behavior_policy</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># run experiment and get the value functions from agent</span>
<span class="n">returns</span> <span class="o">=</span> <span class="n">run_loop</span><span class="p">(</span><span class="n">environment</span><span class="o">=</span><span class="n">environment</span><span class="p">,</span> <span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">)</span>

<span class="c1"># get the q-values</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">q_values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>

<span class="c1"># visualize value functions</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'AFTER </span><span class="si">{:,}</span><span class="s1"> STEPS ...'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_steps</span><span class="p">))</span>
<span class="n">plot_action_values</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># visualise the greedy policy</span>
<span class="n">grid</span><span class="o">.</span><span class="n">plot_greedy_policy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="experiment-with-different-levels-of-greediness">
<h3>Experiment with different levels of greediness<a class="headerlink" href="#experiment-with-different-levels-of-greediness" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The default was <span class="math notranslate nohighlight">\(\epsilon=1.\)</span>, what does this correspond to?</p></li>
<li><p>Try also <span class="math notranslate nohighlight">\(\epsilon =0.1, 0.5\)</span>. What do you observe? Does the behaviour policy affect the training in any way?</p></li>
</ul>
<div class="section" id="run-the-cell">
<h4>Run the cell<a class="headerlink" href="#run-the-cell" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Run the cell</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># @param {type:"number"}</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mf">1e5</span>  <span class="c1"># @param {type:"number"}</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_steps</span><span class="p">)</span>

<span class="c1"># environment</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s1">'obstacle'</span><span class="p">)</span>
<span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">setup_environment</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>

<span class="c1"># behavior policy</span>
<span class="n">behavior_policy</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">qval</span><span class="p">:</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">qval</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>

<span class="c1"># agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">QLearningAgent</span><span class="p">(</span><span class="n">environment_spec</span><span class="p">,</span> <span class="n">behavior_policy</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># run experiment and get the value functions from agent</span>
<span class="n">returns</span> <span class="o">=</span> <span class="n">run_loop</span><span class="p">(</span><span class="n">environment</span><span class="o">=</span><span class="n">environment</span><span class="p">,</span> <span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">)</span>

<span class="c1"># get the q-values</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">q_values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">layout</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>

<span class="c1"># visualize value functions</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'AFTER </span><span class="si">{:,}</span><span class="s1"> STEPS ...'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_steps</span><span class="p">))</span>
<span class="n">plot_action_values</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>

<span class="c1"># visualise the greedy policy</span>
<span class="n">grid</span><span class="o">.</span><span class="n">plot_greedy_policy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-6-function-approximation">
<h1>Section 6: Function Approximation<a class="headerlink" href="#section-6-function-approximation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-6-function-approximation">
<h2>Video 6: Function approximation<a class="headerlink" href="#video-6-function-approximation" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<center>
<img src="https://drive.google.com/uc?id=1XIj68U3eB1bKYfIEHAcVbfwobmMYQQ4X" width="500"/>
</center>
<p>So far we only considered look-up tables for value-functions. In all previous cases every state and action pair <span class="math notranslate nohighlight">\((\color{red}{s}, \color{blue}{a})\)</span>, had an entry in our <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-table. Again, this is possible in this environment as the number of states is equal to the number of cells in the grid. But this is not scalable to situations where, say, the goal location changes or the obstacles are in different locations at every episode (consider how big the table could be in this situation?).</p>
<p>An example (not covered in this tutorial) is ATARI from pixels, where the number of possible frames an agent can see is exponential in the number of pixels on the screen.</p>
<center><img alt="portfolio_view" src="https://miro.medium.com/max/1760/1*XyIpmXXAjbXerDzmGQL1yA.gif" width="200"/></center>
<p>But what we <strong>really</strong> want is just to be able to <em>compute</em> the Q-value, when fed with a particular <span class="math notranslate nohighlight">\((\color{red}{s}, \color{blue}{a})\)</span> pair. So if we had a way to get a function to do this work instead of keeping a big table, we’d get around this problem.</p>
<p>To address this, we can use <strong>function approximation</strong> as a way to generalize Q-values over some representation of the very large state space, and <strong>train</strong> them to output the values they should. In this section, we will explore <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-learning with function approximation, which (although it has been theoretically proven to diverge for some degenerate MDPs) can yield impressive results in very large environments. In particular, we will look at <a class="reference external" href="http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf">Neural Fitted Q (NFQ) Iteration</a> and <a class="reference external" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Deep Q-Networks (DQN)</a>.</p>
</div>
<div class="section" id="section-6-1-replay-buffers">
<h2>Section 6.1 Replay Buffers<a class="headerlink" href="#section-6-1-replay-buffers" title="Permalink to this headline">¶</a></h2>
<p>An important property of off-policy methods like <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-learning is that they involve two policies: one for exploration and one that is being optimized (via the <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-function updates). This means that we can generate data from the <strong>behavior</strong> policy and insert that data into some form of data storage—usually referred to as <strong>replay</strong>.</p>
<p>In order to optimize the <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-function we can then sample data from the replay <font color="purple"><strong>dataset</strong></font> and use that data to perform an update. An illustration of this learning loop is shown below.</p>
<center><img src="https://drive.google.com/uc?id=1ivTQBHWkYi_J9vWwXFd2sSWg5f2TB5T-" width="400"/></center>
<p>In the next cell we will show how to implement a simple replay buffer. This can be as simple as a python list containing transition data. In more complicated scenarios we might want to have a more performance-tuned variant, we might have to be more concerned about how large replay is and what to do when its full, and we might want to sample from replay in different ways. But a simple python list can go a surprisingly long way.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simple replay buffer</span>

<span class="c1"># Create a convenient container for the SARS tuples required by deep RL agents.</span>
<span class="n">Transitions</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s1">'Transitions'</span><span class="p">,</span> <span class="p">[</span><span class="s1">'state'</span><span class="p">,</span> <span class="s1">'action'</span><span class="p">,</span> <span class="s1">'reward'</span><span class="p">,</span> <span class="s1">'discount'</span><span class="p">,</span> <span class="s1">'next_state'</span><span class="p">])</span>

<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="sd">"""A simple Python replay buffer."""</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">capacity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">capacity</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prev_state</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span> <span class="nf">add_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prev_state</span> <span class="o">=</span> <span class="n">initial_timestep</span><span class="o">.</span><span class="n">observation</span>

  <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="n">transition</span> <span class="o">=</span> <span class="n">Transitions</span><span class="p">(</span>
        <span class="n">state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prev_state</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="n">action</span><span class="p">,</span>
        <span class="n">reward</span><span class="o">=</span><span class="n">timestep</span><span class="o">.</span><span class="n">reward</span><span class="p">,</span>
        <span class="n">discount</span><span class="o">=</span><span class="n">timestep</span><span class="o">.</span><span class="n">discount</span><span class="p">,</span>
        <span class="n">next_state</span><span class="o">=</span><span class="n">timestep</span><span class="o">.</span><span class="n">observation</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">transition</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_prev_state</span> <span class="o">=</span> <span class="n">timestep</span><span class="o">.</span><span class="n">observation</span>

  <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transitions</span><span class="p">:</span>
    <span class="c1"># Sample a random batch of Transitions as a list.</span>
    <span class="n">batch_as_list</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="c1"># Convert the list of `batch_size` Transitions into a single Transitions</span>
    <span class="c1"># object where each field has `batch_size` stacked fields.</span>
    <span class="k">return</span> <span class="n">tree_utils</span><span class="o">.</span><span class="n">stack_sequence_fields</span><span class="p">(</span><span class="n">batch_as_list</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">flush</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transitions</span><span class="p">:</span>
    <span class="n">entire_buffer</span> <span class="o">=</span> <span class="n">tree_utils</span><span class="o">.</span><span class="n">stack_sequence_fields</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">entire_buffer</span>

  <span class="k">def</span> <span class="nf">is_ready</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">buffer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="section-6-2-nfq-agent">
<h2>Section 6.2: NFQ Agent<a class="headerlink" href="#section-6-2-nfq-agent" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf">Neural Fitted Q Iteration</a> was one of the first papers to demonstrate how to leverage recent advances in Deep Learning to approximate the Q-value by a neural network.<span class="math notranslate nohighlight">\(^1\)</span>
In other words, the value <span class="math notranslate nohighlight">\(\color{green}Q(\color{red}{s}, \color{blue}{a})\)</span> are approximated by the output of a neural network <span class="math notranslate nohighlight">\(\color{green}{Q_w}(\color{red}{s}, \color{blue}{a})\)</span> for each possible action <span class="math notranslate nohighlight">\(\color{blue}{a} \in \color{blue}{\mathcal{A}}\)</span>.<span class="math notranslate nohighlight">\(^2\)</span></p>
<p>When introducing function approximations, and neural networks in particular, we need to have a loss to optimize. But looking back at the tabular setting above, you can see that we already have some notion of error: the <strong>TD error</strong>.</p>
<p>By training our neural network to output values such that the <em>TD error is minimized</em>, we will also satisfy the Bellman Optimality Equation, which is a good sufficient condition to enforce, to obtain an optimal policy.
Thanks to automatic differentiation, we can just write the TD error as a loss, e.g., with an <span class="math notranslate nohighlight">\(\ell^2\)</span> loss, but others would work too:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a668d872-e6ef-47c4-a20b-667c8e242a74">
<span class="eqno">(100)<a class="headerlink" href="#equation-a668d872-e6ef-47c4-a20b-667c8e242a74" title="Permalink to this equation">¶</a></span>\[\begin{equation}
L(\color{green}w) = \mathbb{E}\left[ \left( \color{green}{r} + \gamma \max_\color{blue}{a'} \color{green}{Q_w}(\color{red}{s'}, \color{blue}{a'}) − \color{green}{Q_w}(\color{red}{s}, \color{blue}{a})  \right)^2\right].
\end{equation}\]</div>
<p>Then we can compute the gradient with respect to the parameters of the neural network and improve our Q-value approximation incrementally.</p>
<p>NFQ builds on <span class="math notranslate nohighlight">\(\color{green}Q\)</span>-learning, but if one were to update the Q-values online directly, the training can be unstable and very slow.
Instead, NFQ uses a replay buffer, similar to what we see implemented above (Section 6.1), to update the Q-value in a batched setting.</p>
<p>When it was introduced, it also was entirely off-policy using a uniformly random policy to collect data, which was prone to instability when applied to more complex environments (e.g. when the input are pixels or the tasks are longer and more complicated).
But it is a good stepping stone to the more complex agents used today. Here, we will look at a slightly different and modernised implementation of NFQ.</p>
<p>Below you will find an incomplete NFQ agent that takes in observations from a gridworld. Instead of receiving a tabular state, it receives an observation in the form of its (x,y) coordinates in the gridworld, and the (x,y) coordinates of the goal.
<br/></p>
<p>The goal of this coding exercise is to complete this agent by implementing the loss, using mean squared error.</p>
<hr class="docutils"/>
<p><sub><span class="math notranslate nohighlight">\(^1\)</span> If you read the NFQ paper, they use a “control” notation, where there is a “cost to minimize”, instead of “rewards to maximize”, so don’t be surprised if signs/max/min do not correspond.</sub></p>
<p><sub><span class="math notranslate nohighlight">\(^2\)</span> We could feed it <span class="math notranslate nohighlight">\(\color{blue}{a}\)</span> as well and ask <span class="math notranslate nohighlight">\(Q_w\)</span> for a single scalar value, but given we have a fixed number of actions and we usually need to take an <span class="math notranslate nohighlight">\(argmax\)</span> over them, it’s easiest to just output them all in one pass.</sub></p>
<div class="section" id="coding-exercise-6-1-implement-nfq">
<h3>Coding Exercise 6.1: Implement NFQ<a class="headerlink" href="#coding-exercise-6-1-implement-nfq" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a convenient container for the SARS tuples required by NFQ.</span>
<span class="n">Transitions</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s1">'Transitions'</span><span class="p">,</span> <span class="p">[</span><span class="s1">'state'</span><span class="p">,</span> <span class="s1">'action'</span><span class="p">,</span> <span class="s1">'reward'</span><span class="p">,</span> <span class="s1">'discount'</span><span class="p">,</span> <span class="s1">'next_state'</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">NeuralFittedQAgent</span><span class="p">(</span><span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">environment_spec</span><span class="p">:</span> <span class="n">specs</span><span class="o">.</span><span class="n">EnvironmentSpec</span><span class="p">,</span>
               <span class="n">q_network</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">replay_capacity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100_000</span><span class="p">,</span>
               <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
               <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
               <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3e-4</span><span class="p">):</span>

    <span class="c1"># Store agent hyperparameters and network.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span> <span class="o">=</span> <span class="n">q_network</span>

    <span class="c1"># Container for the computed loss (see run_loop implementation above).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Create the replay buffer.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">replay_capacity</span><span class="p">)</span>

    <span class="c1"># Setup optimizer that will train the network to minimize the loss.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span><span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="c1"># Compute Q-values.</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># Adds batch dimension.</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>   <span class="c1"># Removes batch dimension</span>

    <span class="c1"># Select epsilon-greedy action.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span> <span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">action</span>

  <span class="k">def</span> <span class="nf">q_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">q_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">is_ready</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">):</span>
      <span class="c1"># If the replay buffer is not ready to sample from, do nothing.</span>
      <span class="k">return</span>

    <span class="c1"># Sample a minibatch of transitions from experience replay.</span>
    <span class="n">transitions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">)</span>

    <span class="c1"># Note: each of these tensors will be of shape [batch_size, ...].</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">action</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">reward</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">discount</span><span class="p">)</span>
    <span class="n">next_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">next_state</span><span class="p">)</span>

    <span class="c1"># Compute the Q-values at next states in the transitions.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">q_next_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">next_s</span><span class="p">)</span>  <span class="c1"># Shape [batch_size, num_actions].</span>
      <span class="n">max_q_next_s</span> <span class="o">=</span> <span class="n">q_next_s</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="c1"># Compute the TD error and then the losses.</span>
      <span class="n">target_q_value</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">max_q_next_s</span>

    <span class="c1"># Compute the Q-values at original state.</span>
    <span class="n">q_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># Gather the Q-value corresponding to each action in the batch.</span>
    <span class="n">q_s_a</span> <span class="o">=</span> <span class="n">q_s</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your implementation</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete the NFQ Agent"</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># TODO Average the squared TD errors over the entire batch using</span>
    <span class="c1"># self._loss_fn, which is defined above as nn.MSELoss()</span>
    <span class="c1"># HINT: Take a look at the reference for nn.MSELoss here:</span>
    <span class="c1">#  https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html</span>
    <span class="c1">#  What should you put for the input and the target?</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Compute the gradients of the loss with respect to the q_network variables.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Apply the gradient update.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Store the loss for logging purposes (see run_loop implementation above).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">observe_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">add_first</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_f42d1415.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="train-and-evaluate-the-nfq-agent">
<h3>Train and Evaluate the NFQ Agent<a class="headerlink" href="#train-and-evaluate-the-nfq-agent" title="Permalink to this headline">¶</a></h3>
<div class="section" id="training-the-nfq-agent">
<h4>Training the NFQ Agent<a class="headerlink" href="#training-the-nfq-agent" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Training the NFQ Agent</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.4</span> <span class="c1"># @param {type:"number"}</span>

<span class="n">max_episode_length</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Create the environment.</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s1">'simple'</span><span class="p">,</span>
    <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">AGENT_GOAL_POS</span><span class="p">,</span>
    <span class="n">max_episode_length</span><span class="o">=</span><span class="n">max_episode_length</span><span class="p">)</span>
<span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">setup_environment</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>

<span class="c1"># Define the neural function approximator (aka Q network).</span>
<span class="n">q_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span><span class="p">))</span>
<span class="c1"># Build the trainable Q-learning agent</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">NeuralFittedQAgent</span><span class="p">(</span>
    <span class="n">environment_spec</span><span class="p">,</span>
    <span class="n">q_network</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
    <span class="n">replay_capacity</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="n">returns</span> <span class="o">=</span> <span class="n">run_loop</span><span class="p">(</span>
    <span class="n">environment</span><span class="o">=</span><span class="n">environment</span><span class="p">,</span>
    <span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">logger_time_delta</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
    <span class="n">log_loss</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="evaluating-the-agent-set-epsilon-0">
<h4>Evaluating the agent (set <span class="math notranslate nohighlight">\(\epsilon=0\)</span>)<a class="headerlink" href="#evaluating-the-agent-set-epsilon-0" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Evaluating the agent (set $\epsilon=0$)</span>
<span class="c1"># Temporarily change epsilon to be more greedy; remember to change it back.</span>
<span class="n">agent</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="mf">0.0</span>

<span class="c1"># Record a few episodes.</span>
<span class="n">frames</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">environment</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">evaluation_episodes</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Change epsilon back.</span>
<span class="n">agent</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>

<span class="c1"># Display the video of the episodes.</span>
<span class="n">display_video</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="n">frame_rate</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="visualise-the-learned-q-values">
<h4>Visualise the learned <span class="math notranslate nohighlight">\(Q\)</span> values<a class="headerlink" href="#visualise-the-learned-q-values" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Visualise the learned $Q$ values</span>

<span class="c1"># Evaluate the policy for every state, similar to tabular agents above.</span>

<span class="n">environment</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span> <span class="o">+</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="p">))</span>
<span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="c1"># Hack observation to see what the Q-network would output at that point.</span>
    <span class="n">environment</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">get_obs</span><span class="p">()</span>
    <span class="n">q</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">q_values</span><span class="p">(</span><span class="n">obs</span><span class="p">))</span>
    <span class="n">pi</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">obs</span><span class="p">))</span>

<span class="n">plot_action_values</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Compare the Q-values approximated with the neural network with the tabular case in <strong>Section 5.3</strong>. Notice how the neural network is generalizing from the visited states to the unvisited similar states, while in the tabular case we updated the value of each state only when we visited that state.</p>
</div>
</div>
<div class="section" id="compare-the-greedy-and-behaviour-epsilon-greedy-policies">
<h3>Compare the greedy and behaviour (<span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy) policies<a class="headerlink" href="#compare-the-greedy-and-behaviour-epsilon-greedy-policies" title="Permalink to this headline">¶</a></h3>
<div class="section" id="compare-the-greedy-policy-with-the-agent-s-policy">
<h4>Compare the greedy policy with the agent’s policy<a class="headerlink" href="#compare-the-greedy-policy-with-the-agent-s-policy" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Compare the greedy policy with the agent's policy</span>

<span class="c1"># @markdown Notice that the agent's behavior policy has a lot more randomness,</span>
<span class="c1"># @markdown due to the high $\epsilon$. However, the greedy policy that's learned</span>
<span class="c1"># @markdown is optimal.</span>

<span class="n">environment</span><span class="o">.</span><span class="n">plot_greedy_policy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figtext</span><span class="p">(</span><span class="o">-</span><span class="mf">.08</span><span class="p">,</span> <span class="mf">.95</span><span class="p">,</span> <span class="s1">'Greedy policy using the learnt Q-values'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">environment</span><span class="o">.</span><span class="n">plot_policy</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figtext</span><span class="p">(</span><span class="o">-</span><span class="mf">.08</span><span class="p">,</span> <span class="mf">.95</span><span class="p">,</span> <span class="s2">"Policy using the agent's behavior policy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-7-dqn">
<h1>Section 7: DQN<a class="headerlink" href="#section-7-dqn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-7-deep-q-networks-dqn">
<h2>Video 7: Deep Q-Networks (DQN)<a class="headerlink" href="#video-7-deep-q-networks-dqn" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<!-- <center><img src="https://drive.google.com/uc?id=1ivTQBHWkYi_J9vWwXFd2sSWg5f2TB5T-" width="500" /></center>  -->
<center><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnature14236/MediaObjects/41586_2015_Article_BFnature14236_Fig1_HTML.jpg" width="500"/></center>
<p>In this section, we will look at an advanced deep RL Agent based on the following publication, <a class="reference external" href="https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning">Playing Atari with Deep Reinforcement Learning</a>, which introduced the first deep learning model to successfully learn control policies directly from high-dimensional pixel inputs using RL.</p>
<p>Here the agent will act directly on a pixel representation of the gridworld. You can find an incomplete implementation below.</p>
<div class="section" id="coding-exercise-7-1-run-a-dqn-agent">
<h3>Coding Exercise 7.1: Run a DQN Agent<a class="headerlink" href="#coding-exercise-7-1-run-a-dqn-agent" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">acme</span><span class="o">.</span><span class="n">Actor</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">environment_spec</span><span class="p">:</span> <span class="n">specs</span><span class="o">.</span><span class="n">EnvironmentSpec</span><span class="p">,</span>
               <span class="n">network</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
               <span class="n">replay_capacity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100_000</span><span class="p">,</span>
               <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
               <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
               <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5e-4</span><span class="p">,</span>
               <span class="n">target_update_frequency</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>

    <span class="c1"># Store agent hyperparameters and network.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span> <span class="o">=</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span> <span class="o">=</span> <span class="n">q_network</span>

    <span class="c1"># create a second q net with the same structure and initial values, which</span>
    <span class="c1"># we'll be updating separately from the learned q-network.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_target_network</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">)</span>

    <span class="c1"># Container for the computed loss (see run_loop implementation above).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_loss</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="c1"># Create the replay buffer.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">replay_capacity</span><span class="p">)</span>
    <span class="c1"># Keep an internal tracker of steps</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># How often to update the target network</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_target_update_frequency</span> <span class="o">=</span> <span class="n">target_update_frequency</span>
    <span class="c1"># Setup optimizer that will train the network to minimize the loss.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="c1"># Compute Q-values.</span>
    <span class="c1"># Sonnet requires a batch dimension, which we squeeze out right after.</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>  <span class="c1"># Adds batch dimension.</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>   <span class="c1"># Removes batch dimension</span>

    <span class="c1"># Select epsilon-greedy action.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_epsilon</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">q_values</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_actions</span> <span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">action</span>

  <span class="k">def</span> <span class="nf">q_values</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation</span><span class="p">):</span>
    <span class="n">q_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">q_values</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">is_ready</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">):</span>
      <span class="c1"># If the replay buffer is not ready to sample from, do nothing.</span>
      <span class="k">return</span>

    <span class="c1"># Sample a minibatch of transitions from experience replay.</span>
    <span class="n">transitions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">)</span>

    <span class="c1"># Optionally unpack the transitions to lighten notation.</span>
    <span class="c1"># Note: each of these tensors will be of shape [batch_size, ...].</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">action</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">reward</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">discount</span><span class="p">)</span>
    <span class="n">next_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">transitions</span><span class="o">.</span><span class="n">next_state</span><span class="p">)</span>

    <span class="c1"># Compute the Q-values at next states in the transitions.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="c1">#################################################</span>
      <span class="c1"># Fill in missing code below (...),</span>
      <span class="c1"># then remove or comment the line below to test your implementation</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete the DQN Agent"</span><span class="p">)</span>
      <span class="c1">#################################################</span>
      <span class="c1">#TODO get the value of the next states evaluated by the target network</span>
      <span class="c1">#HINT: use self._target_network, defined above.</span>
      <span class="n">q_next_s</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># Shape [batch_size, num_actions].</span>
      <span class="n">max_q_next_s</span> <span class="o">=</span> <span class="n">q_next_s</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
      <span class="c1"># Compute the TD error and then the losses.</span>
      <span class="n">target_q_value</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">max_q_next_s</span>

    <span class="c1"># Compute the Q-values at original state.</span>
    <span class="n">q_s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

    <span class="c1"># Gather the Q-value corresponding to each action in the batch.</span>
    <span class="n">q_s_a</span> <span class="o">=</span> <span class="n">q_s</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Average the squared TD errors over the entire batch</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">target_q_value</span><span class="p">,</span> <span class="n">q_s_a</span><span class="p">)</span>

    <span class="c1"># Compute the gradients of the loss with respect to the q_network variables.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Apply the gradient update.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_current_step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_target_update_frequency</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_target_network</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_q_network</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
    <span class="c1"># Store the loss for logging purposes (see run_loop implementation above).</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">last_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">observe_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">add_first</span><span class="p">(</span><span class="n">timestep</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">observe</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">:</span> <span class="n">dm_env</span><span class="o">.</span><span class="n">TimeStep</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_replay_buffer</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">next_timestep</span><span class="p">)</span>


<span class="c1"># Create a convenient container for the SARS tuples required by NFQ.</span>
<span class="n">Transitions</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s1">'Transitions'</span><span class="p">,</span> <span class="p">[</span><span class="s1">'state'</span><span class="p">,</span> <span class="s1">'action'</span><span class="p">,</span> <span class="s1">'reward'</span><span class="p">,</span> <span class="s1">'discount'</span><span class="p">,</span> <span class="s1">'next_state'</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_d6d1b1d0.py"><em>Click for solution</em></a></p>
<div class="section" id="train-and-evaluate-the-dqn-agent">
<h4>Train and evaluate the DQN agent<a class="headerlink" href="#train-and-evaluate-the-dqn-agent" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Train and evaluate the DQN agent</span>

<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.25</span>  <span class="c1"># @param {type: "number"}</span>
<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># @param {type: "integer"}</span>
<span class="n">max_episode_length</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># @param {type: "integer"}</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">build_gridworld_task</span><span class="p">(</span>
    <span class="n">task</span><span class="o">=</span><span class="s1">'simple'</span><span class="p">,</span>
    <span class="n">observation_type</span><span class="o">=</span><span class="n">ObservationType</span><span class="o">.</span><span class="n">GRID</span><span class="p">,</span>
    <span class="n">max_episode_length</span><span class="o">=</span><span class="n">max_episode_length</span><span class="p">)</span>
<span class="n">environment</span><span class="p">,</span> <span class="n">environment_spec</span> <span class="o">=</span> <span class="n">setup_environment</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>

<span class="c1"># Build the agent's network.</span>
<span class="k">class</span> <span class="nc">Permute</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">order</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Permute</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">order</span> <span class="o">=</span> <span class="n">order</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">order</span><span class="p">)</span>

<span class="n">q_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Permute</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                    <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">384</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                          <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">environment_spec</span><span class="o">.</span><span class="n">actions</span><span class="o">.</span><span class="n">num_values</span><span class="p">)</span>
                          <span class="p">)</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span>
    <span class="n">environment_spec</span><span class="o">=</span><span class="n">environment_spec</span><span class="p">,</span>
    <span class="n">network</span><span class="o">=</span><span class="n">q_network</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
    <span class="n">target_update_frequency</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="n">returns</span> <span class="o">=</span> <span class="n">run_loop</span><span class="p">(</span>
    <span class="n">environment</span><span class="o">=</span><span class="n">environment</span><span class="p">,</span>
    <span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span>
    <span class="n">num_episodes</span><span class="o">=</span><span class="n">num_episodes</span><span class="p">,</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id3">
<h4>Visualise the learned <span class="math notranslate nohighlight">\(Q\)</span> values<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Visualise the learned $Q$ values</span>
<span class="c1"># Evaluate the policy for every state, similar to tabular agents above.</span>
<span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span> <span class="o">+</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>
<span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">_layout_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="c1"># Hack observation to see what the Q-network would output at that point.</span>
    <span class="n">environment</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">environment</span><span class="o">.</span><span class="n">get_obs</span><span class="p">()</span>
    <span class="n">q</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">q_values</span><span class="p">(</span><span class="n">obs</span><span class="p">))</span>
    <span class="n">pi</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">select_action</span><span class="p">(</span><span class="n">obs</span><span class="p">))</span>

<span class="n">plot_action_values</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id4">
<h4>Compare the greedy policy with the agent’s policy<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Compare the greedy policy with the agent's policy</span>

<span class="n">environment</span><span class="o">.</span><span class="n">plot_greedy_policy</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figtext</span><span class="p">(</span><span class="o">-</span><span class="mf">.08</span><span class="p">,</span> <span class="mf">.95</span><span class="p">,</span> <span class="s2">"Greedy policy using the learnt Q-values"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">environment</span><span class="o">.</span><span class="n">plot_policy</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figtext</span><span class="p">(</span><span class="o">-</span><span class="mf">.08</span><span class="p">,</span> <span class="mf">.95</span><span class="p">,</span> <span class="s2">"Policy using the agent's epsilon-greedy policy"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">''</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Note:</strong> You’ll get a better estimate of the value functions if you increase <code class="docutils literal notranslate"><span class="pre">num_episodes</span></code> and <code class="docutils literal notranslate"><span class="pre">max_episode_length</span></code>, but this will take longer to train. Feel free to play around after the day!</p>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-8-beyond-value-based-model-free-methods">
<h1>Section 8: Beyond Value Based Model-Free Methods<a class="headerlink" href="#section-8-beyond-value-based-model-free-methods" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-8-other-rl-methods">
<h2>Video 8: Other RL Methods<a class="headerlink" href="#video-8-other-rl-methods" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="cartpole-task">
<h2>Cartpole task<a class="headerlink" href="#cartpole-task" title="Permalink to this headline">¶</a></h2>
<p>Here we switch to training on a different kind of task, which has a continuous action space: Cartpole in <a class="reference external" href="https://gym.openai.com/">Gym</a>. As you recall from the video, policy-based methods are particularly well-suited for these kinds of tasks. We will be exploring two of those methods below.</p>
<center><img height="250" src="https://user-images.githubusercontent.com/10624937/42135683-dde5c6f0-7d13-11e8-90b1-8770df3e40cf.gif"/></center><div class="section" id="make-a-cartpole-environment-gym-make-cartpole-v1">
<h3>Make a CartPole environment, <code class="docutils literal notranslate"><span class="pre">gym.make('CartPole-v1')</span></code><a class="headerlink" href="#make-a-cartpole-environment-gym-make-cartpole-v1" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Make a CartPole environment, `gym.make('CartPole-v1')`</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">'CartPole-v1'</span><span class="p">)</span>

<span class="c1"># Set seeds</span>
<span class="n">env</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-8-1-policy-gradient">
<h2>Section 8.1: Policy gradient<a class="headerlink" href="#section-8-1-policy-gradient" title="Permalink to this headline">¶</a></h2>
<p>Now we will turn to policy gradient methods. Rather than defining the policy in terms of a value function, i.e. <span class="math notranslate nohighlight">\(\color{blue}\pi(\color{red}s) = \arg\max_{\color{blue}a}\color{green}Q(\color{red}s, \color{blue}a)\)</span>, we will directly parameterize the policy and write it as the distribution</p>
<div class="amsmath math notranslate nohighlight" id="equation-7b62fff8-56c8-49f9-85d4-194e33d00f30">
<span class="eqno">(101)<a class="headerlink" href="#equation-7b62fff8-56c8-49f9-85d4-194e33d00f30" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{blue}a_t \sim \color{blue}\pi_{\theta}(\color{blue}a_t|\color{red}s_t).
\end{equation}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\theta\)</span> represent the parameters of the policy. We will update the policy parameters using gradient ascent to <strong>maximize</strong> expected future reward.</p>
<p>One convenient way to represent the conditional distribution above is as a function that takes a state <span class="math notranslate nohighlight">\(\color{red}s\)</span> and returns a distribution over actions <span class="math notranslate nohighlight">\(\color{blue}a\)</span>.</p>
<p>Defined below is an agent which implements the REINFORCE algorithm.
REINFORCE (Williams 1992) is the simplest model-free general reinforcement learning technique.</p>
<p>The <strong>basic idea</strong> is to use probabilistic action choice. If the reward at the end turns out to be high, we make <strong>all</strong> actions in this sequence <strong>more likely</strong> (otherwise, we do the opposite).</p>
<p>This strategy could reinforce “bad” actions as well, however they will turn out to be part of trajectories with low reward and will likely not get accentuated.</p>
<p>From the lectures, we know that we need to compute</p>
<div class="amsmath math notranslate nohighlight" id="equation-5a557cbf-7262-4547-9f50-736da34f4d96">
<span class="eqno">(102)<a class="headerlink" href="#equation-5a557cbf-7262-4547-9f50-736da34f4d96" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\nabla J(\theta) 
= \mathbb{E}
\left[
  \sum_{t=0}^T \color{green} G_t 
  \nabla\log\color{blue}\pi_\theta(\color{red}{s_t})
\right]
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\color{green} G_t\)</span> is the sum over future rewards from time <span class="math notranslate nohighlight">\(t\)</span>, defined as</p>
<div class="amsmath math notranslate nohighlight" id="equation-d5f877cb-9a90-4f43-acb0-8f31d72d81ae">
<span class="eqno">(103)<a class="headerlink" href="#equation-d5f877cb-9a90-4f43-acb0-8f31d72d81ae" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{green} G_t 
= \sum_{n=t}^T \gamma^{n-t} 
\color{green} R(\color{red}{s_t}, \color{blue}{a_t}, \color{red}{s_{t+1}}).
\end{equation}\]</div>
<p>The algorithm below will collect the state, action, and reward data in its buffer until it reaches a full trajectory. It will then update its policy given the above gradient (and the Adam optimizer).</p>
<p>A policy gradient trains an agent without explicitly mapping the value for every state-action pair in an environment by taking small steps and updating the policy based on the reward associated with that step. In this section, we will build a small network that trains using policy gradient using PyTorch.</p>
<p>The agent can receive a reward immediately for an action or it can receive the award at a later time such as the end of the episode.</p>
<p>The policy function our agent will try to learn is <span class="math notranslate nohighlight">\(\pi_\theta(a,s)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> is the parameter vector, <span class="math notranslate nohighlight">\(s\)</span> is a particular state, and <span class="math notranslate nohighlight">\(a\)</span> is an action.</p>
<p>Monte-Carlo Policy Gradient approach will be used, which means the agent will run through an entire episode and then update policy based on the rewards obtained.</p>
<div class="section" id="set-the-hyperparameters-for-policy-gradient">
<h3>Set the hyperparameters for Policy Gradient<a class="headerlink" href="#set-the-hyperparameters-for-policy-gradient" title="Permalink to this headline">¶</a></h3>
<p>Only used in Policy Gradient Method:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set the hyperparameters for Policy Gradient</span>

<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">300</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># @param {type:"number"}</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>  <span class="c1"># @param {type:"number"}</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.6</span> <span class="c1"># @param {type:"number"}</span>

<span class="c1"># @markdown Only used in Policy Gradient Method:</span>
<span class="n">hidden_neurons</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># @param {type:"integer"}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-8-1-creating-a-simple-neural-network">
<h3>Coding Exercise 8.1: Creating a simple neural network<a class="headerlink" href="#coding-exercise-8-1-creating-a-simple-neural-network" title="Permalink to this headline">¶</a></h3>
<p>Below you will find some incomplete code. Fill in the missing code to construct the specified neural network.</p>
<p>Let us define a simple feed forward neural network with one hidden layer of 128 neurons and a dropout of 0.6. Let’s use Adam as our optimizer and a learning rate of 0.01. Use the hyperparameters already defined rather than using explicit values.</p>
<p>Using dropout will significantly improve the performance of the policy. Do compare your results with and without dropout and experiment with other hyper-parameter values as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PolicyGradientNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PolicyGradientNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">state_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="c1">#################################################</span>
    <span class="c1">## TODO for students: Define two linear layers</span>
    <span class="c1">## from the first expression</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: Create FF neural network."</span><span class="p">)</span>
    <span class="c1">#################################################</span>
    <span class="c1"># HINT: you can construct linear layers using nn.Linear(); what are the</span>
    <span class="c1"># sizes of the inputs and outputs of each of the layers? Also remember</span>
    <span class="c1"># that you need to use hidden_neurons (see hyperparameters section above).</span>
    <span class="c1">#   https://pytorch.org/docs/stable/generated/torch.nn.Linear.html</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="o">...</span>
    <span class="c1"># Episode policy and past rewards</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">past_policy</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reward_episode</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Overall reward and past loss</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">past_reward</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">past_loss</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">,</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">,</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_9aaf4a83.py"><em>Click for solution</em></a></p>
<p>Now let’s create an instance of the network we have defined and use Adam as the optimizer using the learning_rate as hyperparameter already defined above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">policy</span> <span class="o">=</span> <span class="n">PolicyGradientNet</span><span class="p">()</span>
<span class="n">pg_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="select-action">
<h3>Select Action<a class="headerlink" href="#select-action" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">select_action()</span></code> function chooses an action based on our policy probability distribution using the PyTorch distributions package.  Our policy returns a probability for each possible action in our action space (move left or move right) as an array of length two such as [0.7, 0.3].  We then choose an action based on these probabilities, record our history, and return our action.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
  <span class="c1">#Select an action (0 or 1) by running policy model and choosing based on the probabilities in state</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
  <span class="n">c</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
  <span class="n">action</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

  <span class="c1"># Add log probability of chosen action</span>
  <span class="k">if</span> <span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">)])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span> <span class="o">=</span> <span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">action</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="update-policy">
<h3>Update policy<a class="headerlink" href="#update-policy" title="Permalink to this headline">¶</a></h3>
<p>This function updates the policy.</p>
<div class="section" id="reward-g-t">
<h4>Reward <span class="math notranslate nohighlight">\(G_t\)</span><a class="headerlink" href="#reward-g-t" title="Permalink to this headline">¶</a></h4>
<p>We update our policy by taking a sample of the action value function <span class="math notranslate nohighlight">\(Q^{\pi_\theta} (s_t,a_t)\)</span> by playing through episodes of the game.  <span class="math notranslate nohighlight">\(Q^{\pi_\theta} (s_t,a_t)\)</span> is defined as the expected return by taking action <span class="math notranslate nohighlight">\(a\)</span> in state <span class="math notranslate nohighlight">\(s\)</span> following policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>We know that for every step the simulation continues we receive a reward of 1.  We can use this to calculate the policy gradient at each time step, where <span class="math notranslate nohighlight">\(r\)</span> is the reward for a particular state-action pair.  Rather than using the instantaneous reward, <span class="math notranslate nohighlight">\(r\)</span>, we instead use a long term reward <span class="math notranslate nohighlight">\( v_{t} \)</span> where <span class="math notranslate nohighlight">\(v_t\)</span> is the discounted sum of all future rewards for the length of the episode.   <span class="math notranslate nohighlight">\(v_{t}\)</span> is then,</p>
<div class="amsmath math notranslate nohighlight" id="equation-8754346c-02db-410a-9ff0-16277226e04c">
<span class="eqno">(104)<a class="headerlink" href="#equation-8754346c-02db-410a-9ff0-16277226e04c" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\color{green} G_t 
= \sum_{n=t}^T \gamma^{n-t} 
\color{green} R(\color{red}{s_t}, \color{blue}{a_t}, \color{red}{s_{t+1}}).
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is the discount factor (0.99).  For example, if an episode lasts 5 steps, the reward for each step will be [4.90, 3.94, 2.97, 1.99, 1].
Next we scale our reward vector by substracting the mean from each element and scaling to unit variance by dividing by the standard deviation.  This practice is common for machine learning applications and the same operation as Scikit Learn’s <strong><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">StandardScaler</a></strong>.  It also has the effect of compensating for future uncertainty.</p>
</div>
<div class="section" id="update-policy-equation">
<h4>Update Policy: equation<a class="headerlink" href="#update-policy-equation" title="Permalink to this headline">¶</a></h4>
<p>After each episode we apply Monte-Carlo Policy Gradient to improve our policy according to the equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-80ac7681-118d-47c5-abc8-fd5f6b508a10">
<span class="eqno">(105)<a class="headerlink" href="#equation-80ac7681-118d-47c5-abc8-fd5f6b508a10" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\Delta\theta_t = \alpha\nabla_\theta \, \log \pi_\theta (s_t,a_t)G_t
\end{equation}\]</div>
<p>We will then feed our policy history multiplied by our rewards to our optimizer and update the weights of our neural network using stochastic gradient <strong>ascent</strong>.  This should increase the likelihood of actions that got our agent a larger reward.</p>
<p>The following function <code class="docutils literal notranslate"><span class="pre">update_policy</span></code> updates the network weights and therefore the policy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_policy</span><span class="p">():</span>
  <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Discount future rewards back to the present using gamma</span>
  <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">policy</span><span class="o">.</span><span class="n">reward_episode</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">policy</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">R</span>
    <span class="n">rewards</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>

  <span class="c1"># Scale rewards</span>
  <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
  <span class="n">rewards</span> <span class="o">=</span> <span class="p">(</span><span class="n">rewards</span> <span class="o">-</span> <span class="n">rewards</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">rewards</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span>
                                          <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

  <span class="c1"># Calculate loss</span>
  <span class="n">pg_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span><span class="p">,</span>
                              <span class="n">Variable</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
  <span class="c1"># Update network weights</span>
  <span class="c1"># Use zero_grad(), backward() and step() methods of the optimizer instance.</span>
  <span class="n">pg_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
  <span class="n">pg_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
  <span class="c1"># Update the weights</span>
  <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">policy</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
      <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">pg_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

  <span class="c1"># Save and intialize episode past counters</span>
  <span class="n">policy</span><span class="o">.</span><span class="n">past_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pg_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
  <span class="n">policy</span><span class="o">.</span><span class="n">past_reward</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">reward_episode</span><span class="p">))</span>
  <span class="n">policy</span><span class="o">.</span><span class="n">past_policy</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">())</span>
  <span class="n">policy</span><span class="o">.</span><span class="n">reward_episode</span><span class="o">=</span> <span class="p">[]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<p>This is our main policy training loop.  For each step in a training episode, we choose an action, take a step through the environment, and record the resulting new state and reward.  We call update_policy() at the end of each episode to feed the episode history to our neural network and improve our policy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_gradient_train</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
  <span class="n">running_reward</span> <span class="o">=</span> <span class="mi">10</span>
  <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
      <span class="n">action</span> <span class="o">=</span> <span class="n">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
      <span class="c1"># Step through environment using chosen action</span>
      <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

      <span class="c1"># Save reward</span>
      <span class="n">policy</span><span class="o">.</span><span class="n">reward_episode</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># Used to determine when the environment is solved.</span>
    <span class="n">running_reward</span> <span class="o">=</span> <span class="p">(</span><span class="n">running_reward</span> <span class="o">*</span> <span class="n">gamma</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">time</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gamma</span><span class="p">))</span>

    <span class="n">update_policy</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Episode </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="se">\t</span><span class="s2">Last length: </span><span class="si">{</span><span class="n">time</span><span class="si">:</span><span class="s2">5.0f</span><span class="si">}</span><span class="s2">"</span>
            <span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">Average length: </span><span class="si">{</span><span class="n">running_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">running_reward</span> <span class="o">&gt;</span> <span class="n">env</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">reward_threshold</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Solved! Running reward is now </span><span class="si">{</span><span class="n">running_reward</span><span class="si">}</span><span class="s2"> "</span>
            <span class="sa">f</span><span class="s2">"and the last episode runs to </span><span class="si">{</span><span class="n">time</span><span class="si">}</span><span class="s2"> time steps!"</span><span class="p">)</span>
      <span class="k">break</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="run-the-model">
<h3>Run the model<a class="headerlink" href="#run-the-model" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">episodes</span> <span class="o">=</span> <span class="mi">500</span>   <span class="c1">#@param {type:"integer"}</span>
<span class="n">policy_gradient_train</span><span class="p">(</span><span class="n">episodes</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plot-the-results">
<h3>Plot the results<a class="headerlink" href="#plot-the-results" title="Permalink to this headline">¶</a></h3>
<div class="section" id="plot-the-training-performance-for-policy-gradient">
<h4>Plot the training performance for policy gradient<a class="headerlink" href="#plot-the-training-performance-for-policy-gradient" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plot the training performance for policy gradient</span>

<span class="k">def</span> <span class="nf">plot_policy_gradient_training</span><span class="p">():</span>
  <span class="n">window</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">episodes</span> <span class="o">/</span> <span class="mi">20</span><span class="p">)</span>

  <span class="n">fig</span><span class="p">,</span> <span class="p">((</span><span class="n">ax1</span><span class="p">),</span> <span class="p">(</span><span class="n">ax2</span><span class="p">))</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">]);</span>
  <span class="n">rolling_mean</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">past_reward</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
  <span class="n">std</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">past_reward</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rolling_mean</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">past_reward</span><span class="p">)),</span>
                   <span class="n">rolling_mean</span><span class="o">-</span><span class="n">std</span><span class="p">,</span> <span class="n">rolling_mean</span><span class="o">+</span><span class="n">std</span><span class="p">,</span>
                   <span class="n">color</span><span class="o">=</span><span class="s1">'orange'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Episode Length Moving Average (</span><span class="si">{</span><span class="n">window</span><span class="si">}</span><span class="s2">-episode window)"</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Episode'</span><span class="p">);</span> <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Episode Length'</span><span class="p">)</span>

  <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">past_reward</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Episode Length'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Episode'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Episode Length'</span><span class="p">)</span>

  <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_policy_gradient_training</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise-8-1-explore-different-hyperparameters">
<h3>Exercise 8.1: Explore different hyperparameters.<a class="headerlink" href="#exercise-8-1-explore-different-hyperparameters" title="Permalink to this headline">¶</a></h3>
<p>Try running the model again, by modifying the hyperparameters and observe the outputs. Be sure to rerun the function definition cells in order to pick up on the updated values.</p>
<p>What do you see when you</p>
<ol class="simple">
<li><p>increase learning rate</p></li>
<li><p>decrease learning rate</p></li>
<li><p>decrease gamma (<span class="math notranslate nohighlight">\(\gamma\)</span>)</p></li>
<li><p>increase number of hidden neurons in the network</p></li>
</ol>
</div>
</div>
<div class="section" id="section-8-2-actor-critic">
<h2>Section 8.2: Actor-critic<a class="headerlink" href="#section-8-2-actor-critic" title="Permalink to this headline">¶</a></h2>
<p>Recall the policy gradient</p>
<div class="amsmath math notranslate nohighlight" id="equation-5a29ada8-bea4-4f90-b408-3720ba9eae06">
<span class="eqno">(106)<a class="headerlink" href="#equation-5a29ada8-bea4-4f90-b408-3720ba9eae06" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\nabla J(\theta) 
= \mathbb{E}
\left[
  \sum_{t=0}^T \color{green} G_t 
  \nabla\log\color{blue}\pi_\theta(\color{red}{s_t})
\right]
\end{equation}\]</div>
<p>The policy parameters are updated using Monte Carlo technique and uses random samples. This introduces high variability in log probabilities and cumulative reward values. This leads to noisy gradients and can cause unstable learning.</p>
<p>One way to reduce variance and increase stability is subtracting the cumulative reward by a baseline:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ccd93c91-464a-451c-ac0a-b92190253627">
<span class="eqno">(107)<a class="headerlink" href="#equation-ccd93c91-464a-451c-ac0a-b92190253627" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\nabla J(\theta) 
= \mathbb{E}
\left[
   \sum_{t=0}^T \color{green} (G_t  - b)
  \nabla\log\color{blue}\pi_\theta(\color{red}{s_t})
\right]
\end{equation}\]</div>
<p>Intuitively, reducing cumulative reward will make smaller gradients and thus smaller and more stable (hopefully) updates.</p>
<p>From the lecture slides, we know that in Actor Critic Method:</p>
<ol class="simple">
<li><p>The “Critic” estimates the value function. This could be the action-value (the Q value) or state-value (the V value).</p></li>
<li><p>The “Actor” updates the policy distribution in the direction suggested by the Critic (such as with policy gradients).</p></li>
</ol>
<p>Both the Critic and Actor functions are parameterized with neural networks. The “Critic” network parameterizes the Q-value.</p>
<div class="section" id="set-the-hyperparameters-for-actor-critic">
<h3>Set the hyperparameters for Actor Critic<a class="headerlink" href="#set-the-hyperparameters-for-actor-critic" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set the hyperparameters for Actor Critic</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># @param {type:"number"}</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>  <span class="c1"># @param {type:"number"}</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.6</span>

<span class="c1"># Only used in Actor-Critic Method</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">256</span>  <span class="c1"># @param {type:"integer"}</span>

<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">300</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="actor-critic-network">
<h3>Actor Critic Network<a class="headerlink" href="#actor-critic-network" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ActorCriticNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">3e-4</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ActorCriticNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">num_actions</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">critic_linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">critic_linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">actor_linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">actor_linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_actions</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">all_rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">all_lengths</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">average_lengths</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">critic_linear1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
    <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">critic_linear2</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

    <span class="n">policy_dist</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actor_linear1</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
    <span class="n">policy_dist</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actor_linear2</span><span class="p">(</span><span class="n">policy_dist</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">value</span><span class="p">,</span> <span class="n">policy_dist</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id5">
<h3>Training<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">actor_critic_train</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
  <span class="n">all_lengths</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">average_lengths</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">all_rewards</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">entropy_term</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">steps</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
      <span class="n">value</span><span class="p">,</span> <span class="n">policy_dist</span> <span class="o">=</span> <span class="n">actor_critic</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
      <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
      <span class="n">dist</span> <span class="o">=</span> <span class="n">policy_dist</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

      <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dist</span><span class="p">))</span>
      <span class="n">log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">policy_dist</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)[</span><span class="n">action</span><span class="p">])</span>
      <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">dist</span><span class="p">))</span>
      <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

      <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
      <span class="n">values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
      <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
      <span class="n">entropy_term</span> <span class="o">+=</span> <span class="n">entropy</span>
      <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>

      <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">steps</span> <span class="o">==</span> <span class="n">num_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">qval</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">actor_critic</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">new_state</span><span class="p">)</span>
        <span class="n">qval</span> <span class="o">=</span> <span class="n">qval</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">all_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">))</span>
        <span class="n">all_lengths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>
        <span class="n">average_lengths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_lengths</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]))</span>
        <span class="k">if</span> <span class="n">episode</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"episode: </span><span class="si">{</span><span class="n">episode</span><span class="si">}</span><span class="s2">,</span><span class="se">\t</span><span class="s2">reward: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="si">}</span><span class="s2">,"</span>
                <span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">total length: </span><span class="si">{</span><span class="n">steps</span><span class="si">}</span><span class="s2">,"</span>
                <span class="sa">f</span><span class="s2">"</span><span class="se">\t</span><span class="s2">average length: </span><span class="si">{</span><span class="n">average_lengths</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">break</span>

    <span class="c1"># compute Q values</span>
    <span class="n">qvals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rewards</span><span class="p">))):</span>
      <span class="n">qval</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">qval</span>
      <span class="n">qvals</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">qval</span>

    <span class="c1">#update actor critic</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    <span class="n">qvals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">qvals</span><span class="p">)</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>

    <span class="n">advantage</span> <span class="o">=</span> <span class="n">qvals</span> <span class="o">-</span> <span class="n">values</span>
    <span class="n">actor_loss</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">log_probs</span> <span class="o">*</span> <span class="n">advantage</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">critic_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">advantage</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">ac_loss</span> <span class="o">=</span> <span class="n">actor_loss</span> <span class="o">+</span> <span class="n">critic_loss</span> <span class="o">+</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="n">entropy_term</span>

    <span class="n">ac_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">ac_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">ac_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

  <span class="c1"># Store results</span>
  <span class="n">actor_critic</span><span class="o">.</span><span class="n">average_lengths</span> <span class="o">=</span> <span class="n">average_lengths</span>
  <span class="n">actor_critic</span><span class="o">.</span><span class="n">all_rewards</span> <span class="o">=</span> <span class="n">all_rewards</span>
  <span class="n">actor_critic</span><span class="o">.</span><span class="n">all_lengths</span> <span class="o">=</span> <span class="n">all_lengths</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id6">
<h3>Run the model<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">episodes</span> <span class="o">=</span> <span class="mi">500</span>   <span class="c1"># @param {type:"integer"}</span>

<span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="n">num_inputs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">num_outputs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>

<span class="n">actor_critic</span> <span class="o">=</span> <span class="n">ActorCriticNet</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="n">ac_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

<span class="n">actor_critic_train</span><span class="p">(</span><span class="n">episodes</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id7">
<h3>Plot the results<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<div class="section" id="plot-the-training-performance-for-actor-critic">
<h4>Plot the training performance for Actor Critic<a class="headerlink" href="#plot-the-training-performance-for-actor-critic" title="Permalink to this headline">¶</a></h4>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plot the training performance for Actor Critic</span>
<span class="k">def</span> <span class="nf">plot_actor_critic_training</span><span class="p">(</span><span class="n">actor_critic</span><span class="p">,</span> <span class="n">episodes</span><span class="p">):</span>
  <span class="n">window</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">episodes</span> <span class="o">/</span> <span class="mi">20</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

  <span class="n">smoothed_rewards</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">all_rewards</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
  <span class="n">std</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">all_rewards</span><span class="p">)</span><span class="o">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">window</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">smoothed_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Smoothed rewards'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">smoothed_rewards</span><span class="p">)),</span>
                   <span class="n">smoothed_rewards</span> <span class="o">-</span> <span class="n">std</span><span class="p">,</span> <span class="n">smoothed_rewards</span> <span class="o">+</span> <span class="n">std</span><span class="p">,</span>
                   <span class="n">color</span><span class="o">=</span><span class="s1">'orange'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Episode'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Reward'</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">all_lengths</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'All lengths'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">actor_critic</span><span class="o">.</span><span class="n">average_lengths</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Average lengths'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Episode'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Episode length'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="n">plot_actor_critic_training</span><span class="p">(</span><span class="n">actor_critic</span><span class="p">,</span> <span class="n">episodes</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="exercise-8-3-effect-of-episodes-on-performance">
<h3>Exercise 8.3: Effect of episodes on performance<a class="headerlink" href="#exercise-8-3-effect-of-episodes-on-performance" title="Permalink to this headline">¶</a></h3>
<p>Change the episodes from 500 to 3000 and observe the performance impact.</p>
</div>
<div class="section" id="exercise-8-4-effect-of-learning-rate-on-performance">
<h3>Exercise 8.4: Effect of learning rate on performance<a class="headerlink" href="#exercise-8-4-effect-of-learning-rate-on-performance" title="Permalink to this headline">¶</a></h3>
<p>Modify the hyperparameters related to learning_rate and gamma and observe the impact on the performance.</p>
<p>Be sure to rerun the function definition cells in order to pick up on the updated values.</p>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-9-rl-in-the-real-world">
<h1>Section 9: RL in the real world<a class="headerlink" href="#section-9-rl-in-the-real-world" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-9-real-world-applications-and-ethics">
<h2>Video 9: Real-world applications and ethics<a class="headerlink" href="#video-9-real-world-applications-and-ethics" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="exercise-9-group-discussion">
<h2>Exercise 9: Group discussion<a class="headerlink" href="#exercise-9-group-discussion" title="Permalink to this headline">¶</a></h2>
<p>Form a group of 2-3 and have discussions (roughly 3 minutes each) of the following questions:</p>
<ol class="simple">
<li><p><strong>Safety</strong>: what are some safety issues that arise in RL that don’t arise with e.g. supervised learning?</p></li>
<li><p><strong>Generalization</strong>: What happens if your RL agent is presented with data it hasn’t trained on? (“goes out of distribution”)</p></li>
<li><p>How important do you think <strong>interpretability</strong> is in the ethical and safe deployment of RL agents in the real world?</p></li>
</ol>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D2_BasicReinforcementLearning/solutions/W3D2_Tutorial1_Solution_99944c89.py"><em>Click for solution</em></a></p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-10-how-to-learn-more">
<h1>Section 10: How to learn more<a class="headerlink" href="#section-10-how-to-learn-more" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-10-how-to-learn-more">
<h2>Video 10: How to learn more<a class="headerlink" href="#video-10-how-to-learn-more" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="appendix-and-further-reading">
<h1>Appendix and further reading<a class="headerlink" href="#appendix-and-further-reading" title="Permalink to this headline">¶</a></h1>
<p>Books and lecture notes</p>
<ul class="simple">
<li><p><a class="reference external" href="http://incompleteideas.net/book/RLbook2018.pdf">Reinforcement Learning: an Introduction by Sutton &amp; Barto</a></p></li>
<li><p><a class="reference external" href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Algorithms for Reinforcement Learning by Csaba Szepesvari</a></p></li>
</ul>
<p>Lectures and course</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/playlist?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-">RL Course by David Silver</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb">Reinforcement Learning Course | UCL &amp; DeepMind</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Emma Brunskill Stanford RL Course</a></p></li>
<li><p><a class="reference external" href="https://www.coursera.org/specializations/reinforcement-learning">RL Course on Coursera by Martha White &amp; Adam White</a></p></li>
</ul>
<p>More practical:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://spinningup.openai.com/en/latest/">Spinning Up in Deep RL by Josh Achiam</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2006.00979">Acme white paper</a> &amp; <a class="reference external" href="https://github.com/deepmind/acme/blob/master/examples/tutorial.ipynb">Colab tutorial</a></p></li>
</ul>
<br/>
<p><a class="reference external" href="https://twitter.com/FeryalMP/status/1407272291579355136?s=20">Link to the tweet thread with resources recommended by the community</a>.</p>
<br/>
<p>This Colab is based on the <a class="reference external" href="https://colab.research.google.com/github/eemlcommunity/PracticalSessions2020/blob/master/rl/EEML2020_RL_Tutorial.ipynb">EEML 2020 RL practical</a> by Feryal Behbahani &amp; Gheorghe Comanici. If you are interested in JAX you should try the colab. If you are interested in Tensorflow, there is also a version of the colab for the <a class="reference external" href="https://github.com/Feryal/rl_mlss_2020">MLSS 2020 RL Tutorial</a> that you can try :)</p>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W3D2_BasicReinforcementLearning/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
<div class="prev-next-bottom">
<a class="left-prev" href="../chapter_title.html" id="prev-link" title="previous page">Basic Reinforcement Learning</a>
<a class="right-next" href="../../W3D3_ReinforcementLearningForGames/chapter_title.html" id="next-link" title="next page">Reinforcement Learning For Games</a>
</div>
</div>
</div>
<footer class="footer mt-5 mt-md-0">
<div class="container">
<p>
        
          By Neuromatch<br>
        
            © Copyright 2021.<br>
</br></br></p>
</div>
</footer>
</main>
</div>
</div>
<script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>
</body>
</html>