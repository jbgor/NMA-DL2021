
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 2: Training loop of CNNs — Neuromatch Academy: Deep Learning</title>
<link href="../../../_static/css/theme.css" rel="stylesheet"/>
<link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<link as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script src="../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
<script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
<script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<link href="../../../_static/nma-dl-logo-square-4xp.jpeg" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="W2D1_Tutorial3.html" rel="next" title="Tutorial 3: Introduction to RNNs"/>
<link href="W2D1_Tutorial1.html" rel="prev" title="Tutorial 1: Introduction to CNNs"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</link></link></link></link></head>
<body data-offset="80" data-spy="scroll" data-target="#bd-toc-nav">
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
<div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<img alt="logo" class="logo" src="../../../_static/nma-dl-logo-square-4xp.jpeg"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Deep Learning</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main navigation" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
   Introduction
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using Discord
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  The Basics
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/chapter_title.html">
   Basics And Pytorch (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_BasicsAndPytorch/student/W1D1_Tutorial1.html">
     Tutorial 1: PyTorch
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/chapter_title.html">
   Linear Deep Learning (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html">
     Tutorial 1: Gradient Descent and AutoGrad
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial2.html">
     Tutorial 2: Learning Hyperparameters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_LinearDeepLearning/student/W1D2_Tutorial3.html">
     Tutorial 3: Deep linear neural networks
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/chapter_title.html">
   Multi Layer Perceptrons (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial1.html">
     Tutorial 1: Biological vs. Artificial neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_MultiLayerPerceptrons/student/W1D3_Tutorial2.html">
     Tutorial 2: Deep MLPs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_Optimization/chapter_title.html">
   Optimization (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_Optimization/student/W1D4_Tutorial1.html">
     Tutorial 1: Optimization techniques
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_Regularization/chapter_title.html">
   Regularization (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial1.html">
     Tutorial 1: Regularization techniques part 1
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_Regularization/student/W1D5_Tutorial2.html">
     Tutorial 2: Regularization techniques part 2
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Doing more with fewer parameters
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Convnets And Recurrent Neural Networks (W2D1)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="W2D1_Tutorial1.html">
     Tutorial 1: Introduction to CNNs
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 2: Training loop of CNNs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W2D1_Tutorial3.html">
     Tutorial 3: Introduction to RNNs
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_ModernConvnets/chapter_title.html">
   Modern Convnets (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial1.html">
     Tutorial 1: Learn how to use modern convnets
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_ModernConvnets/student/W2D2_Tutorial2.html">
     Tutorial 2: Facial recognition using modern convnets
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/chapter_title.html">
   Modern Recurrent Neural Networks (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial1.html">
     Tutorial 1: Modeling sequencies and encoding text
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_ModernRecurrentNeuralNetworks/student/W2D3_Tutorial2.html">
     Tutorial 2: Modern RNNs and their variants
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/chapter_title.html">
   Attention And Transformers (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_AttentionAndTransformers/student/W2D4_Tutorial1.html">
     Tutorial 1: Learn how to work with Transformers
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D5_GenerativeModels/chapter_title.html">
   Generative Models (W2D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial1.html">
     Tutorial 1: Variational Autoencoders (VAEs)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial2.html">
     Tutorial 2: Introduction to GANs and Density Ratio Estimation Perspective of GANs
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D5_GenerativeModels/student/W2D5_Tutorial3.html">
     Tutorial 3: Conditional GANs and Implications of GAN Technology
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Advanced topics
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/chapter_title.html">
   Unsupervised And Self Supervised Learning (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_UnsupervisedAndSelfSupervisedLearning/student/W3D1_Tutorial1.html">
     Tutorial 1: Un/Self-supervised learning methods
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/chapter_title.html">
   Basic Reinforcement Learning (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_BasicReinforcementLearning/student/W3D2_Tutorial1.html">
     Tutorial 1: Introduction to Reinforcement Learning
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/chapter_title.html">
   Reinforcement Learning For Games (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_ReinforcementLearningForGames/student/W3D3_Tutorial1.html">
     Tutorial 1: Learn to play games with RL
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ContinualLearning/chapter_title.html">
   Continual Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial1.html">
     Tutorial 1: Introduction to Continual Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ContinualLearning/student/W3D4_Tutorial2.html">
     Tutorial 2: Out-of-distribution (OOD) Learning
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction to projects
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through2_DL.html">
     Modeling Steps 1 - 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_3through4_DL.html">
     Modeling Steps 3 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through6_DL.html">
     Modeling Steps 5 - 6
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_7through9_DL.html">
     Modeling Steps 7 - 9
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_10_DL.html">
     Modeling Steps 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProjectDL.html">
     Example Data Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModelingProjectDL.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/Example_Deep_Learning_Project.html">
     Example Deep Learning Project
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/projects_overview.html">
   Project Templates
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ComputerVision/README.html">
     Computer Vision
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/em_synapses.html">
       Knowledge Extraction from a Convolutional Neural Network
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/spectrogram_analysis.html">
       Music classification and generation with spectrograms
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/screws.html">
       Something Screwy - image recognition, detection, and classification of screws
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/image_alignment.html">
       Image Alignment
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/data_augmentation.html">
       Data Augmentation in image classification models
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ComputerVision/transfer_learning.html">
       Transfer Learning
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/ReinforcementLearning/README.html">
     Reinforcement Learning
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/robolympics.html">
       NMA Robolympics: Controlling robots using reinforcement learning
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/lunar_lander.html">
       Performance Analysis of DQN Algorithm on the Lunar Lander task
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ReinforcementLearning/human_rl.html">
<strong>
        ⚠️ This is a work in progress.
       </strong>
</a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/README.html">
     Natural Language Processing
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/NaturalLanguageProcessing/ideas_and_datasets.html">
       Ideas
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/Neuroscience/README.html">
     Neuroscience
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/slides.html">
       Slides
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/ideas_and_datasets.html">
       Ideas
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/pose_estimation.html">
       Animal Pose Estimation
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/cellular_segmentation.html">
       Segmentation and Denoising
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/algonauts_videos.html">
       Load algonauts videos
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/blurry_vision.html">
       Vision with Lost Glasses: Modelling how the brain deals with noisy input
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/finetuning_fmri.html">
       Moving beyond Labels: Finetuning CNNs on BOLD response
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/Neuroscience/neuro_seq_to_seq.html">
       Focus on what matters: inferring low-dimensional dynamics from neural recordings
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/datasets_and_models.html">
   Models and Data sets
  </a>
</li>
</ul>
</div>
</nav> <!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
<div class="topbar container-xl fixed-top">
<div class="topbar-contents row">
<div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
<div class="col pl-md-4 topbar-main">
<button aria-controls="site-navigation" aria-expanded="true" aria-label="Toggle navigation" class="navbar-toggler ml-0" data-placement="left" data-target=".site-navigation" data-toggle="tooltip" id="navbar-toggler" title="Toggle navigation" type="button">
<i class="fas fa-bars"></i>
<i class="fas fa-arrow-left"></i>
<i class="fas fa-arrow-up"></i>
</button>
<div class="dropdown-buttons-trigger">
<button aria-label="Download this page" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fas fa-download"></i></button>
<div class="dropdown-buttons">
<!-- ipynb file if we had a myst markdown file -->
<!-- Download raw file -->
<a class="dropdown-buttons" href="../../../_sources/tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.ipynb"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Download source file" type="button">.ipynb</button></a>
<!-- Download PDF via print -->
<button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" id="download-print" onclick="window.print()" title="Print to PDF" type="button">.pdf</button>
</div>
</div>
<!-- Source interaction buttons -->
<div class="dropdown-buttons-trigger">
<button aria-label="Connect with source repository" class="btn btn-secondary topbarbtn" id="dropdown-buttons-trigger"><i class="fab fa-github"></i></button>
<div class="dropdown-buttons sourcebuttons">
<a class="repository-button" href="https://github.com/NeuromatchAcademy/course-content-dl"><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Source repository" type="button"><i class="fab fa-github"></i>repository</button></a>
<a class="issues-button" href="https://github.com/NeuromatchAcademy/course-content-dl/issues/new?title=Issue%20on%20page%20%2Ftutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.html&amp;body=Your%20issue%20content%20here."><button class="btn btn-secondary topbarbtn" data-placement="left" data-toggle="tooltip" title="Open an issue" type="button"><i class="fas fa-lightbulb"></i>open issue</button></a>
</div>
</div>
<!-- Full screen (wrap in <a> to have style consistency -->
<a class="full-screen-button"><button aria-label="Fullscreen mode" class="btn btn-secondary topbarbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode" type="button"><i class="fas fa-expand"></i></button></a>
<!-- Launch buttons -->
</div>
<!-- Table of contents -->
<div class="d-none d-md-block col-md-2 bd-toc show">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
            </div>
<nav id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 2: Training loop of CNNs
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-slides">
     Tutorial slides
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#install-dependencies">
     Install dependencies
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-random-seed">
     Set random seed
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#set-device-gpu-or-cpu-execute-set-device">
     Set device (GPU or CPU). Execute
     <code class="docutils literal notranslate">
<span class="pre">
       set_device()
      </span>
</code>
</a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-training-a-cnn">
   Section 1: Training a CNN
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-writing-your-own-training-loop">
     Video 1: Writing your own training loop
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-understand-the-dataset">
     Section 1.1: Understand the Dataset
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-the-training-loop">
       Video 2: The Training Loop
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-backpropagation-reminder">
     Section 1.2: Backpropagation Reminder
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#load-a-sample-dataset">
       Load a sample dataset
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-3-fashion-mnist-dataset">
     Section 1.3: Fashion-MNIST dataset
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#getting-the-dataloaders-run-me">
       Getting the DataLoaders (Run Me)
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-code-the-training-loop">
     Coding Exercise 1: Code the training loop
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-1-overfitting">
     Think! 1: Overfitting
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-overfitting-symptoms-and-cures">
   Section 2: Overfitting - symptoms and cures
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-1-adding-regularization">
     Coding Exercise 2.1: Adding Regularization
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-2-1-regularization">
     Think! 2.1: Regularization
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-2-dropout-exploration">
     Interactive Demo 2: Dropout exploration
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-2-how-much-does-augmentation-help">
     Coding Exercise 2.2: How much does augmentation help?
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-3-1-data-augmentation">
     Think! 3.1: Data Augmentation
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="row" id="main-content">
<div class="col-12 col-md-9 pl-md-3 pr-md-0">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/student/W2D1_Tutorial2.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a></p>
<div class="section" id="tutorial-2-training-loop-of-cnns">
<h1>Tutorial 2: Training loop of CNNs<a class="headerlink" href="#tutorial-2-training-loop-of-cnns" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 2, Day 1: Convnets And Recurrent Neural Networks</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Dawn McKnight, Richard Gerum, Cassidy Pirlot, Rohan Saha, Liam Peet-Pare, Saeed Najafi, Alona Fyshe</p>
<p><strong>Content reviewers:</strong> Saeed Salehi, Lily Cheng, Yu-Fang Yang, Polina Turishcheva, Bettina Hein</p>
<p><strong>Content editors:</strong> Nina Kudryashova, Anmol Gupta, Spiros Chavlis</p>
<p><strong>Production editors:</strong> Alex Tran-Van-Minh, Spiros Chavlis</p>
<p><em>Based on material from:</em> Konrad Kording, Hmrishav Bandyopadhyay, Rahul Shekhar, Tejas Srivastava</p>
<p><strong>Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs</strong></p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial Objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p>At the end of this tutorial, we will be able to:</p>
<ul class="simple">
<li><p>Understand pooling</p></li>
<li><p>Code a simple Convolutional Neural Nework (CNN) in pytorch</p></li>
</ul>
<div class="section" id="tutorial-slides">
<h2>Tutorial slides<a class="headerlink" href="#tutorial-slides" title="Permalink to this headline">¶</a></h2>
<p>These are the slides for the videos in this tutorial</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html">
<iframe allowfullscreen="" frameborder="0" height="480" src="https://mfr.ca-1.osf.io/render?url=https://osf.io/mkgqs/?direct%26mode=render%26action=download%26mode=render" width="854"></iframe>
</div></div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="section" id="install-dependencies">
<h2>Install dependencies<a class="headerlink" href="#install-dependencies" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Install dependencies</span>
<span class="o">!</span>pip install livelossplot --quiet
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class="-Color -Color-Yellow">WARNING: You are using pip version 21.2.1; however, version 21.2.2 is available.</span>
<span class="-Color -Color-Yellow">You should consider upgrading via the '/opt/hostedtoolcache/Python/3.7.11/x64/bin/python -m pip install --upgrade pip' command.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">torchvision.datasets</span> <span class="k">as</span> <span class="nn">datasets</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>

<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span><span class="p">,</span> <span class="n">trange</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>       <span class="c1"># interactive display</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"mpl_toolkits.legacy_colorbar"</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="s2">"matplotlib"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/ipykernel_launcher.py:6: MatplotlibDeprecationWarning: 
The mpl_toolkits.legacy_colorbar rcparam was deprecated in Matplotlib 3.4 and will be removed two minor releases later.
  
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions">
<h2>Helper functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper functions</span>

<span class="c1"># just returns accuracy on test data</span>
<span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">):</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

  <span class="n">acc</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
  <span class="k">return</span> <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">acc</span><span class="si">}</span><span class="s2">%"</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plotting-functions">
<h2>Plotting functions<a class="headerlink" href="#plotting-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plotting functions</span>

<span class="c1"># code to plot loss and accuracy</span>
<span class="k">def</span> <span class="nf">plot_loss_accuracy</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">validation_loss</span><span class="p">,</span> <span class="n">validation_acc</span><span class="p">):</span>
  <span class="n">epochs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)),</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Training Loss'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)),</span> <span class="n">validation_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Validation Loss'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Epochs'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Loss'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Epoch vs Loss'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

  <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)),</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Training Accuracy'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)),</span> <span class="n">validation_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Validation Accuracy'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Epochs'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Accuracy'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Epoch vs Accuracy'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mf">15.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">)</span>
  <span class="c1">#plt.show()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-random-seed">
<h2>Set random seed<a class="headerlink" href="#set-random-seed" title="Permalink to this headline">¶</a></h2>
<p>Executing <code class="docutils literal notranslate"><span class="pre">set_seed(seed=seed)</span></code> you are setting the seed</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set random seed</span>

<span class="c1"># @markdown Executing `set_seed(seed=seed)` you are setting the seed</span>

<span class="c1"># for DL its critical to set the random seed so that students can have a</span>
<span class="c1"># baseline to compare their results to expected results.</span>
<span class="c1"># Read more here: https://pytorch.org/docs/stable/notes/randomness.html</span>

<span class="c1"># Call `set_seed` function in the exercises to ensure reproducibility.</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Random seed </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1"> has been set.'</span><span class="p">)</span>


<span class="c1"># In case that `DataLoader` is used</span>
<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
  <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="set-device-gpu-or-cpu-execute-set-device">
<h2>Set device (GPU or CPU). Execute <code class="docutils literal notranslate"><span class="pre">set_device()</span></code><a class="headerlink" href="#set-device-gpu-or-cpu-execute-set-device" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Set device (GPU or CPU). Execute `set_device()`</span>
<span class="c1"># especially if torch modules used.</span>

<span class="c1"># inform the user if the notebook uses GPU or CPU.</span>

<span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"WARNING: For this notebook to perform best, "</span>
        <span class="s2">"if possible, in the menu under `Runtime` -&gt; "</span>
        <span class="s2">"`Change runtime type.`  select `GPU` "</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook."</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SEED</span> <span class="o">=</span> <span class="mi">2021</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -&gt; `Change runtime type.`  select `GPU` 
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-training-a-cnn">
<h1>Section 1: Training a CNN<a class="headerlink" href="#section-1-training-a-cnn" title="Permalink to this headline">¶</a></h1>
<p>In the last section we coded up a CNN, but trained it with some predefined functions.  In this section, we will walk through an example of training loop for a convolution net. In this section, we will train a CNN using convolution layers and maxpool and then observe what the training and validation curves look like. In Section 6, we will add regularization and data augmentation to see what effects they have on the curves and why it is important to incorporate them while training our network.
<br/></p>
<div class="section" id="video-1-writing-your-own-training-loop">
<h2>Video 1: Writing your own training loop<a class="headerlink" href="#video-1-writing-your-own-training-loop" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "fdb7a146079b4bb1a21c44b8d8b2a6d3"}
</script></div>
</div>
</div>
<div class="section" id="section-1-1-understand-the-dataset">
<h2>Section 1.1: Understand the Dataset<a class="headerlink" href="#section-1-1-understand-the-dataset" title="Permalink to this headline">¶</a></h2>
<p>The dataset we are going to use for this task is called Fashion-MNIST. It consists of a training set of 60,000 examples and a test set of 10,000 examples. We further divide the test set into a validation set and a test set (8000 and 2000 resp). Each example is a 28*28 gray scale image, associated with a label from 10 classes. Following are the labels of the dataset:</p>
<p>0 T-shirt/top <br/>
1 Trouser <br/>
2 Pullover <br/>
3 Dress <br/>
4 Coat <br/>
5 Sandal <br/>
6 Shirt <br/>
7 Sneaker <br/>
8 Bag <br/>
9 Ankle boot <br/></p>
<p><strong>NOTE:</strong> we will reduce the dataset to just the two categories T-shirt/top and Shirt to reduce the training time from about 10min to 2min. We later provide pretrained results to give you an idea how the results would look on the whole dataset.</p>
<p>Getting Fashion-Mnist Data</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Getting Fashion-Mnist Data</span>

<span class="k">def</span> <span class="nf">get_fashion_mnist_dataset</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
  <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
      <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
      <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
      <span class="p">])</span>

  <span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>

  <span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span>  <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                    <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
  <span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">validation_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span>
                                                              <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)),</span>
                                                              <span class="nb">int</span><span class="p">(</span><span class="mf">0.2</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">))])</span>

  <span class="k">return</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">,</span> <span class="n">test_data</span>

<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">get_fashion_mnist_dataset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "75128e9cdb40483190b291b4b23a1d89"}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "b60a6d2e15e247fdb4e47b7e14f1bdc9"}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "9feff9eb19de419b8cacc17c3ec1c147"}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw

Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "2467865d32b4411f8b24484487734e1d"}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw

Random seed 2021 has been set.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Reducing Fashion-Mnist Data (to two categories)</span>

<span class="c1"># @markdown *NOTE: if you want to train on the whole dataset, just run the cell above</span>
<span class="c1"># @markdown and do not execute this cell.*</span>
<span class="c1"># need to split into train, validation, test</span>
<span class="k">def</span> <span class="nf">reduce_classes</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
  <span class="c1"># only want T-Shirts (0) and Shirts (6) labels</span>
  <span class="n">train_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">targets</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">targets</span> <span class="o">==</span> <span class="mi">6</span><span class="p">)</span>
  <span class="n">data</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
  <span class="n">data</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>

  <span class="c1"># convert Xs predictions to 1, Os predictions to 0</span>
  <span class="n">data</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">targets</span> <span class="o">==</span> <span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="k">return</span> <span class="n">data</span>


<span class="k">def</span> <span class="nf">get_fashion_mnist_dataset</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
  <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
      <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
      <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
      <span class="p">])</span>

  <span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                     <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
  <span class="n">train_data</span> <span class="o">=</span> <span class="n">reduce_classes</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

  <span class="n">test_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span>  <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                    <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
  <span class="n">test_data</span> <span class="o">=</span> <span class="n">reduce_classes</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

  <span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">validation_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span>
                                                             <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.8</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)),</span>
                                                              <span class="nb">int</span><span class="p">(</span><span class="mf">0.2</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">))])</span>

  <span class="k">return</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">,</span> <span class="n">test_data</span>


<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">train_data</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">get_fashion_mnist_dataset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
</pre></div>
</div>
</div>
</div>
<p>Here’s some code to visualize the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">,</span> <span class="n">ax4</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'gray'</span><span class="p">))</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'gray'</span><span class="p">))</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'gray'</span><span class="p">))</span>
<span class="n">ax4</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span><span class="mi">3</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">'gray'</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mf">18.5</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/W2D1_Tutorial2_34_0.png" src="../../../_images/W2D1_Tutorial2_34_0.png"/>
</div>
</div>
<p>Take a minute with your pod and talk about which classes you think would be most confusable.  How hard will it be to differentiate t-shirt/tops from shirts?</p>
<div class="section" id="video-2-the-training-loop">
<h3>Video 2: The Training Loop<a class="headerlink" href="#video-2-the-training-loop" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "6e155689064f4317969d036a3f430bc9"}
</script></div>
</div>
</div>
</div>
<div class="section" id="section-1-2-backpropagation-reminder">
<h2>Section 1.2: Backpropagation Reminder<a class="headerlink" href="#section-1-2-backpropagation-reminder" title="Permalink to this headline">¶</a></h2>
<p><em>Feel free to skip if you’ve got a good handle on Backpropagation</em></p>
<p>We know that we multiply the input data/tensors with weight matrices to obtain some output. Initially, we don’t know what the actual weight matrices are so we initialize them with some random values. These random weight matrices when applied as a transformation on the input gives us some output. At first the outputs/predictions will match the true labels only by chance.</p>
<p>To improve performance, we need to change the weight matrices so that the predicted outputs are similar to the true outputs (labels). We first calculate how far away the predicted outputs are to the true outputs using a loss function. Based on the loss function, we change the values of our weight matrices using the gradients of the error with respect to the weight matrices.</p>
<p>Since we are using PyTorch throughout the course, we will use the built-in functions to update the weights. We call the <code class="docutils literal notranslate"><span class="pre">backward()</span></code> method on our ‘loss’ variable to calculate the gradients/derivatives with respect to all the weight matrices and biases. And then we call the <code class="docutils literal notranslate"><span class="pre">step()</span></code> method on the optimizer variable to apply the gradient updates to our weight matrices.</p>
<p>Here’s an animation of backpropagation works.</p>
<figure>
<center><img src="https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif"/>
<figcaption> Stride Two </figcaption>
</center>
</figure>
<p><a class="reference external" href="https://machinelearningknowledge.ai/animated-explanation-of-feed-forward-neural-network-architecture/">This</a> article has more animations.</p>
<p>Let’s first see a sample training loop. First, we create the network and load a dataset. Then we look at the training loop.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a sample network</span>
<span class="k">class</span> <span class="nc">emnist_net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># First define the layers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">26</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># Conv layer 1.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Conv layer 2.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Fully connected layer 1.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="o">*</span><span class="mi">7</span><span class="o">*</span><span class="mi">64</span><span class="p">)</span>  <span class="c1"># You have to first flatten the ourput from the</span>
                            <span class="c1"># previous convolution layer.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Fully connected layer 2.</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># x = F.softmax(x)</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="load-a-sample-dataset">
<h3>Load a sample dataset<a class="headerlink" href="#load-a-sample-dataset" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown ### Load a sample dataset</span>
<span class="c1"># Load the data</span>
<span class="n">mnist_train</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">EMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">"./datasets"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                              <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                              <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">'letters'</span><span class="p">)</span>
<span class="n">mnist_test</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">EMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">"./datasets"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                             <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                             <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s1">'letters'</span><span class="p">)</span>

<span class="c1"># labels should start from 0</span>
<span class="n">mnist_train</span><span class="o">.</span><span class="n">targets</span> <span class="o">-=</span> <span class="mi">1</span>
<span class="n">mnist_test</span><span class="o">.</span><span class="n">targets</span> <span class="o">-=</span> <span class="mi">1</span>

<span class="c1"># create data loaders</span>
<span class="n">g_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
<span class="n">g_seed</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                           <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span>
                                           <span class="n">generator</span><span class="o">=</span><span class="n">g_seed</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                          <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span>
                                          <span class="n">generator</span><span class="o">=</span><span class="n">g_seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to ./datasets/EMNIST/raw/gzip.zip
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "29cc0ce44bf14131a9a318ab6c85a6e3"}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting ./datasets/EMNIST/raw/gzip.zip to ./datasets/EMNIST/raw
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training</span>
<span class="c1"># Instantiate model</span>
<span class="c1"># Puts the Model on the GPU (Select runtime-type as GPU</span>
<span class="c1">#                            from the 'Runtime-&gt;Change Runtime type' option).</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">emnist_net</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

<span class="c1"># Loss and Optimizer</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># &lt;---- change here</span>

<span class="c1"># Iterate through train set minibatchs</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>  <span class="c1"># &lt;---- change here</span>
  <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>

    <span class="c1"># Zero out the gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># Fill this out.</span>

    <span class="c1"># Forward pass</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">images</span>
    <span class="c1"># Move the data to GPU for faster execution.</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">labs</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Calculate loss.</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">labs</span><span class="p">)</span>

    <span class="c1"># Backpropagation and gradient update.</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># Calculate gradients.</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># Apply gradient udpate.</span>


<span class="c1">## Testing</span>
<span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mnist_test</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="c1"># Iterate through test set minibatchs</span>
  <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>
    <span class="c1"># Forward pass</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">images</span>
      <span class="c1"># Move the data to GPU for faster execution.</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">labs</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labs</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Test accuracy: </span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "b4876f24eb784fcd97e3b975d73867db"}
</script><script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "5a11ca16aea24e87a2eb781de00905ef"}
</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_9116</span><span class="o">/</span><span class="mf">701509453.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span>     <span class="c1"># Move the data to GPU for faster execution.</span>
<span class="g g-Whitespace">     </span><span class="mi">21</span>     <span class="n">x</span><span class="p">,</span> <span class="n">labs</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">22</span>     <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">23</span> 
<span class="g g-Whitespace">     </span><span class="mi">24</span>     <span class="c1"># Calculate loss.</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1049</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1050</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1051</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1052</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1053</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">/tmp/ipykernel_9116/395564816.py</span> in <span class="ni">forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span>   <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="g g-Whitespace">     </span><span class="mi">12</span>     <span class="c1"># Conv layer 1.</span>
<span class="ne">---&gt; </span><span class="mi">13</span>     <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span>     <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span>     <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/nn/modules/module.py</span> in <span class="ni">_call_impl</span><span class="nt">(self, *input, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1049</span>         <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1050</span>                 <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1051</span>             <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1052</span>         <span class="c1"># Do not call functions when jit is used</span>
<span class="g g-Whitespace">   </span><span class="mi">1053</span>         <span class="n">full_backward_hooks</span><span class="p">,</span> <span class="n">non_full_backward_hooks</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/nn/modules/conv.py</span> in <span class="ni">forward</span><span class="nt">(self, input)</span>
<span class="g g-Whitespace">    </span><span class="mi">441</span> 
<span class="g g-Whitespace">    </span><span class="mi">442</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">443</span>         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_conv_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">444</span> 
<span class="g g-Whitespace">    </span><span class="mi">445</span> <span class="k">class</span> <span class="nc">Conv3d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>

<span class="nn">/opt/hostedtoolcache/Python/3.7.11/x64/lib/python3.7/site-packages/torch/nn/modules/conv.py</span> in <span class="ni">_conv_forward</span><span class="nt">(self, input, weight, bias)</span>
<span class="g g-Whitespace">    </span><span class="mi">438</span>                             <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">439</span>         <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
<span class="ne">--&gt; </span><span class="mi">440</span>                         <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">441</span> 
<span class="g g-Whitespace">    </span><span class="mi">442</span>     <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<p>You already coded the structure of a CNN. Now, you are going to implement the training loop for a CNN.</p>
<ul class="simple">
<li><p>Choose the correct criterion</p></li>
<li><p>Code up the training part (calculating gradients, loss, stepping forward)</p></li>
<li><p>Keep a track of the running loss i.e for each epoch we want to to know the average loss of the batch size. We have already done the same for accuracy for you.</p></li>
</ul>
</div>
</div>
<div class="section" id="section-1-3-fashion-mnist-dataset">
<h2>Section 1.3: Fashion-MNIST dataset<a class="headerlink" href="#section-1-3-fashion-mnist-dataset" title="Permalink to this headline">¶</a></h2>
<p>Now Let us train on the actual Fashion-MNIST dataset.</p>
<div class="section" id="getting-the-dataloaders-run-me">
<h3>Getting the DataLoaders (Run Me)<a class="headerlink" href="#getting-the-dataloaders-run-me" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown #### Getting the DataLoaders (Run Me)</span>
<span class="k">def</span> <span class="nf">get_data_loaders</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">validation_dataset</span><span class="p">,</span> <span class="n">test_dataset</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span>
                     <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>

  <span class="n">g_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
  <span class="n">g_seed</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

  <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span>
                            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                            <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                            <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span>
                            <span class="n">generator</span><span class="o">=</span><span class="n">g_seed</span><span class="p">)</span>
  <span class="n">validation_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">validation_dataset</span><span class="p">,</span>
                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                 <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                 <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span>
                                 <span class="n">generator</span><span class="o">=</span><span class="n">g_seed</span><span class="p">)</span>
  <span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span>
                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                           <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                           <span class="n">worker_init_fn</span><span class="o">=</span><span class="n">seed_worker</span><span class="p">,</span>
                           <span class="n">generator</span><span class="o">=</span><span class="n">g_seed</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">validation_loader</span><span class="p">,</span> <span class="n">test_loader</span>


<span class="n">train_loader</span><span class="p">,</span> <span class="n">validation_loader</span><span class="p">,</span> <span class="n">test_loader</span> <span class="o">=</span> <span class="n">get_data_loaders</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span>
                                                                <span class="n">validation_data</span><span class="p">,</span>
                                                                <span class="n">test_data</span><span class="p">,</span> <span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This cell contains the code for the CNN we will be using in this section.</span>
<span class="k">class</span> <span class="nc">FMNIST_Net1</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FMNIST_Net1</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">9216</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-1-code-the-training-loop">
<h2>Coding Exercise 1: Code the training loop<a class="headerlink" href="#coding-exercise-1-code-the-training-loop" title="Permalink to this headline">¶</a></h2>
<p>Now try coding the training loop.</p>
<p>You should first have a <code class="docutils literal notranslate"><span class="pre">criterion</span></code> defined (you can use <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> here, which you learned about last week) so that you can calculate the loss. Next, you should to put everything together. Start the training process by first obtaining the model output, calculating the loss, and finally updating the weights.</p>
<p><em>Don’t forget to zero out the gradients.</em></p>
<p>NOTE: The comments in the <code class="docutils literal notranslate"><span class="pre">train</span></code> function provides many hints that will help you fill in the missing code. This will give you a solid understanding of the different steps involved in the training loop.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">validation_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>

  <span class="n">criterion</span> <span class="o">=</span>  <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                            <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
  <span class="n">train_loss</span><span class="p">,</span> <span class="n">validation_loss</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
  <span class="n">train_acc</span><span class="p">,</span> <span class="n">validation_acc</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
  <span class="k">with</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">),</span> <span class="n">unit</span><span class="o">=</span><span class="s1">'epoch'</span><span class="p">)</span> <span class="k">as</span> <span class="n">tepochs</span><span class="p">:</span>
    <span class="n">tepochs</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s1">'Training'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tepochs</span><span class="p">:</span>
      <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
      <span class="c1"># keeps track of the running loss</span>
      <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.</span>
      <span class="n">correct</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
      <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="c1">####################################################################</span>
        <span class="c1"># Fill in missing code below (...),</span>
        <span class="c1"># then remove or comment the line below to test your function</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Update the steps of the train loop"</span><span class="p">)</span>
        <span class="c1">####################################################################</span>
        <span class="c1"># COMPLETE CODE FOR TRAINING LOOP by following these steps</span>
        <span class="c1"># 1. Get the model output (call the model with the data from this batch)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="o">...</span>

        <span class="c1"># 2. Zero the gradients out (i.e. reset the gradient that the optimizer</span>
        <span class="c1">#                       has collected so far with optimizer.zero_grad())</span>
        <span class="o">...</span>

        <span class="c1"># 3. Get the Loss (call the loss criterion with the model's output</span>
        <span class="c1">#                  and the target values)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>

        <span class="c1"># 4. Calculate the gradients (do the pass backwards from the loss</span>
        <span class="c1">#                             with loss.backward())</span>
        <span class="o">...</span>

        <span class="c1"># 5. Update the weights (using the training step of the optimizer,</span>
        <span class="c1">#                        optimizer.step())</span>
        <span class="o">...</span>

        <span class="c1">####################################################################</span>
        <span class="c1"># Fill in missing code below (...),</span>
        <span class="c1"># then remove or comment the line below to test your function</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Update the set_postfix function"</span><span class="p">)</span>
        <span class="c1">####################################################################</span>
        <span class="c1"># set loss to whatever you end up naming your variable when</span>
        <span class="c1"># calling criterion</span>
        <span class="c1"># for example, loss = criterion(output, target)</span>
        <span class="c1"># then set loss = loss.item() in the set_postfix function</span>
        <span class="n">tepochs</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=...</span><span class="p">)</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="o">...</span>  <span class="c1"># add the loss for this batch</span>

        <span class="c1"># get accuracy</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

      <span class="c1">####################################################################</span>
      <span class="c1"># Fill in missing code below (...),</span>
      <span class="c1"># then remove or comment the line below to test your function</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Append the train_loss"</span><span class="p">)</span>
      <span class="c1">####################################################################</span>
      <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># append the loss for this epoch (running loss divided by the number of batches e.g. len(train_loader))</span>
      <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="p">)</span>

      <span class="c1"># evaluate on validation data</span>
      <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
      <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.</span>
      <span class="n">correct</span><span class="p">,</span> <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
      <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">validation_loader</span><span class="p">:</span>
        <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">tepochs</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="c1"># get accuracy</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

      <span class="n">validation_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">running_loss</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">validation_loader</span><span class="p">))</span>
      <span class="n">validation_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">validation_loss</span><span class="p">,</span> <span class="n">validation_acc</span>


<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="c1">## Uncomment to test your training loop</span>
<span class="c1"># net = FMNIST_Net1().to(DEVICE)</span>
<span class="c1"># train_loss, train_acc, validation_loss, validation_acc = train(net, DEVICE, train_loader, validation_loader, 20)</span>
<span class="c1"># plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/solutions/W2D1_Tutorial2_Solution_8b552183.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/static/W2D1_Tutorial2_Solution_8b552183_3.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/static/W2D1_Tutorial2_Solution_8b552183_3.png" style="width: 2195.0px; height: 755.0px;"/></a>
<p>The next cell contains the code for the CNN we will be using in this section.</p>
<p>Run the next cell to get the accuracy on the data!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="think-1-overfitting">
<h2>Think! 1: Overfitting<a class="headerlink" href="#think-1-overfitting" title="Permalink to this headline">¶</a></h2>
<p>Do you think this network is overfitting?
If yes, what can you do to combat this?</p>
<p><strong>Hint</strong>: overfitting occurs when the training accuracy greatly exceeds the validation accuracy</p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/solutions/W2D1_Tutorial2_Solution_a367e834.py"><em>Click for solution</em></a></p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-overfitting-symptoms-and-cures">
<h1>Section 2: Overfitting - symptoms and cures<a class="headerlink" href="#section-2-overfitting-symptoms-and-cures" title="Permalink to this headline">¶</a></h1>
<p>So you spent some time last week learning about regularization techniques. Below is a copy of the CNN model we used previously.  Now we want you to add some dropout regularization, and check if that helps reduce overfitting. If you’re up for a challenge, you can try methods other than dropout as well.</p>
<div class="section" id="coding-exercise-2-1-adding-regularization">
<h2>Coding Exercise 2.1: Adding Regularization<a class="headerlink" href="#coding-exercise-2-1-adding-regularization" title="Permalink to this headline">¶</a></h2>
<p>Add various regularization methods, feel free to add any and play around!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">FMNIST_Net2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FMNIST_Net2</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Fill in missing code below (...),</span>
    <span class="c1"># then remove or comment the line below to test your function</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Add regularization layers"</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="o">...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">9216</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1">####################################################################</span>
    <span class="c1"># Now add the layers in your forward pass in appropriate order</span>
    <span class="c1"># then remove or comment the line below to test your function</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Add regularization in the forward pass"</span><span class="p">)</span>
    <span class="c1">####################################################################</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>


<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="c1">## Uncomment below to check your code</span>
<span class="c1"># net2 = FMNIST_Net2().to(DEVICE)</span>
<span class="c1"># train_loss, train_acc, validation_loss, validation_acc = train(net2, DEVICE, train_loader, validation_loader, 20)</span>
<span class="c1"># plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/solutions/W2D1_Tutorial2_Solution_36a8481f.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/static/W2D1_Tutorial2_Solution_36a8481f_3.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/static/W2D1_Tutorial2_Solution_36a8481f_3.png" style="width: 2195.0px; height: 755.0px;"/></a>
<p>Run the next cell to get the accuracy on the data!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test</span><span class="p">(</span><span class="n">net2</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="think-2-1-regularization">
<h2>Think! 2.1: Regularization<a class="headerlink" href="#think-2-1-regularization" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Is the training accuracy slightly reduced from before adding regularization? What accuracy were you able to reduce it to?</p></li>
<li><p>Why does the validation accuracy start higher than training accuracy?</p></li>
</ol>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/solutions/W2D1_Tutorial2_Solution_6e9ea2ef.py"><em>Click for solution</em></a></p>
</div>
<div class="section" id="interactive-demo-2-dropout-exploration">
<h2>Interactive Demo 2: Dropout exploration<a class="headerlink" href="#interactive-demo-2-dropout-exploration" title="Permalink to this headline">¶</a></h2>
<p>If you want to try out more dropout parameter combinations, but do not have the time to run them, we have here precalculated some combinations you can use the sliders to explore them.</p>
<p><em>Run this cell to enable the widget</em></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown  *Run this cell to enable the widget*</span>

<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">widgets</span><span class="p">,</span> <span class="n">interactive_output</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.3495898238046372</span><span class="p">,</span> <span class="mf">0.2901147632522786</span><span class="p">,</span> <span class="mf">0.2504794800931469</span><span class="p">,</span> <span class="mf">0.23571575765914105</span><span class="p">,</span> <span class="mf">0.21297093365896255</span><span class="p">,</span> <span class="mf">0.19087818914905508</span><span class="p">,</span> <span class="mf">0.186408187797729</span><span class="p">,</span> <span class="mf">0.19487689035211472</span><span class="p">,</span> <span class="mf">0.16774938120803934</span><span class="p">,</span> <span class="mf">0.1548648244958926</span><span class="p">,</span> <span class="mf">0.1390149021382503</span><span class="p">,</span> <span class="mf">0.10919439224922593</span><span class="p">,</span> <span class="mf">0.10054351237820501</span><span class="p">,</span> <span class="mf">0.09900783193594914</span><span class="p">,</span> <span class="mf">0.08370604479507088</span><span class="p">,</span> <span class="mf">0.07831853718318521</span><span class="p">,</span> <span class="mf">0.06859792241866285</span><span class="p">,</span> <span class="mf">0.06152600247383197</span><span class="p">,</span> <span class="mf">0.046342475851873885</span><span class="p">,</span> <span class="mf">0.055123823092992796</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.83475</span><span class="p">,</span> <span class="mf">0.8659166666666667</span><span class="p">,</span> <span class="mf">0.8874166666666666</span><span class="p">,</span> <span class="mf">0.8913333333333333</span><span class="p">,</span> <span class="mf">0.8998333333333334</span><span class="p">,</span> <span class="mf">0.9140833333333334</span><span class="p">,</span> <span class="mf">0.9178333333333333</span><span class="p">,</span> <span class="mf">0.9138333333333334</span><span class="p">,</span> <span class="mf">0.9251666666666667</span><span class="p">,</span> <span class="mf">0.92975</span><span class="p">,</span> <span class="mf">0.939</span><span class="p">,</span> <span class="mf">0.9525833333333333</span><span class="p">,</span> <span class="mf">0.9548333333333333</span><span class="p">,</span> <span class="mf">0.9585833333333333</span><span class="p">,</span> <span class="mf">0.9655833333333333</span><span class="p">,</span> <span class="mf">0.9661666666666666</span><span class="p">,</span> <span class="mf">0.9704166666666667</span><span class="p">,</span> <span class="mf">0.9743333333333334</span><span class="p">,</span> <span class="mf">0.9808333333333333</span><span class="p">,</span> <span class="mf">0.9775</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.334623601436615</span><span class="p">,</span> <span class="mf">0.2977438402175903</span><span class="p">,</span> <span class="mf">0.2655304968357086</span><span class="p">,</span> <span class="mf">0.25506321132183074</span><span class="p">,</span> <span class="mf">0.2588835284113884</span><span class="p">,</span> <span class="mf">0.2336345863342285</span><span class="p">,</span> <span class="mf">0.3029863876104355</span><span class="p">,</span> <span class="mf">0.240766831189394</span><span class="p">,</span> <span class="mf">0.2719801160693169</span><span class="p">,</span> <span class="mf">0.25231350839138034</span><span class="p">,</span> <span class="mf">0.2500132185220718</span><span class="p">,</span> <span class="mf">0.26699506521224975</span><span class="p">,</span> <span class="mf">0.2934862145781517</span><span class="p">,</span> <span class="mf">0.361227530837059</span><span class="p">,</span> <span class="mf">0.33196919202804565</span><span class="p">,</span> <span class="mf">0.36985905408859254</span><span class="p">,</span> <span class="mf">0.4042587959766388</span><span class="p">,</span> <span class="mf">0.3716402840614319</span><span class="p">,</span> <span class="mf">0.3707024946808815</span><span class="p">,</span> <span class="mf">0.4652537405490875</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.866875</span><span class="p">,</span> <span class="mf">0.851875</span><span class="p">,</span> <span class="mf">0.8775</span><span class="p">,</span> <span class="mf">0.889375</span><span class="p">,</span> <span class="mf">0.881875</span><span class="p">,</span> <span class="mf">0.900625</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.898125</span><span class="p">,</span> <span class="mf">0.885625</span><span class="p">,</span> <span class="mf">0.876875</span><span class="p">,</span> <span class="mf">0.899375</span><span class="p">,</span> <span class="mf">0.90625</span><span class="p">,</span> <span class="mf">0.89875</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.898125</span><span class="p">,</span> <span class="mf">0.884375</span><span class="p">,</span> <span class="mf">0.874375</span><span class="p">,</span> <span class="mf">0.89375</span><span class="p">,</span> <span class="mf">0.903125</span><span class="p">,</span> <span class="mf">0.890625</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.35404509995528993</span><span class="p">,</span> <span class="mf">0.30616586227366266</span><span class="p">,</span> <span class="mf">0.2872369573946963</span><span class="p">,</span> <span class="mf">0.27564131199045383</span><span class="p">,</span> <span class="mf">0.25969504263806853</span><span class="p">,</span> <span class="mf">0.24728168408445855</span><span class="p">,</span> <span class="mf">0.23505379509260046</span><span class="p">,</span> <span class="mf">0.21552803914280647</span><span class="p">,</span> <span class="mf">0.209761732277718</span><span class="p">,</span> <span class="mf">0.19977611067526518</span><span class="p">,</span> <span class="mf">0.19632092922767427</span><span class="p">,</span> <span class="mf">0.18672360206379535</span><span class="p">,</span> <span class="mf">0.16564940239124476</span><span class="p">,</span> <span class="mf">0.1654047035671612</span><span class="p">,</span> <span class="mf">0.1684555298985636</span><span class="p">,</span> <span class="mf">0.1627526102349796</span><span class="p">,</span> <span class="mf">0.13878319327263755</span><span class="p">,</span> <span class="mf">0.12881529055773577</span><span class="p">,</span> <span class="mf">0.12628930977525862</span><span class="p">,</span> <span class="mf">0.11346105090837846</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8324166666666667</span><span class="p">,</span> <span class="mf">0.8604166666666667</span><span class="p">,</span> <span class="mf">0.8680833333333333</span><span class="p">,</span> <span class="mf">0.8728333333333333</span><span class="p">,</span> <span class="mf">0.8829166666666667</span><span class="p">,</span> <span class="mf">0.88625</span><span class="p">,</span> <span class="mf">0.89425</span><span class="p">,</span> <span class="mf">0.90125</span><span class="p">,</span> <span class="mf">0.9015833333333333</span><span class="p">,</span> <span class="mf">0.90925</span><span class="p">,</span> <span class="mf">0.9114166666666667</span><span class="p">,</span> <span class="mf">0.917</span><span class="p">,</span> <span class="mf">0.9268333333333333</span><span class="p">,</span> <span class="mf">0.92475</span><span class="p">,</span> <span class="mf">0.921</span><span class="p">,</span> <span class="mf">0.9255833333333333</span><span class="p">,</span> <span class="mf">0.9385</span><span class="p">,</span> <span class="mf">0.9428333333333333</span><span class="p">,</span> <span class="mf">0.9424166666666667</span><span class="p">,</span> <span class="mf">0.9484166666666667</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3533937376737595</span><span class="p">,</span> <span class="mf">0.29569859683513644</span><span class="p">,</span> <span class="mf">0.27531551957130435</span><span class="p">,</span> <span class="mf">0.2576177391409874</span><span class="p">,</span> <span class="mf">0.26947550356388095</span><span class="p">,</span> <span class="mf">0.25361743807792664</span><span class="p">,</span> <span class="mf">0.2527468180656433</span><span class="p">,</span> <span class="mf">0.24179009914398195</span><span class="p">,</span> <span class="mf">0.28664454460144045</span><span class="p">,</span> <span class="mf">0.23347773611545564</span><span class="p">,</span> <span class="mf">0.24672816634178163</span><span class="p">,</span> <span class="mf">0.27822364538908007</span><span class="p">,</span> <span class="mf">0.2380720081925392</span><span class="p">,</span> <span class="mf">0.24426509588956832</span><span class="p">,</span> <span class="mf">0.2443918392062187</span><span class="p">,</span> <span class="mf">0.24207917481660843</span><span class="p">,</span> <span class="mf">0.2519641682505608</span><span class="p">,</span> <span class="mf">0.3075403380393982</span><span class="p">,</span> <span class="mf">0.2798181238770485</span><span class="p">,</span> <span class="mf">0.26709021866321564</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.826875</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.870625</span><span class="p">,</span> <span class="mf">0.8875</span><span class="p">,</span> <span class="mf">0.883125</span><span class="p">,</span> <span class="mf">0.88625</span><span class="p">,</span> <span class="mf">0.891875</span><span class="p">,</span> <span class="mf">0.891875</span><span class="p">,</span> <span class="mf">0.890625</span><span class="p">,</span> <span class="mf">0.903125</span><span class="p">,</span> <span class="mf">0.89375</span><span class="p">,</span> <span class="mf">0.885625</span><span class="p">,</span> <span class="mf">0.903125</span><span class="p">,</span> <span class="mf">0.888125</span><span class="p">,</span> <span class="mf">0.899375</span><span class="p">,</span> <span class="mf">0.898125</span><span class="p">,</span> <span class="mf">0.905</span><span class="p">,</span> <span class="mf">0.905625</span><span class="p">,</span> <span class="mf">0.898125</span><span class="p">,</span> <span class="mf">0.901875</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.39775496332886373</span><span class="p">,</span> <span class="mf">0.33771887778284704</span><span class="p">,</span> <span class="mf">0.321900939132939</span><span class="p">,</span> <span class="mf">0.3079229625774191</span><span class="p">,</span> <span class="mf">0.304149763301966</span><span class="p">,</span> <span class="mf">0.28249239723416086</span><span class="p">,</span> <span class="mf">0.2861261191044716</span><span class="p">,</span> <span class="mf">0.27356165798103554</span><span class="p">,</span> <span class="mf">0.2654648520686525</span><span class="p">,</span> <span class="mf">0.2697350280557541</span><span class="p">,</span> <span class="mf">0.25354846321204877</span><span class="p">,</span> <span class="mf">0.24612889034633942</span><span class="p">,</span> <span class="mf">0.23482802549892284</span><span class="p">,</span> <span class="mf">0.2389904112416379</span><span class="p">,</span> <span class="mf">0.23742155821875055</span><span class="p">,</span> <span class="mf">0.232423192127905</span><span class="p">,</span> <span class="mf">0.22337309338469455</span><span class="p">,</span> <span class="mf">0.2141852991932884</span><span class="p">,</span> <span class="mf">0.20677659985549907</span><span class="p">,</span> <span class="mf">0.19355326712607068</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8155</span><span class="p">,</span> <span class="mf">0.83625</span><span class="p">,</span> <span class="mf">0.8481666666666666</span><span class="p">,</span> <span class="mf">0.8530833333333333</span><span class="p">,</span> <span class="mf">0.8571666666666666</span><span class="p">,</span> <span class="mf">0.86775</span><span class="p">,</span> <span class="mf">0.8623333333333333</span><span class="p">,</span> <span class="mf">0.8711666666666666</span><span class="p">,</span> <span class="mf">0.8748333333333334</span><span class="p">,</span> <span class="mf">0.8685833333333334</span><span class="p">,</span> <span class="mf">0.8785</span><span class="p">,</span> <span class="mf">0.8804166666666666</span><span class="p">,</span> <span class="mf">0.8835833333333334</span><span class="p">,</span> <span class="mf">0.8840833333333333</span><span class="p">,</span> <span class="mf">0.88875</span><span class="p">,</span> <span class="mf">0.8919166666666667</span><span class="p">,</span> <span class="mf">0.8946666666666667</span><span class="p">,</span> <span class="mf">0.8960833333333333</span><span class="p">,</span> <span class="mf">0.906</span><span class="p">,</span> <span class="mf">0.9063333333333333</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3430288594961166</span><span class="p">,</span> <span class="mf">0.4062050700187683</span><span class="p">,</span> <span class="mf">0.29745822548866274</span><span class="p">,</span> <span class="mf">0.27728439271450045</span><span class="p">,</span> <span class="mf">0.28092808067798614</span><span class="p">,</span> <span class="mf">0.2577864158153534</span><span class="p">,</span> <span class="mf">0.2651400637626648</span><span class="p">,</span> <span class="mf">0.25632822573184966</span><span class="p">,</span> <span class="mf">0.3082498562335968</span><span class="p">,</span> <span class="mf">0.2812121778726578</span><span class="p">,</span> <span class="mf">0.26345942318439486</span><span class="p">,</span> <span class="mf">0.2577408078312874</span><span class="p">,</span> <span class="mf">0.25757989794015884</span><span class="p">,</span> <span class="mf">0.26434457510709763</span><span class="p">,</span> <span class="mf">0.24917411386966706</span><span class="p">,</span> <span class="mf">0.27261342853307724</span><span class="p">,</span> <span class="mf">0.2445397639274597</span><span class="p">,</span> <span class="mf">0.26001051396131514</span><span class="p">,</span> <span class="mf">0.24147838801145555</span><span class="p">,</span> <span class="mf">0.2471102523803711</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.82875</span><span class="p">,</span> <span class="mf">0.795625</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.87375</span><span class="p">,</span> <span class="mf">0.865625</span><span class="p">,</span> <span class="mf">0.8825</span><span class="p">,</span> <span class="mf">0.8825</span><span class="p">,</span> <span class="mf">0.87625</span><span class="p">,</span> <span class="mf">0.848125</span><span class="p">,</span> <span class="mf">0.87875</span><span class="p">,</span> <span class="mf">0.8675</span><span class="p">,</span> <span class="mf">0.889375</span><span class="p">,</span> <span class="mf">0.8925</span><span class="p">,</span> <span class="mf">0.866875</span><span class="p">,</span> <span class="mf">0.87375</span><span class="p">,</span> <span class="mf">0.87125</span><span class="p">,</span> <span class="mf">0.895625</span><span class="p">,</span> <span class="mf">0.90375</span><span class="p">,</span> <span class="mf">0.90125</span><span class="p">,</span> <span class="mf">0.88625</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.4454924576777093</span><span class="p">,</span> <span class="mf">0.43416607585993217</span><span class="p">,</span> <span class="mf">0.42200265769311723</span><span class="p">,</span> <span class="mf">0.40520024616667566</span><span class="p">,</span> <span class="mf">0.41137005166804536</span><span class="p">,</span> <span class="mf">0.404100904280835</span><span class="p">,</span> <span class="mf">0.40118067664034823</span><span class="p">,</span> <span class="mf">0.40139733080534223</span><span class="p">,</span> <span class="mf">0.3797615355158106</span><span class="p">,</span> <span class="mf">0.3596332479030528</span><span class="p">,</span> <span class="mf">0.3600061919460905</span><span class="p">,</span> <span class="mf">0.3554147962242999</span><span class="p">,</span> <span class="mf">0.34480382890460337</span><span class="p">,</span> <span class="mf">0.3329520877054397</span><span class="p">,</span> <span class="mf">0.33164913056695716</span><span class="p">,</span> <span class="mf">0.31860941466181836</span><span class="p">,</span> <span class="mf">0.30702565340919696</span><span class="p">,</span> <span class="mf">0.30605297186907304</span><span class="p">,</span> <span class="mf">0.2953788426486736</span><span class="p">,</span> <span class="mf">0.2877389984403519</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7788333333333334</span><span class="p">,</span> <span class="mf">0.7825</span><span class="p">,</span> <span class="mf">0.7854166666666667</span><span class="p">,</span> <span class="mf">0.7916666666666666</span><span class="p">,</span> <span class="mf">0.7885</span><span class="p">,</span> <span class="mf">0.7833333333333333</span><span class="p">,</span> <span class="mf">0.7923333333333333</span><span class="p">,</span> <span class="mf">0.79525</span><span class="p">,</span> <span class="mf">0.805</span><span class="p">,</span> <span class="mf">0.81475</span><span class="p">,</span> <span class="mf">0.8161666666666667</span><span class="p">,</span> <span class="mf">0.8188333333333333</span><span class="p">,</span> <span class="mf">0.817</span><span class="p">,</span> <span class="mf">0.8266666666666667</span><span class="p">,</span> <span class="mf">0.82225</span><span class="p">,</span> <span class="mf">0.8360833333333333</span><span class="p">,</span> <span class="mf">0.8456666666666667</span><span class="p">,</span> <span class="mf">0.8430833333333333</span><span class="p">,</span> <span class="mf">0.8491666666666666</span><span class="p">,</span> <span class="mf">0.8486666666666667</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3507828885316849</span><span class="p">,</span> <span class="mf">0.3337512403726578</span><span class="p">,</span> <span class="mf">0.34320746660232543</span><span class="p">,</span> <span class="mf">0.3476085543632507</span><span class="p">,</span> <span class="mf">0.3326113569736481</span><span class="p">,</span> <span class="mf">0.33033264458179473</span><span class="p">,</span> <span class="mf">0.32014619171619413</span><span class="p">,</span> <span class="mf">0.3182142299413681</span><span class="p">,</span> <span class="mf">0.30076164126396177</span><span class="p">,</span> <span class="mf">0.3263852882385254</span><span class="p">,</span> <span class="mf">0.27597591280937195</span><span class="p">,</span> <span class="mf">0.29062016785144806</span><span class="p">,</span> <span class="mf">0.2765174686908722</span><span class="p">,</span> <span class="mf">0.269492534995079</span><span class="p">,</span> <span class="mf">0.2679423809051514</span><span class="p">,</span> <span class="mf">0.2691828978061676</span><span class="p">,</span> <span class="mf">0.2726386785507202</span><span class="p">,</span> <span class="mf">0.2541181230545044</span><span class="p">,</span> <span class="mf">0.2580208206176758</span><span class="p">,</span> <span class="mf">0.26315389811992645</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.839375</span><span class="p">,</span> <span class="mf">0.843125</span><span class="p">,</span> <span class="mf">0.823125</span><span class="p">,</span> <span class="mf">0.821875</span><span class="p">,</span> <span class="mf">0.81875</span><span class="p">,</span> <span class="mf">0.819375</span><span class="p">,</span> <span class="mf">0.8225</span><span class="p">,</span> <span class="mf">0.826875</span><span class="p">,</span> <span class="mf">0.835625</span><span class="p">,</span> <span class="mf">0.865</span><span class="p">,</span> <span class="mf">0.868125</span><span class="p">,</span> <span class="mf">0.855625</span><span class="p">,</span> <span class="mf">0.868125</span><span class="p">,</span> <span class="mf">0.884375</span><span class="p">,</span> <span class="mf">0.883125</span><span class="p">,</span> <span class="mf">0.875</span><span class="p">,</span> <span class="mf">0.87375</span><span class="p">,</span> <span class="mf">0.883125</span><span class="p">,</span> <span class="mf">0.8975</span><span class="p">,</span> <span class="mf">0.885</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.34561181647029326</span><span class="p">,</span> <span class="mf">0.2834314257699124</span><span class="p">,</span> <span class="mf">0.2583787844298368</span><span class="p">,</span> <span class="mf">0.23892096465730922</span><span class="p">,</span> <span class="mf">0.23207981773513428</span><span class="p">,</span> <span class="mf">0.20245029634617745</span><span class="p">,</span> <span class="mf">0.183908417583146</span><span class="p">,</span> <span class="mf">0.17489413774393975</span><span class="p">,</span> <span class="mf">0.17696723581707857</span><span class="p">,</span> <span class="mf">0.15615438255778652</span><span class="p">,</span> <span class="mf">0.14469048382833283</span><span class="p">,</span> <span class="mf">0.12424647461305907</span><span class="p">,</span> <span class="mf">0.11314761043189371</span><span class="p">,</span> <span class="mf">0.11249036608422373</span><span class="p">,</span> <span class="mf">0.10725672634199579</span><span class="p">,</span> <span class="mf">0.09081190969160896</span><span class="p">,</span> <span class="mf">0.0942245383271353</span><span class="p">,</span> <span class="mf">0.08525650047677312</span><span class="p">,</span> <span class="mf">0.06622548752583246</span><span class="p">,</span> <span class="mf">0.06039895973307021</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8356666666666667</span><span class="p">,</span> <span class="mf">0.8675833333333334</span><span class="p">,</span> <span class="mf">0.88175</span><span class="p">,</span> <span class="mf">0.8933333333333333</span><span class="p">,</span> <span class="mf">0.8975833333333333</span><span class="p">,</span> <span class="mf">0.91175</span><span class="p">,</span> <span class="mf">0.91825</span><span class="p">,</span> <span class="mf">0.9249166666666667</span><span class="p">,</span> <span class="mf">0.9238333333333333</span><span class="p">,</span> <span class="mf">0.9305</span><span class="p">,</span> <span class="mf">0.938</span><span class="p">,</span> <span class="mf">0.9465833333333333</span><span class="p">,</span> <span class="mf">0.9525833333333333</span><span class="p">,</span> <span class="mf">0.9539166666666666</span><span class="p">,</span> <span class="mf">0.9555</span><span class="p">,</span> <span class="mf">0.9615</span><span class="p">,</span> <span class="mf">0.9606666666666667</span><span class="p">,</span> <span class="mf">0.96275</span><span class="p">,</span> <span class="mf">0.9725</span><span class="p">,</span> <span class="mf">0.9764166666666667</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.31630186855792997</span><span class="p">,</span> <span class="mf">0.2702121251821518</span><span class="p">,</span> <span class="mf">0.2915778249502182</span><span class="p">,</span> <span class="mf">0.26050266206264494</span><span class="p">,</span> <span class="mf">0.27837209939956664</span><span class="p">,</span> <span class="mf">0.24276352763175965</span><span class="p">,</span> <span class="mf">0.3567117482423782</span><span class="p">,</span> <span class="mf">0.2752074319124222</span><span class="p">,</span> <span class="mf">0.2423130339384079</span><span class="p">,</span> <span class="mf">0.2565067422389984</span><span class="p">,</span> <span class="mf">0.28710135877132414</span><span class="p">,</span> <span class="mf">0.266545415520668</span><span class="p">,</span> <span class="mf">0.31818037331104276</span><span class="p">,</span> <span class="mf">0.28757534325122835</span><span class="p">,</span> <span class="mf">0.2777567034959793</span><span class="p">,</span> <span class="mf">0.2998969575762749</span><span class="p">,</span> <span class="mf">0.3292293107509613</span><span class="p">,</span> <span class="mf">0.30775387287139894</span><span class="p">,</span> <span class="mf">0.32681577146053314</span><span class="p">,</span> <span class="mf">0.44882203072309496</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.85375</span><span class="p">,</span> <span class="mf">0.879375</span><span class="p">,</span> <span class="mf">0.875625</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="mf">0.86125</span><span class="p">,</span> <span class="mf">0.884375</span><span class="p">,</span> <span class="mf">0.851875</span><span class="p">,</span> <span class="mf">0.8875</span><span class="p">,</span> <span class="mf">0.89625</span><span class="p">,</span> <span class="mf">0.875625</span><span class="p">,</span> <span class="mf">0.8675</span><span class="p">,</span> <span class="mf">0.895</span><span class="p">,</span> <span class="mf">0.888125</span><span class="p">,</span> <span class="mf">0.89125</span><span class="p">,</span> <span class="mf">0.889375</span><span class="p">,</span> <span class="mf">0.880625</span><span class="p">,</span> <span class="mf">0.87875</span><span class="p">,</span> <span class="mf">0.8875</span><span class="p">,</span> <span class="mf">0.894375</span><span class="p">,</span> <span class="mf">0.891875</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.35970850011452715</span><span class="p">,</span> <span class="mf">0.31336131549261986</span><span class="p">,</span> <span class="mf">0.2881505932421126</span><span class="p">,</span> <span class="mf">0.2732012960267194</span><span class="p">,</span> <span class="mf">0.26232245425753137</span><span class="p">,</span> <span class="mf">0.2490472443639598</span><span class="p">,</span> <span class="mf">0.24866499093935845</span><span class="p">,</span> <span class="mf">0.22930880945096624</span><span class="p">,</span> <span class="mf">0.21745950407645803</span><span class="p">,</span> <span class="mf">0.20700296882460725</span><span class="p">,</span> <span class="mf">0.197304340356842</span><span class="p">,</span> <span class="mf">0.20665066804182022</span><span class="p">,</span> <span class="mf">0.19864868348900308</span><span class="p">,</span> <span class="mf">0.184807124210799</span><span class="p">,</span> <span class="mf">0.1684703354703936</span><span class="p">,</span> <span class="mf">0.17377675851767369</span><span class="p">,</span> <span class="mf">0.16638460063791655</span><span class="p">,</span> <span class="mf">0.15944768343754906</span><span class="p">,</span> <span class="mf">0.14876513817208878</span><span class="p">,</span> <span class="mf">0.1388207479835825</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.83375</span><span class="p">,</span> <span class="mf">0.85175</span><span class="p">,</span> <span class="mf">0.86725</span><span class="p">,</span> <span class="mf">0.8719166666666667</span><span class="p">,</span> <span class="mf">0.8761666666666666</span><span class="p">,</span> <span class="mf">0.8865833333333333</span><span class="p">,</span> <span class="mf">0.88275</span><span class="p">,</span> <span class="mf">0.8956666666666667</span><span class="p">,</span> <span class="mf">0.8995833333333333</span><span class="p">,</span> <span class="mf">0.9034166666666666</span><span class="p">,</span> <span class="mf">0.90825</span><span class="p">,</span> <span class="mf">0.9043333333333333</span><span class="p">,</span> <span class="mf">0.9093333333333333</span><span class="p">,</span> <span class="mf">0.9145</span><span class="p">,</span> <span class="mf">0.9196666666666666</span><span class="p">,</span> <span class="mf">0.9196666666666666</span><span class="p">,</span> <span class="mf">0.9216666666666666</span><span class="p">,</span> <span class="mf">0.9273333333333333</span><span class="p">,</span> <span class="mf">0.9299166666666666</span><span class="p">,</span> <span class="mf">0.93675</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3166788029670715</span><span class="p">,</span> <span class="mf">0.28422485530376435</span><span class="p">,</span> <span class="mf">0.38055971562862395</span><span class="p">,</span> <span class="mf">0.2586472672224045</span><span class="p">,</span> <span class="mf">0.2588653892278671</span><span class="p">,</span> <span class="mf">0.27983254253864287</span><span class="p">,</span> <span class="mf">0.25693483114242555</span><span class="p">,</span> <span class="mf">0.26412731170654297</span><span class="p">,</span> <span class="mf">0.2733065390586853</span><span class="p">,</span> <span class="mf">0.24399636536836625</span><span class="p">,</span> <span class="mf">0.24481021404266357</span><span class="p">,</span> <span class="mf">0.2689305514097214</span><span class="p">,</span> <span class="mf">0.2527604129910469</span><span class="p">,</span> <span class="mf">0.24829535871744157</span><span class="p">,</span> <span class="mf">0.2654112687706947</span><span class="p">,</span> <span class="mf">0.23074268400669098</span><span class="p">,</span> <span class="mf">0.24625462979078294</span><span class="p">,</span> <span class="mf">0.26423920392990113</span><span class="p">,</span> <span class="mf">0.25540480852127073</span><span class="p">,</span> <span class="mf">0.25536185175180437</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.856875</span><span class="p">,</span> <span class="mf">0.86625</span><span class="p">,</span> <span class="mf">0.815</span><span class="p">,</span> <span class="mf">0.8825</span><span class="p">,</span> <span class="mf">0.88125</span><span class="p">,</span> <span class="mf">0.875625</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="mf">0.8775</span><span class="p">,</span> <span class="mf">0.870625</span><span class="p">,</span> <span class="mf">0.895</span><span class="p">,</span> <span class="mf">0.8975</span><span class="p">,</span> <span class="mf">0.87375</span><span class="p">,</span> <span class="mf">0.88625</span><span class="p">,</span> <span class="mf">0.89125</span><span class="p">,</span> <span class="mf">0.903125</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.893125</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="mf">0.8925</span><span class="p">,</span> <span class="mf">0.899375</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.3975753842040579</span><span class="p">,</span> <span class="mf">0.34884724409339274</span><span class="p">,</span> <span class="mf">0.3296900932142075</span><span class="p">,</span> <span class="mf">0.3150389680361494</span><span class="p">,</span> <span class="mf">0.31285368667003954</span><span class="p">,</span> <span class="mf">0.30415422033439293</span><span class="p">,</span> <span class="mf">0.29553352716438314</span><span class="p">,</span> <span class="mf">0.289314468094009</span><span class="p">,</span> <span class="mf">0.2806722329969102</span><span class="p">,</span> <span class="mf">0.2724469883486311</span><span class="p">,</span> <span class="mf">0.26634286379719035</span><span class="p">,</span> <span class="mf">0.2645016222241077</span><span class="p">,</span> <span class="mf">0.2619251853766594</span><span class="p">,</span> <span class="mf">0.2551752221473354</span><span class="p">,</span> <span class="mf">0.26411766035759704</span><span class="p">,</span> <span class="mf">0.24515971153023394</span><span class="p">,</span> <span class="mf">0.2390686312412962</span><span class="p">,</span> <span class="mf">0.23573122312255362</span><span class="p">,</span> <span class="mf">0.221005061562074</span><span class="p">,</span> <span class="mf">0.22358600648635246</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8106666666666666</span><span class="p">,</span> <span class="mf">0.8286666666666667</span><span class="p">,</span> <span class="mf">0.844</span><span class="p">,</span> <span class="mf">0.8513333333333334</span><span class="p">,</span> <span class="mf">0.84975</span><span class="p">,</span> <span class="mf">0.8570833333333333</span><span class="p">,</span> <span class="mf">0.8624166666666667</span><span class="p">,</span> <span class="mf">0.8626666666666667</span><span class="p">,</span> <span class="mf">0.866</span><span class="p">,</span> <span class="mf">0.8706666666666667</span><span class="p">,</span> <span class="mf">0.8738333333333334</span><span class="p">,</span> <span class="mf">0.8748333333333334</span><span class="p">,</span> <span class="mf">0.8778333333333334</span><span class="p">,</span> <span class="mf">0.8798333333333334</span><span class="p">,</span> <span class="mf">0.87375</span><span class="p">,</span> <span class="mf">0.8865</span><span class="p">,</span> <span class="mf">0.8898333333333334</span><span class="p">,</span> <span class="mf">0.8885833333333333</span><span class="p">,</span> <span class="mf">0.8991666666666667</span><span class="p">,</span> <span class="mf">0.8968333333333334</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3597823417186737</span><span class="p">,</span> <span class="mf">0.31115993797779085</span><span class="p">,</span> <span class="mf">0.29929635107517244</span><span class="p">,</span> <span class="mf">0.2986589139699936</span><span class="p">,</span> <span class="mf">0.2938830828666687</span><span class="p">,</span> <span class="mf">0.28118040919303894</span><span class="p">,</span> <span class="mf">0.2711684626340866</span><span class="p">,</span> <span class="mf">0.2844697123765945</span><span class="p">,</span> <span class="mf">0.26613601863384245</span><span class="p">,</span> <span class="mf">0.2783134698867798</span><span class="p">,</span> <span class="mf">0.2540236383676529</span><span class="p">,</span> <span class="mf">0.25821100890636445</span><span class="p">,</span> <span class="mf">0.2618845862150192</span><span class="p">,</span> <span class="mf">0.2554920208454132</span><span class="p">,</span> <span class="mf">0.26543013513088226</span><span class="p">,</span> <span class="mf">0.24074569433927537</span><span class="p">,</span> <span class="mf">0.26475649774074556</span><span class="p">,</span> <span class="mf">0.25578504264354707</span><span class="p">,</span> <span class="mf">0.2648500043153763</span><span class="p">,</span> <span class="mf">0.25700133621692656</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.825</span><span class="p">,</span> <span class="mf">0.8375</span><span class="p">,</span> <span class="mf">0.85875</span><span class="p">,</span> <span class="mf">0.855625</span><span class="p">,</span> <span class="mf">0.861875</span><span class="p">,</span> <span class="mf">0.868125</span><span class="p">,</span> <span class="mf">0.875</span><span class="p">,</span> <span class="mf">0.85375</span><span class="p">,</span> <span class="mf">0.886875</span><span class="p">,</span> <span class="mf">0.86375</span><span class="p">,</span> <span class="mf">0.88375</span><span class="p">,</span> <span class="mf">0.885625</span><span class="p">,</span> <span class="mf">0.875625</span><span class="p">,</span> <span class="mf">0.87375</span><span class="p">,</span> <span class="mf">0.8875</span><span class="p">,</span> <span class="mf">0.895</span><span class="p">,</span> <span class="mf">0.874375</span><span class="p">,</span> <span class="mf">0.89125</span><span class="p">,</span> <span class="mf">0.88625</span><span class="p">,</span> <span class="mf">0.895625</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.4584837538447786</span><span class="p">,</span> <span class="mf">0.4506375778545725</span><span class="p">,</span> <span class="mf">0.4378386567089152</span><span class="p">,</span> <span class="mf">0.4066803843734112</span><span class="p">,</span> <span class="mf">0.3897064097542712</span><span class="p">,</span> <span class="mf">0.3855383962868376</span><span class="p">,</span> <span class="mf">0.39160584618753574</span><span class="p">,</span> <span class="mf">0.3731403942120836</span><span class="p">,</span> <span class="mf">0.37915910170116324</span><span class="p">,</span> <span class="mf">0.36966170814443144</span><span class="p">,</span> <span class="mf">0.35735995298687445</span><span class="p">,</span> <span class="mf">0.35630573094525236</span><span class="p">,</span> <span class="mf">0.346426092167484</span><span class="p">,</span> <span class="mf">0.34040802899510303</span><span class="p">,</span> <span class="mf">0.32829743726773464</span><span class="p">,</span> <span class="mf">0.3284692421872565</span><span class="p">,</span> <span class="mf">0.3186114077713895</span><span class="p">,</span> <span class="mf">0.32295761503120685</span><span class="p">,</span> <span class="mf">0.3201326223764014</span><span class="p">,</span> <span class="mf">0.30581602454185486</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7803333333333333</span><span class="p">,</span> <span class="mf">0.7709166666666667</span><span class="p">,</span> <span class="mf">0.7723333333333333</span><span class="p">,</span> <span class="mf">0.7850833333333334</span><span class="p">,</span> <span class="mf">0.7885</span><span class="p">,</span> <span class="mf">0.7903333333333333</span><span class="p">,</span> <span class="mf">0.7986666666666666</span><span class="p">,</span> <span class="mf">0.805</span><span class="p">,</span> <span class="mf">0.8011666666666667</span><span class="p">,</span> <span class="mf">0.8068333333333333</span><span class="p">,</span> <span class="mf">0.8095833333333333</span><span class="p">,</span> <span class="mf">0.8226666666666667</span><span class="p">,</span> <span class="mf">0.8285</span><span class="p">,</span> <span class="mf">0.83125</span><span class="p">,</span> <span class="mf">0.8369166666666666</span><span class="p">,</span> <span class="mf">0.8395</span><span class="p">,</span> <span class="mf">0.8441666666666666</span><span class="p">,</span> <span class="mf">0.8393333333333334</span><span class="p">,</span> <span class="mf">0.8490833333333333</span><span class="p">,</span> <span class="mf">0.8546666666666667</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.43526833415031435</span><span class="p">,</span> <span class="mf">0.3598956459760666</span><span class="p">,</span> <span class="mf">0.3492005372047424</span><span class="p">,</span> <span class="mf">0.33501910269260404</span><span class="p">,</span> <span class="mf">0.31689528703689573</span><span class="p">,</span> <span class="mf">0.3113307124376297</span><span class="p">,</span> <span class="mf">0.32388085544109346</span><span class="p">,</span> <span class="mf">0.3084335786104202</span><span class="p">,</span> <span class="mf">0.3013568025827408</span><span class="p">,</span> <span class="mf">0.28992725372314454</span><span class="p">,</span> <span class="mf">0.28726822674274444</span><span class="p">,</span> <span class="mf">0.26945948660373686</span><span class="p">,</span> <span class="mf">0.276592333316803</span><span class="p">,</span> <span class="mf">0.27462401330471037</span><span class="p">,</span> <span class="mf">0.27574350595474245</span><span class="p">,</span> <span class="mf">0.2710308712720871</span><span class="p">,</span> <span class="mf">0.2702724140882492</span><span class="p">,</span> <span class="mf">0.27323003828525544</span><span class="p">,</span> <span class="mf">0.25551479041576386</span><span class="p">,</span> <span class="mf">0.26488787233829497</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.808125</span><span class="p">,</span> <span class="mf">0.81625</span><span class="p">,</span> <span class="mf">0.805</span><span class="p">,</span> <span class="mf">0.8325</span><span class="p">,</span> <span class="mf">0.846875</span><span class="p">,</span> <span class="mf">0.835625</span><span class="p">,</span> <span class="mf">0.850625</span><span class="p">,</span> <span class="mf">0.838125</span><span class="p">,</span> <span class="mf">0.836875</span><span class="p">,</span> <span class="mf">0.861875</span><span class="p">,</span> <span class="mf">0.85375</span><span class="p">,</span> <span class="mf">0.866875</span><span class="p">,</span> <span class="mf">0.858125</span><span class="p">,</span> <span class="mf">0.8825</span><span class="p">,</span> <span class="mf">0.879375</span><span class="p">,</span> <span class="mf">0.874375</span><span class="p">,</span> <span class="mf">0.874375</span><span class="p">,</span> <span class="mf">0.886875</span><span class="p">,</span> <span class="mf">0.883125</span><span class="p">,</span> <span class="mf">0.86875</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.3579516930783049</span><span class="p">,</span> <span class="mf">0.29596046564426826</span><span class="p">,</span> <span class="mf">0.2779693031247626</span><span class="p">,</span> <span class="mf">0.2563994538356015</span><span class="p">,</span> <span class="mf">0.24771526356802342</span><span class="p">,</span> <span class="mf">0.2324555875693864</span><span class="p">,</span> <span class="mf">0.2139121579362991</span><span class="p">,</span> <span class="mf">0.20474095547452886</span><span class="p">,</span> <span class="mf">0.19138856208387842</span><span class="p">,</span> <span class="mf">0.18883306279461434</span><span class="p">,</span> <span class="mf">0.1763652620757831</span><span class="p">,</span> <span class="mf">0.1698919345248253</span><span class="p">,</span> <span class="mf">0.16033914366221808</span><span class="p">,</span> <span class="mf">0.1557997044651432</span><span class="p">,</span> <span class="mf">0.1432509447467771</span><span class="p">,</span> <span class="mf">0.13817814606776896</span><span class="p">,</span> <span class="mf">0.12609625801919622</span><span class="p">,</span> <span class="mf">0.11830132696381275</span><span class="p">,</span> <span class="mf">0.11182412960903441</span><span class="p">,</span> <span class="mf">0.112559904720872</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8314166666666667</span><span class="p">,</span> <span class="mf">0.8611666666666666</span><span class="p">,</span> <span class="mf">0.8736666666666667</span><span class="p">,</span> <span class="mf">0.8800833333333333</span><span class="p">,</span> <span class="mf">0.885</span><span class="p">,</span> <span class="mf">0.8944166666666666</span><span class="p">,</span> <span class="mf">0.9036666666666666</span><span class="p">,</span> <span class="mf">0.9090833333333334</span><span class="p">,</span> <span class="mf">0.9193333333333333</span><span class="p">,</span> <span class="mf">0.9161666666666667</span><span class="p">,</span> <span class="mf">0.92225</span><span class="p">,</span> <span class="mf">0.9255</span><span class="p">,</span> <span class="mf">0.93075</span><span class="p">,</span> <span class="mf">0.93225</span><span class="p">,</span> <span class="mf">0.939</span><span class="p">,</span> <span class="mf">0.9414166666666667</span><span class="p">,</span> <span class="mf">0.94375</span><span class="p">,</span> <span class="mf">0.9485833333333333</span><span class="p">,</span> <span class="mf">0.9535833333333333</span><span class="p">,</span> <span class="mf">0.9524166666666667</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.30677567660808563</span><span class="p">,</span> <span class="mf">0.32954772651195524</span><span class="p">,</span> <span class="mf">0.25747098088264464</span><span class="p">,</span> <span class="mf">0.2736126834154129</span><span class="p">,</span> <span class="mf">0.2561805549263954</span><span class="p">,</span> <span class="mf">0.23671718776226044</span><span class="p">,</span> <span class="mf">0.24553639352321624</span><span class="p">,</span> <span class="mf">0.2338863667845726</span><span class="p">,</span> <span class="mf">0.24586652517318724</span><span class="p">,</span> <span class="mf">0.23423030972480774</span><span class="p">,</span> <span class="mf">0.26579618513584136</span><span class="p">,</span> <span class="mf">0.2781539523601532</span><span class="p">,</span> <span class="mf">0.27084136098623274</span><span class="p">,</span> <span class="mf">0.23948652744293214</span><span class="p">,</span> <span class="mf">0.26023868829011915</span><span class="p">,</span> <span class="mf">0.2419952344894409</span><span class="p">,</span> <span class="mf">0.2511997854709625</span><span class="p">,</span> <span class="mf">0.23935708701610564</span><span class="p">,</span> <span class="mf">0.2701922015845776</span><span class="p">,</span> <span class="mf">0.27307246536016466</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.870625</span><span class="p">,</span> <span class="mf">0.855625</span><span class="p">,</span> <span class="mf">0.886875</span><span class="p">,</span> <span class="mf">0.875625</span><span class="p">,</span> <span class="mf">0.878125</span><span class="p">,</span> <span class="mf">0.8925</span><span class="p">,</span> <span class="mf">0.885</span><span class="p">,</span> <span class="mf">0.890625</span><span class="p">,</span> <span class="mf">0.876875</span><span class="p">,</span> <span class="mf">0.896875</span><span class="p">,</span> <span class="mf">0.881875</span><span class="p">,</span> <span class="mf">0.8875</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="mf">0.898125</span><span class="p">,</span> <span class="mf">0.896875</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="mf">0.89875</span><span class="p">,</span> <span class="mf">0.904375</span><span class="p">,</span> <span class="mf">0.906875</span><span class="p">,</span> <span class="mf">0.894375</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.3712943946903056</span><span class="p">,</span> <span class="mf">0.3198322071594761</span><span class="p">,</span> <span class="mf">0.29978102302931725</span><span class="p">,</span> <span class="mf">0.295274139798068</span><span class="p">,</span> <span class="mf">0.2861913934032968</span><span class="p">,</span> <span class="mf">0.27165328782606635</span><span class="p">,</span> <span class="mf">0.25972246442069397</span><span class="p">,</span> <span class="mf">0.2543164194819141</span><span class="p">,</span> <span class="mf">0.24795781916126292</span><span class="p">,</span> <span class="mf">0.24630710007028378</span><span class="p">,</span> <span class="mf">0.23296909834793272</span><span class="p">,</span> <span class="mf">0.23382153587931015</span><span class="p">,</span> <span class="mf">0.2239028559799524</span><span class="p">,</span> <span class="mf">0.21443849290780564</span><span class="p">,</span> <span class="mf">0.2149274461367663</span><span class="p">,</span> <span class="mf">0.20642021417300752</span><span class="p">,</span> <span class="mf">0.19801520536396097</span><span class="p">,</span> <span class="mf">0.1978839404009124</span><span class="p">,</span> <span class="mf">0.19118623847657062</span><span class="p">,</span> <span class="mf">0.18144798041024107</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8235833333333333</span><span class="p">,</span> <span class="mf">0.8538333333333333</span><span class="p">,</span> <span class="mf">0.8604166666666667</span><span class="p">,</span> <span class="mf">0.86075</span><span class="p">,</span> <span class="mf">0.8664166666666666</span><span class="p">,</span> <span class="mf">0.8754166666666666</span><span class="p">,</span> <span class="mf">0.8799166666666667</span><span class="p">,</span> <span class="mf">0.8815833333333334</span><span class="p">,</span> <span class="mf">0.88725</span><span class="p">,</span> <span class="mf">0.8848333333333334</span><span class="p">,</span> <span class="mf">0.8936666666666667</span><span class="p">,</span> <span class="mf">0.8935</span><span class="p">,</span> <span class="mf">0.895</span><span class="p">,</span> <span class="mf">0.8995</span><span class="p">,</span> <span class="mf">0.89625</span><span class="p">,</span> <span class="mf">0.9068333333333334</span><span class="p">,</span> <span class="mf">0.9098333333333334</span><span class="p">,</span> <span class="mf">0.9120833333333334</span><span class="p">,</span> <span class="mf">0.91375</span><span class="p">,</span> <span class="mf">0.9175833333333333</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3184810388088226</span><span class="p">,</span> <span class="mf">0.2948088157176971</span><span class="p">,</span> <span class="mf">0.29438531696796416</span><span class="p">,</span> <span class="mf">0.27669853866100313</span><span class="p">,</span> <span class="mf">0.2634278678894043</span><span class="p">,</span> <span class="mf">0.25847582578659056</span><span class="p">,</span> <span class="mf">0.2500907778739929</span><span class="p">,</span> <span class="mf">0.2538330048322678</span><span class="p">,</span> <span class="mf">0.25127841770648957</span><span class="p">,</span> <span class="mf">0.2519759064912796</span><span class="p">,</span> <span class="mf">0.2455715072154999</span><span class="p">,</span> <span class="mf">0.2437664610147476</span><span class="p">,</span> <span class="mf">0.259639236330986</span><span class="p">,</span> <span class="mf">0.24515749186277389</span><span class="p">,</span> <span class="mf">0.2553828465938568</span><span class="p">,</span> <span class="mf">0.2324645048379898</span><span class="p">,</span> <span class="mf">0.24492083072662355</span><span class="p">,</span> <span class="mf">0.24482838332653045</span><span class="p">,</span> <span class="mf">0.23327024638652802</span><span class="p">,</span> <span class="mf">0.2520161652565002</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.855</span><span class="p">,</span> <span class="mf">0.865</span><span class="p">,</span> <span class="mf">0.8525</span><span class="p">,</span> <span class="mf">0.856875</span><span class="p">,</span> <span class="mf">0.876875</span><span class="p">,</span> <span class="mf">0.88125</span><span class="p">,</span> <span class="mf">0.8825</span><span class="p">,</span> <span class="mf">0.8875</span><span class="p">,</span> <span class="mf">0.8925</span><span class="p">,</span> <span class="mf">0.8925</span><span class="p">,</span> <span class="mf">0.88875</span><span class="p">,</span> <span class="mf">0.889375</span><span class="p">,</span> <span class="mf">0.87375</span><span class="p">,</span> <span class="mf">0.895</span><span class="p">,</span> <span class="mf">0.889375</span><span class="p">,</span> <span class="mf">0.90625</span><span class="p">,</span> <span class="mf">0.883125</span><span class="p">,</span> <span class="mf">0.895</span><span class="p">,</span> <span class="mf">0.899375</span><span class="p">,</span> <span class="mf">0.901875</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.40442772225496615</span><span class="p">,</span> <span class="mf">0.36662670541951</span><span class="p">,</span> <span class="mf">0.355034276367502</span><span class="p">,</span> <span class="mf">0.3396551510755052</span><span class="p">,</span> <span class="mf">0.3378269396563794</span><span class="p">,</span> <span class="mf">0.32084332002287214</span><span class="p">,</span> <span class="mf">0.31314464951766297</span><span class="p">,</span> <span class="mf">0.2982726935693558</span><span class="p">,</span> <span class="mf">0.2885229691387491</span><span class="p">,</span> <span class="mf">0.2888992782285873</span><span class="p">,</span> <span class="mf">0.2893476904706752</span><span class="p">,</span> <span class="mf">0.281817957996688</span><span class="p">,</span> <span class="mf">0.2771622718490185</span><span class="p">,</span> <span class="mf">0.2693793097550565</span><span class="p">,</span> <span class="mf">0.2617615883416952</span><span class="p">,</span> <span class="mf">0.2657115764995205</span><span class="p">,</span> <span class="mf">0.25631817549150043</span><span class="p">,</span> <span class="mf">0.24793559907281654</span><span class="p">,</span> <span class="mf">0.2538738044652533</span><span class="p">,</span> <span class="mf">0.23912971732305718</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8093333333333333</span><span class="p">,</span> <span class="mf">0.82825</span><span class="p">,</span> <span class="mf">0.8341666666666666</span><span class="p">,</span> <span class="mf">0.84525</span><span class="p">,</span> <span class="mf">0.84525</span><span class="p">,</span> <span class="mf">0.8515</span><span class="p">,</span> <span class="mf">0.8583333333333333</span><span class="p">,</span> <span class="mf">0.8626666666666667</span><span class="p">,</span> <span class="mf">0.8688333333333333</span><span class="p">,</span> <span class="mf">0.8685</span><span class="p">,</span> <span class="mf">0.8689166666666667</span><span class="p">,</span> <span class="mf">0.8693333333333333</span><span class="p">,</span> <span class="mf">0.8711666666666666</span><span class="p">,</span> <span class="mf">0.8766666666666667</span><span class="p">,</span> <span class="mf">0.88275</span><span class="p">,</span> <span class="mf">0.88175</span><span class="p">,</span> <span class="mf">0.8839166666666667</span><span class="p">,</span> <span class="mf">0.8866666666666667</span><span class="p">,</span> <span class="mf">0.8839166666666667</span><span class="p">,</span> <span class="mf">0.8929166666666667</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.38392188608646394</span><span class="p">,</span> <span class="mf">0.3653419762849808</span><span class="p">,</span> <span class="mf">0.3050421380996704</span><span class="p">,</span> <span class="mf">0.30614266455173494</span><span class="p">,</span> <span class="mf">0.2937217426300049</span><span class="p">,</span> <span class="mf">0.30008585572242735</span><span class="p">,</span> <span class="mf">0.2794034606218338</span><span class="p">,</span> <span class="mf">0.27541795969009397</span><span class="p">,</span> <span class="mf">0.31378355383872986</span><span class="p">,</span> <span class="mf">0.2670704126358032</span><span class="p">,</span> <span class="mf">0.26745485186576845</span><span class="p">,</span> <span class="mf">0.2471194839477539</span><span class="p">,</span> <span class="mf">0.26509816259145735</span><span class="p">,</span> <span class="mf">0.25458798944950106</span><span class="p">,</span> <span class="mf">0.2481587851047516</span><span class="p">,</span> <span class="mf">0.25591064751148224</span><span class="p">,</span> <span class="mf">0.2596563971042633</span><span class="p">,</span> <span class="mf">0.2569611769914627</span><span class="p">,</span> <span class="mf">0.2435744071006775</span><span class="p">,</span> <span class="mf">0.2507249677181244</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.820625</span><span class="p">,</span> <span class="mf">0.846875</span><span class="p">,</span> <span class="mf">0.856875</span><span class="p">,</span> <span class="mf">0.868125</span><span class="p">,</span> <span class="mf">0.860625</span><span class="p">,</span> <span class="mf">0.87125</span><span class="p">,</span> <span class="mf">0.86625</span><span class="p">,</span> <span class="mf">0.87375</span><span class="p">,</span> <span class="mf">0.865625</span><span class="p">,</span> <span class="mf">0.87875</span><span class="p">,</span> <span class="mf">0.878125</span><span class="p">,</span> <span class="mf">0.889375</span><span class="p">,</span> <span class="mf">0.87875</span><span class="p">,</span> <span class="mf">0.886875</span><span class="p">,</span> <span class="mf">0.89125</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="mf">0.87375</span><span class="p">,</span> <span class="mf">0.884375</span><span class="p">,</span> <span class="mf">0.88875</span><span class="p">,</span> <span class="mf">0.89375</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.46106574311852455</span><span class="p">,</span> <span class="mf">0.4519433615372536</span><span class="p">,</span> <span class="mf">0.4446939624687459</span><span class="p">,</span> <span class="mf">0.4284856241751224</span><span class="p">,</span> <span class="mf">0.4527993325857406</span><span class="p">,</span> <span class="mf">0.4220876024758562</span><span class="p">,</span> <span class="mf">0.40969764266876463</span><span class="p">,</span> <span class="mf">0.39233948219012704</span><span class="p">,</span> <span class="mf">0.42498463344700793</span><span class="p">,</span> <span class="mf">0.3869199570506177</span><span class="p">,</span> <span class="mf">0.38021832910623954</span><span class="p">,</span> <span class="mf">0.3855376149270129</span><span class="p">,</span> <span class="mf">0.3721433773319772</span><span class="p">,</span> <span class="mf">0.3662295250340979</span><span class="p">,</span> <span class="mf">0.3629763710530514</span><span class="p">,</span> <span class="mf">0.358500304691335</span><span class="p">,</span> <span class="mf">0.3490118366131123</span><span class="p">,</span> <span class="mf">0.34879197790584665</span><span class="p">,</span> <span class="mf">0.33399240054348683</span><span class="p">,</span> <span class="mf">0.3347948451149971</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7866666666666666</span><span class="p">,</span> <span class="mf">0.7865</span><span class="p">,</span> <span class="mf">0.784</span><span class="p">,</span> <span class="mf">0.79375</span><span class="p">,</span> <span class="mf">0.7755833333333333</span><span class="p">,</span> <span class="mf">0.79125</span><span class="p">,</span> <span class="mf">0.7973333333333333</span><span class="p">,</span> <span class="mf">0.8085833333333333</span><span class="p">,</span> <span class="mf">0.7913333333333333</span><span class="p">,</span> <span class="mf">0.8125833333333333</span><span class="p">,</span> <span class="mf">0.81675</span><span class="p">,</span> <span class="mf">0.812</span><span class="p">,</span> <span class="mf">0.8173333333333334</span><span class="p">,</span> <span class="mf">0.8235833333333333</span><span class="p">,</span> <span class="mf">0.831</span><span class="p">,</span> <span class="mf">0.8306666666666667</span><span class="p">,</span> <span class="mf">0.8353333333333334</span><span class="p">,</span> <span class="mf">0.8320833333333333</span><span class="p">,</span> <span class="mf">0.84375</span><span class="p">,</span> <span class="mf">0.8410833333333333</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.35159709095954894</span><span class="p">,</span> <span class="mf">0.3579048192501068</span><span class="p">,</span> <span class="mf">0.3501501774787903</span><span class="p">,</span> <span class="mf">0.33594816565513613</span><span class="p">,</span> <span class="mf">0.3741619431972504</span><span class="p">,</span> <span class="mf">0.34183687329292295</span><span class="p">,</span> <span class="mf">0.3353554099798203</span><span class="p">,</span> <span class="mf">0.32617265462875367</span><span class="p">,</span> <span class="mf">0.3640907108783722</span><span class="p">,</span> <span class="mf">0.33187183618545535</span><span class="p">,</span> <span class="mf">0.32401839792728426</span><span class="p">,</span> <span class="mf">0.30536725163459777</span><span class="p">,</span> <span class="mf">0.31303414940834046</span><span class="p">,</span> <span class="mf">0.2893040508031845</span><span class="p">,</span> <span class="mf">0.3063929396867752</span><span class="p">,</span> <span class="mf">0.2909839802980423</span><span class="p">,</span> <span class="mf">0.2858921372890472</span><span class="p">,</span> <span class="mf">0.2850045281648636</span><span class="p">,</span> <span class="mf">0.28049838364124297</span><span class="p">,</span> <span class="mf">0.2873564797639847</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.816875</span><span class="p">,</span> <span class="mf">0.793125</span><span class="p">,</span> <span class="mf">0.810625</span><span class="p">,</span> <span class="mf">0.821875</span><span class="p">,</span> <span class="mf">0.8175</span><span class="p">,</span> <span class="mf">0.82</span><span class="p">,</span> <span class="mf">0.816875</span><span class="p">,</span> <span class="mf">0.814375</span><span class="p">,</span> <span class="mf">0.828125</span><span class="p">,</span> <span class="mf">0.83875</span><span class="p">,</span> <span class="mf">0.818125</span><span class="p">,</span> <span class="mf">0.843125</span><span class="p">,</span> <span class="mf">0.834375</span><span class="p">,</span> <span class="mf">0.85875</span><span class="p">,</span> <span class="mf">0.874375</span><span class="p">,</span> <span class="mf">0.85375</span><span class="p">,</span> <span class="mf">0.870625</span><span class="p">,</span> <span class="mf">0.85375</span><span class="p">,</span> <span class="mf">0.883125</span><span class="p">,</span> <span class="mf">0.848125</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.37716902824158366</span><span class="p">,</span> <span class="mf">0.3260373148195287</span><span class="p">,</span> <span class="mf">0.3128290904012132</span><span class="p">,</span> <span class="mf">0.2998493126732238</span><span class="p">,</span> <span class="mf">0.29384377892030045</span><span class="p">,</span> <span class="mf">0.2759418967873492</span><span class="p">,</span> <span class="mf">0.26431119905665834</span><span class="p">,</span> <span class="mf">0.2577077782455277</span><span class="p">,</span> <span class="mf">0.25772295725789474</span><span class="p">,</span> <span class="mf">0.24954422610871335</span><span class="p">,</span> <span class="mf">0.24065862928933285</span><span class="p">,</span> <span class="mf">0.23703582263848882</span><span class="p">,</span> <span class="mf">0.23237684028262787</span><span class="p">,</span> <span class="mf">0.2200249534575863</span><span class="p">,</span> <span class="mf">0.22110319957929722</span><span class="p">,</span> <span class="mf">0.21804759631607126</span><span class="p">,</span> <span class="mf">0.21419822757548473</span><span class="p">,</span> <span class="mf">0.19927451733816812</span><span class="p">,</span> <span class="mf">0.19864692467641323</span><span class="p">,</span> <span class="mf">0.18966749441274938</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8215833333333333</span><span class="p">,</span> <span class="mf">0.848</span><span class="p">,</span> <span class="mf">0.8526666666666667</span><span class="p">,</span> <span class="mf">0.8585</span><span class="p">,</span> <span class="mf">0.8639166666666667</span><span class="p">,</span> <span class="mf">0.8716666666666667</span><span class="p">,</span> <span class="mf">0.8783333333333333</span><span class="p">,</span> <span class="mf">0.8849166666666667</span><span class="p">,</span> <span class="mf">0.88325</span><span class="p">,</span> <span class="mf">0.88325</span><span class="p">,</span> <span class="mf">0.8918333333333334</span><span class="p">,</span> <span class="mf">0.8913333333333333</span><span class="p">,</span> <span class="mf">0.896</span><span class="p">,</span> <span class="mf">0.9010833333333333</span><span class="p">,</span> <span class="mf">0.8996666666666666</span><span class="p">,</span> <span class="mf">0.9016666666666666</span><span class="p">,</span> <span class="mf">0.902</span><span class="p">,</span> <span class="mf">0.9120833333333334</span><span class="p">,</span> <span class="mf">0.9105833333333333</span><span class="p">,</span> <span class="mf">0.9160833333333334</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3255926352739334</span><span class="p">,</span> <span class="mf">0.3397491586208343</span><span class="p">,</span> <span class="mf">0.3148202610015869</span><span class="p">,</span> <span class="mf">0.30447013437747955</span><span class="p">,</span> <span class="mf">0.27427292466163633</span><span class="p">,</span> <span class="mf">0.2607581865787506</span><span class="p">,</span> <span class="mf">0.2583494257926941</span><span class="p">,</span> <span class="mf">0.24150457441806794</span><span class="p">,</span> <span class="mf">0.24839721441268922</span><span class="p">,</span> <span class="mf">0.24157819360494615</span><span class="p">,</span> <span class="mf">0.24594406485557557</span><span class="p">,</span> <span class="mf">0.2547012311220169</span><span class="p">,</span> <span class="mf">0.24132476687431337</span><span class="p">,</span> <span class="mf">0.2433958488702774</span><span class="p">,</span> <span class="mf">0.2358475297689438</span><span class="p">,</span> <span class="mf">0.24675665378570558</span><span class="p">,</span> <span class="mf">0.23343635857105255</span><span class="p">,</span> <span class="mf">0.22841362684965133</span><span class="p">,</span> <span class="mf">0.2247604575753212</span><span class="p">,</span> <span class="mf">0.24281086921691894</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.85125</span><span class="p">,</span> <span class="mf">0.85125</span><span class="p">,</span> <span class="mf">0.853125</span><span class="p">,</span> <span class="mf">0.851875</span><span class="p">,</span> <span class="mf">0.876875</span><span class="p">,</span> <span class="mf">0.87875</span><span class="p">,</span> <span class="mf">0.883125</span><span class="p">,</span> <span class="mf">0.888125</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="mf">0.888125</span><span class="p">,</span> <span class="mf">0.88375</span><span class="p">,</span> <span class="mf">0.86625</span><span class="p">,</span> <span class="mf">0.88375</span><span class="p">,</span> <span class="mf">0.888125</span><span class="p">,</span> <span class="mf">0.898125</span><span class="p">,</span> <span class="mf">0.88875</span><span class="p">,</span> <span class="mf">0.896875</span><span class="p">,</span> <span class="mf">0.894375</span><span class="p">,</span> <span class="mf">0.899375</span><span class="p">,</span> <span class="mf">0.88625</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.3795942336796446</span><span class="p">,</span> <span class="mf">0.33614943612446174</span><span class="p">,</span> <span class="mf">0.3235826115024851</span><span class="p">,</span> <span class="mf">0.3267444484728448</span><span class="p">,</span> <span class="mf">0.30353531146303137</span><span class="p">,</span> <span class="mf">0.29750882636042353</span><span class="p">,</span> <span class="mf">0.2964640334248543</span><span class="p">,</span> <span class="mf">0.28714796314214136</span><span class="p">,</span> <span class="mf">0.2744278162717819</span><span class="p">,</span> <span class="mf">0.27310871372514584</span><span class="p">,</span> <span class="mf">0.2624819800257683</span><span class="p">,</span> <span class="mf">0.2579742945889209</span><span class="p">,</span> <span class="mf">0.25963644726954876</span><span class="p">,</span> <span class="mf">0.25635017161356644</span><span class="p">,</span> <span class="mf">0.2501001837960583</span><span class="p">,</span> <span class="mf">0.24249463702769988</span><span class="p">,</span> <span class="mf">0.23696896695393196</span><span class="p">,</span> <span class="mf">0.23254455582417072</span><span class="p">,</span> <span class="mf">0.22419108628751117</span><span class="p">,</span> <span class="mf">0.22851746232110134</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8204166666666667</span><span class="p">,</span> <span class="mf">0.839</span><span class="p">,</span> <span class="mf">0.847</span><span class="p">,</span> <span class="mf">0.8506666666666667</span><span class="p">,</span> <span class="mf">0.8571666666666666</span><span class="p">,</span> <span class="mf">0.8635</span><span class="p">,</span> <span class="mf">0.8639166666666667</span><span class="p">,</span> <span class="mf">0.8711666666666666</span><span class="p">,</span> <span class="mf">0.8711666666666666</span><span class="p">,</span> <span class="mf">0.87475</span><span class="p">,</span> <span class="mf">0.87875</span><span class="p">,</span> <span class="mf">0.87925</span><span class="p">,</span> <span class="mf">0.8805833333333334</span><span class="p">,</span> <span class="mf">0.8845</span><span class="p">,</span> <span class="mf">0.88675</span><span class="p">,</span> <span class="mf">0.8908333333333334</span><span class="p">,</span> <span class="mf">0.8926666666666667</span><span class="p">,</span> <span class="mf">0.89525</span><span class="p">,</span> <span class="mf">0.8985</span><span class="p">,</span> <span class="mf">0.8955833333333333</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3383863967657089</span><span class="p">,</span> <span class="mf">0.31120560944080355</span><span class="p">,</span> <span class="mf">0.32110977828502657</span><span class="p">,</span> <span class="mf">0.3080899566411972</span><span class="p">,</span> <span class="mf">0.2866462391614914</span><span class="p">,</span> <span class="mf">0.27701647162437437</span><span class="p">,</span> <span class="mf">0.29040718913078306</span><span class="p">,</span> <span class="mf">0.2702513742446899</span><span class="p">,</span> <span class="mf">0.2590403389930725</span><span class="p">,</span> <span class="mf">0.26199558019638064</span><span class="p">,</span> <span class="mf">0.26484714448451996</span><span class="p">,</span> <span class="mf">0.2940529054403305</span><span class="p">,</span> <span class="mf">0.2654808533191681</span><span class="p">,</span> <span class="mf">0.25154681205749513</span><span class="p">,</span> <span class="mf">0.26637687146663663</span><span class="p">,</span> <span class="mf">0.24435366928577423</span><span class="p">,</span> <span class="mf">0.24174826145172118</span><span class="p">,</span> <span class="mf">0.2444209086894989</span><span class="p">,</span> <span class="mf">0.247626873254776</span><span class="p">,</span> <span class="mf">0.24192263156175614</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.843125</span><span class="p">,</span> <span class="mf">0.8575</span><span class="p">,</span> <span class="mf">0.86</span><span class="p">,</span> <span class="mf">0.86375</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.875625</span><span class="p">,</span> <span class="mf">0.865</span><span class="p">,</span> <span class="mf">0.88</span><span class="p">,</span> <span class="mf">0.879375</span><span class="p">,</span> <span class="mf">0.885</span><span class="p">,</span> <span class="mf">0.888125</span><span class="p">,</span> <span class="mf">0.85625</span><span class="p">,</span> <span class="mf">0.87625</span><span class="p">,</span> <span class="mf">0.88375</span><span class="p">,</span> <span class="mf">0.879375</span><span class="p">,</span> <span class="mf">0.888125</span><span class="p">,</span> <span class="mf">0.8875</span><span class="p">,</span> <span class="mf">0.886875</span><span class="p">,</span> <span class="mf">0.8825</span><span class="p">,</span> <span class="mf">0.8925</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.41032169133107715</span><span class="p">,</span> <span class="mf">0.37122817583223605</span><span class="p">,</span> <span class="mf">0.35897897873470125</span><span class="p">,</span> <span class="mf">0.3438001747064768</span><span class="p">,</span> <span class="mf">0.33858899811797954</span><span class="p">,</span> <span class="mf">0.3389760729797343</span><span class="p">,</span> <span class="mf">0.32536247420184156</span><span class="p">,</span> <span class="mf">0.3152934226425404</span><span class="p">,</span> <span class="mf">0.30936657058748795</span><span class="p">,</span> <span class="mf">0.3078679118226183</span><span class="p">,</span> <span class="mf">0.30974164977669716</span><span class="p">,</span> <span class="mf">0.30031369174731537</span><span class="p">,</span> <span class="mf">0.29489042173991814</span><span class="p">,</span> <span class="mf">0.28921707251921613</span><span class="p">,</span> <span class="mf">0.28369594476324445</span><span class="p">,</span> <span class="mf">0.2849519875772456</span><span class="p">,</span> <span class="mf">0.27076949349584734</span><span class="p">,</span> <span class="mf">0.26930386248104116</span><span class="p">,</span> <span class="mf">0.26349931491657774</span><span class="p">,</span> <span class="mf">0.26431971300948176</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8086666666666666</span><span class="p">,</span> <span class="mf">0.82875</span><span class="p">,</span> <span class="mf">0.8284166666666667</span><span class="p">,</span> <span class="mf">0.8381666666666666</span><span class="p">,</span> <span class="mf">0.837</span><span class="p">,</span> <span class="mf">0.8389166666666666</span><span class="p">,</span> <span class="mf">0.8490833333333333</span><span class="p">,</span> <span class="mf">0.8488333333333333</span><span class="p">,</span> <span class="mf">0.8533333333333334</span><span class="p">,</span> <span class="mf">0.8551666666666666</span><span class="p">,</span> <span class="mf">0.8509166666666667</span><span class="p">,</span> <span class="mf">0.8615</span><span class="p">,</span> <span class="mf">0.8628333333333333</span><span class="p">,</span> <span class="mf">0.86225</span><span class="p">,</span> <span class="mf">0.8715</span><span class="p">,</span> <span class="mf">0.86775</span><span class="p">,</span> <span class="mf">0.8748333333333334</span><span class="p">,</span> <span class="mf">0.8719166666666667</span><span class="p">,</span> <span class="mf">0.8814166666666666</span><span class="p">,</span> <span class="mf">0.8835</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3464747530221939</span><span class="p">,</span> <span class="mf">0.3193131250143051</span><span class="p">,</span> <span class="mf">0.3464068531990051</span><span class="p">,</span> <span class="mf">0.3129056388139725</span><span class="p">,</span> <span class="mf">0.3131117367744446</span><span class="p">,</span> <span class="mf">0.30689118325710296</span><span class="p">,</span> <span class="mf">0.2929005026817322</span><span class="p">,</span> <span class="mf">0.3131696957349777</span><span class="p">,</span> <span class="mf">0.302835636138916</span><span class="p">,</span> <span class="mf">0.27934255003929137</span><span class="p">,</span> <span class="mf">0.300513002872467</span><span class="p">,</span> <span class="mf">0.26962003886699676</span><span class="p">,</span> <span class="mf">0.2676294481754303</span><span class="p">,</span> <span class="mf">0.26430738389492037</span><span class="p">,</span> <span class="mf">0.2525753951072693</span><span class="p">,</span> <span class="mf">0.2508367341756821</span><span class="p">,</span> <span class="mf">0.25303518533706665</span><span class="p">,</span> <span class="mf">0.24774718701839446</span><span class="p">,</span> <span class="mf">0.24518848478794097</span><span class="p">,</span> <span class="mf">0.26084545016288757</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8225</span><span class="p">,</span> <span class="mf">0.85375</span><span class="p">,</span> <span class="mf">0.849375</span><span class="p">,</span> <span class="mf">0.853125</span><span class="p">,</span> <span class="mf">0.85875</span><span class="p">,</span> <span class="mf">0.848125</span><span class="p">,</span> <span class="mf">0.856875</span><span class="p">,</span> <span class="mf">0.8575</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.869375</span><span class="p">,</span> <span class="mf">0.863125</span><span class="p">,</span> <span class="mf">0.886875</span><span class="p">,</span> <span class="mf">0.8725</span><span class="p">,</span> <span class="mf">0.878125</span><span class="p">,</span> <span class="mf">0.894375</span><span class="p">,</span> <span class="mf">0.888125</span><span class="p">,</span> <span class="mf">0.8875</span><span class="p">,</span> <span class="mf">0.89125</span><span class="p">,</span> <span class="mf">0.88875</span><span class="p">,</span> <span class="mf">0.86875</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.4765880586619073</span><span class="p">,</span> <span class="mf">0.4503744399928032</span><span class="p">,</span> <span class="mf">0.4249279998401378</span><span class="p">,</span> <span class="mf">0.42333967214886176</span><span class="p">,</span> <span class="mf">0.4236916420941657</span><span class="p">,</span> <span class="mf">0.4269233151002133</span><span class="p">,</span> <span class="mf">0.4192506206479478</span><span class="p">,</span> <span class="mf">0.41413671872083174</span><span class="p">,</span> <span class="mf">0.41084911515738104</span><span class="p">,</span> <span class="mf">0.389948022413127</span><span class="p">,</span> <span class="mf">0.39566395788433706</span><span class="p">,</span> <span class="mf">0.3741930383951106</span><span class="p">,</span> <span class="mf">0.3794517093040842</span><span class="p">,</span> <span class="mf">0.3692300356131919</span><span class="p">,</span> <span class="mf">0.3640432547223061</span><span class="p">,</span> <span class="mf">0.3608953575504587</span><span class="p">,</span> <span class="mf">0.3419572095129084</span><span class="p">,</span> <span class="mf">0.34907091543712515</span><span class="p">,</span> <span class="mf">0.33601277535583113</span><span class="p">,</span> <span class="mf">0.3408893179544743</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.77625</span><span class="p">,</span> <span class="mf">0.7823333333333333</span><span class="p">,</span> <span class="mf">0.7916666666666666</span><span class="p">,</span> <span class="mf">0.80075</span><span class="p">,</span> <span class="mf">0.7973333333333333</span><span class="p">,</span> <span class="mf">0.7810833333333334</span><span class="p">,</span> <span class="mf">0.7928333333333333</span><span class="p">,</span> <span class="mf">0.7930833333333334</span><span class="p">,</span> <span class="mf">0.7951666666666667</span><span class="p">,</span> <span class="mf">0.8015833333333333</span><span class="p">,</span> <span class="mf">0.8000833333333334</span><span class="p">,</span> <span class="mf">0.8126666666666666</span><span class="p">,</span> <span class="mf">0.811</span><span class="p">,</span> <span class="mf">0.81775</span><span class="p">,</span> <span class="mf">0.8236666666666667</span><span class="p">,</span> <span class="mf">0.8215</span><span class="p">,</span> <span class="mf">0.8305833333333333</span><span class="p">,</span> <span class="mf">0.8251666666666667</span><span class="p">,</span> <span class="mf">0.8299166666666666</span><span class="p">,</span> <span class="mf">0.836</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3674533206224442</span><span class="p">,</span> <span class="mf">0.36733597874641416</span><span class="p">,</span> <span class="mf">0.35894496202468873</span><span class="p">,</span> <span class="mf">0.3514183223247528</span><span class="p">,</span> <span class="mf">0.35345671892166136</span><span class="p">,</span> <span class="mf">0.36494161546230314</span><span class="p">,</span> <span class="mf">0.35217500329017637</span><span class="p">,</span> <span class="mf">0.3447349113225937</span><span class="p">,</span> <span class="mf">0.34697150766849516</span><span class="p">,</span> <span class="mf">0.36931039452552794</span><span class="p">,</span> <span class="mf">0.3350031852722168</span><span class="p">,</span> <span class="mf">0.3416145300865173</span><span class="p">,</span> <span class="mf">0.32389605045318604</span><span class="p">,</span> <span class="mf">0.3109715062379837</span><span class="p">,</span> <span class="mf">0.3322615468502045</span><span class="p">,</span> <span class="mf">0.327584428191185</span><span class="p">,</span> <span class="mf">0.31910278856754304</span><span class="p">,</span> <span class="mf">0.311815539598465</span><span class="p">,</span> <span class="mf">0.2950947880744934</span><span class="p">,</span> <span class="mf">0.2948034608364105</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.808125</span><span class="p">,</span> <span class="mf">0.789375</span><span class="p">,</span> <span class="mf">0.826875</span><span class="p">,</span> <span class="mf">0.821875</span><span class="p">,</span> <span class="mf">0.81375</span><span class="p">,</span> <span class="mf">0.804375</span><span class="p">,</span> <span class="mf">0.80625</span><span class="p">,</span> <span class="mf">0.83</span><span class="p">,</span> <span class="mf">0.820625</span><span class="p">,</span> <span class="mf">0.848125</span><span class="p">,</span> <span class="mf">0.816875</span><span class="p">,</span> <span class="mf">0.8125</span><span class="p">,</span> <span class="mf">0.83</span><span class="p">,</span> <span class="mf">0.84625</span><span class="p">,</span> <span class="mf">0.824375</span><span class="p">,</span> <span class="mf">0.828125</span><span class="p">,</span> <span class="mf">0.825625</span><span class="p">,</span> <span class="mf">0.840625</span><span class="p">,</span> <span class="mf">0.8475</span><span class="p">,</span> <span class="mf">0.844375</span><span class="p">]]]</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.400307985173582</span><span class="p">,</span> <span class="mf">0.2597426520640662</span><span class="p">,</span> <span class="mf">0.20706942731312025</span><span class="p">,</span> <span class="mf">0.17091670006251475</span><span class="p">,</span> <span class="mf">0.13984850759524653</span><span class="p">,</span> <span class="mf">0.11444453444522518</span><span class="p">,</span> <span class="mf">0.0929887340481538</span><span class="p">,</span> <span class="mf">0.07584588486117436</span><span class="p">,</span> <span class="mf">0.06030314570384176</span><span class="p">,</span> <span class="mf">0.04997897459031356</span><span class="p">,</span> <span class="mf">0.037156337104278056</span><span class="p">,</span> <span class="mf">0.02793900864590992</span><span class="p">,</span> <span class="mf">0.02030197833807442</span><span class="p">,</span> <span class="mf">0.01789472087045391</span><span class="p">,</span> <span class="mf">0.0175876492686666</span><span class="p">,</span> <span class="mf">0.019220354652448274</span><span class="p">,</span> <span class="mf">0.013543135874294319</span><span class="p">,</span> <span class="mf">0.006956856955481477</span><span class="p">,</span> <span class="mf">0.0024507183060002227</span><span class="p">,</span> <span class="mf">0.00206579088377317</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8547833333333333</span><span class="p">,</span> <span class="mf">0.9049</span><span class="p">,</span> <span class="mf">0.9241666666666667</span><span class="p">,</span> <span class="mf">0.9360166666666667</span><span class="p">,</span> <span class="mf">0.94695</span><span class="p">,</span> <span class="mf">0.9585833333333333</span><span class="p">,</span> <span class="mf">0.9658666666666667</span><span class="p">,</span> <span class="mf">0.9723166666666667</span><span class="p">,</span> <span class="mf">0.9780333333333333</span><span class="p">,</span> <span class="mf">0.9820166666666666</span><span class="p">,</span> <span class="mf">0.9868</span><span class="p">,</span> <span class="mf">0.9906666666666667</span><span class="p">,</span> <span class="mf">0.9936833333333334</span><span class="p">,</span> <span class="mf">0.9941333333333333</span><span class="p">,</span> <span class="mf">0.99405</span><span class="p">,</span> <span class="mf">0.9932833333333333</span><span class="p">,</span> <span class="mf">0.9960666666666667</span><span class="p">,</span> <span class="mf">0.9979666666666667</span><span class="p">,</span> <span class="mf">0.9996666666666667</span><span class="p">,</span> <span class="mf">0.9995666666666667</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.36797549843788147</span><span class="p">,</span> <span class="mf">0.2586278670430183</span><span class="p">,</span> <span class="mf">0.24208260095119477</span><span class="p">,</span> <span class="mf">0.24353929474949837</span><span class="p">,</span> <span class="mf">0.24164094921946525</span><span class="p">,</span> <span class="mf">0.2638056704550982</span><span class="p">,</span> <span class="mf">0.2579395814836025</span><span class="p">,</span> <span class="mf">0.27675500786304474</span><span class="p">,</span> <span class="mf">0.2851512663513422</span><span class="p">,</span> <span class="mf">0.30380481338500975</span><span class="p">,</span> <span class="mf">0.3235128371268511</span><span class="p">,</span> <span class="mf">0.3284085538983345</span><span class="p">,</span> <span class="mf">0.3443841063082218</span><span class="p">,</span> <span class="mf">0.41086878085136413</span><span class="p">,</span> <span class="mf">0.457796107493341</span><span class="p">,</span> <span class="mf">0.4356938077956438</span><span class="p">,</span> <span class="mf">0.4109785168170929</span><span class="p">,</span> <span class="mf">0.4433729724138975</span><span class="p">,</span> <span class="mf">0.4688420155197382</span><span class="p">,</span> <span class="mf">0.4773445381522179</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.908375</span><span class="p">,</span> <span class="mf">0.91475</span><span class="p">,</span> <span class="mf">0.915125</span><span class="p">,</span> <span class="mf">0.91525</span><span class="p">,</span> <span class="mf">0.91725</span><span class="p">,</span> <span class="mf">0.924875</span><span class="p">,</span> <span class="mf">0.91975</span><span class="p">,</span> <span class="mf">0.922375</span><span class="p">,</span> <span class="mf">0.92025</span><span class="p">,</span> <span class="mf">0.920375</span><span class="p">,</span> <span class="mf">0.924875</span><span class="p">,</span> <span class="mf">0.9235</span><span class="p">,</span> <span class="mf">0.918125</span><span class="p">,</span> <span class="mf">0.91525</span><span class="p">,</span> <span class="mf">0.918875</span><span class="p">,</span> <span class="mf">0.923625</span><span class="p">,</span> <span class="mf">0.9235</span><span class="p">,</span> <span class="mf">0.92625</span><span class="p">,</span> <span class="mf">0.925</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.4710115425463424</span><span class="p">,</span> <span class="mf">0.3166707545550647</span><span class="p">,</span> <span class="mf">0.25890692547440275</span><span class="p">,</span> <span class="mf">0.22350736999753187</span><span class="p">,</span> <span class="mf">0.19296910860009794</span><span class="p">,</span> <span class="mf">0.17304379170113154</span><span class="p">,</span> <span class="mf">0.15315235079105285</span><span class="p">,</span> <span class="mf">0.13728606270383925</span><span class="p">,</span> <span class="mf">0.12178339355929034</span><span class="p">,</span> <span class="mf">0.10961619754736898</span><span class="p">,</span> <span class="mf">0.10074329449495337</span><span class="p">,</span> <span class="mf">0.08793247367408294</span><span class="p">,</span> <span class="mf">0.07651288138686625</span><span class="p">,</span> <span class="mf">0.06934997136779089</span><span class="p">,</span> <span class="mf">0.06243234033510685</span><span class="p">,</span> <span class="mf">0.056774082654433795</span><span class="p">,</span> <span class="mf">0.05116950291028218</span><span class="p">,</span> <span class="mf">0.04961718403588313</span><span class="p">,</span> <span class="mf">0.04289388027836952</span><span class="p">,</span> <span class="mf">0.040430180404756245</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8289666666666666</span><span class="p">,</span> <span class="mf">0.8851833333333333</span><span class="p">,</span> <span class="mf">0.9045166666666666</span><span class="p">,</span> <span class="mf">0.9167666666666666</span><span class="p">,</span> <span class="mf">0.9294166666666667</span><span class="p">,</span> <span class="mf">0.93545</span><span class="p">,</span> <span class="mf">0.94275</span><span class="p">,</span> <span class="mf">0.9486666666666667</span><span class="p">,</span> <span class="mf">0.95365</span><span class="p">,</span> <span class="mf">0.95855</span><span class="p">,</span> <span class="mf">0.9618833333333333</span><span class="p">,</span> <span class="mf">0.9667</span><span class="p">,</span> <span class="mf">0.9717666666666667</span><span class="p">,</span> <span class="mf">0.9745833333333334</span><span class="p">,</span> <span class="mf">0.9765833333333334</span><span class="p">,</span> <span class="mf">0.9793</span><span class="p">,</span> <span class="mf">0.9809833333333333</span><span class="p">,</span> <span class="mf">0.9820333333333333</span><span class="p">,</span> <span class="mf">0.9839166666666667</span><span class="p">,</span> <span class="mf">0.9849166666666667</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3629846270084381</span><span class="p">,</span> <span class="mf">0.31240448981523516</span><span class="p">,</span> <span class="mf">0.24729759228229523</span><span class="p">,</span> <span class="mf">0.2697310926616192</span><span class="p">,</span> <span class="mf">0.24718070650100707</span><span class="p">,</span> <span class="mf">0.23403583562374114</span><span class="p">,</span> <span class="mf">0.2295891786813736</span><span class="p">,</span> <span class="mf">0.22117181441187858</span><span class="p">,</span> <span class="mf">0.2475375788807869</span><span class="p">,</span> <span class="mf">0.23771390727162361</span><span class="p">,</span> <span class="mf">0.2562992911040783</span><span class="p">,</span> <span class="mf">0.25533875498175623</span><span class="p">,</span> <span class="mf">0.27057862806320193</span><span class="p">,</span> <span class="mf">0.2820998176634312</span><span class="p">,</span> <span class="mf">0.29471745146811007</span><span class="p">,</span> <span class="mf">0.2795617451965809</span><span class="p">,</span> <span class="mf">0.3008101430237293</span><span class="p">,</span> <span class="mf">0.28815430629253386</span><span class="p">,</span> <span class="mf">0.31814645100384953</span><span class="p">,</span> <span class="mf">0.3106237706840038</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.874125</span><span class="p">,</span> <span class="mf">0.88875</span><span class="p">,</span> <span class="mf">0.908875</span><span class="p">,</span> <span class="mf">0.9045</span><span class="p">,</span> <span class="mf">0.9145</span><span class="p">,</span> <span class="mf">0.918125</span><span class="p">,</span> <span class="mf">0.919375</span><span class="p">,</span> <span class="mf">0.9245</span><span class="p">,</span> <span class="mf">0.91975</span><span class="p">,</span> <span class="mf">0.926</span><span class="p">,</span> <span class="mf">0.923625</span><span class="p">,</span> <span class="mf">0.925875</span><span class="p">,</span> <span class="mf">0.92475</span><span class="p">,</span> <span class="mf">0.926375</span><span class="p">,</span> <span class="mf">0.925125</span><span class="p">,</span> <span class="mf">0.92525</span><span class="p">,</span> <span class="mf">0.924625</span><span class="p">,</span> <span class="mf">0.930875</span><span class="p">,</span> <span class="mf">0.924875</span><span class="p">,</span> <span class="mf">0.926625</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.6091368444629316</span><span class="p">,</span> <span class="mf">0.40709905083309106</span><span class="p">,</span> <span class="mf">0.33330900164873106</span><span class="p">,</span> <span class="mf">0.29541655938063605</span><span class="p">,</span> <span class="mf">0.26824146830864043</span><span class="p">,</span> <span class="mf">0.24633059249535552</span><span class="p">,</span> <span class="mf">0.22803501166832219</span><span class="p">,</span> <span class="mf">0.21262132842689435</span><span class="p">,</span> <span class="mf">0.20038021789160745</span><span class="p">,</span> <span class="mf">0.18430457027680647</span><span class="p">,</span> <span class="mf">0.1744787511763288</span><span class="p">,</span> <span class="mf">0.165271017740149</span><span class="p">,</span> <span class="mf">0.15522625095554507</span><span class="p">,</span> <span class="mf">0.1432937567076608</span><span class="p">,</span> <span class="mf">0.13617747858651222</span><span class="p">,</span> <span class="mf">0.12876031456241158</span><span class="p">,</span> <span class="mf">0.12141566201230325</span><span class="p">,</span> <span class="mf">0.11405601029369686</span><span class="p">,</span> <span class="mf">0.11116664642408522</span><span class="p">,</span> <span class="mf">0.10308189516060992</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7803833333333333</span><span class="p">,</span> <span class="mf">0.8559166666666667</span><span class="p">,</span> <span class="mf">0.8823</span><span class="p">,</span> <span class="mf">0.89505</span><span class="p">,</span> <span class="mf">0.9027333333333334</span><span class="p">,</span> <span class="mf">0.9099166666666667</span><span class="p">,</span> <span class="mf">0.9162333333333333</span><span class="p">,</span> <span class="mf">0.9224833333333333</span><span class="p">,</span> <span class="mf">0.9243166666666667</span><span class="p">,</span> <span class="mf">0.9321</span><span class="p">,</span> <span class="mf">0.9345833333333333</span><span class="p">,</span> <span class="mf">0.9375333333333333</span><span class="p">,</span> <span class="mf">0.9418833333333333</span><span class="p">,</span> <span class="mf">0.9456666666666667</span><span class="p">,</span> <span class="mf">0.9482333333333334</span><span class="p">,</span> <span class="mf">0.9513666666666667</span><span class="p">,</span> <span class="mf">0.9527333333333333</span><span class="p">,</span> <span class="mf">0.9559</span><span class="p">,</span> <span class="mf">0.9576166666666667</span><span class="p">,</span> <span class="mf">0.9611</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.36491659212112426</span><span class="p">,</span> <span class="mf">0.29200539910793305</span><span class="p">,</span> <span class="mf">0.2840233483910561</span><span class="p">,</span> <span class="mf">0.2591339669823646</span><span class="p">,</span> <span class="mf">0.24114771646261215</span><span class="p">,</span> <span class="mf">0.2436459481716156</span><span class="p">,</span> <span class="mf">0.2374294084906578</span><span class="p">,</span> <span class="mf">0.24284198743104934</span><span class="p">,</span> <span class="mf">0.22679156363010405</span><span class="p">,</span> <span class="mf">0.2229055170416832</span><span class="p">,</span> <span class="mf">0.21932773572206496</span><span class="p">,</span> <span class="mf">0.23045065227150918</span><span class="p">,</span> <span class="mf">0.23631879675388337</span><span class="p">,</span> <span class="mf">0.22048399156332016</span><span class="p">,</span> <span class="mf">0.2563135535418987</span><span class="p">,</span> <span class="mf">0.2494968646839261</span><span class="p">,</span> <span class="mf">0.24099056956171988</span><span class="p">,</span> <span class="mf">0.23974315640330315</span><span class="p">,</span> <span class="mf">0.24684958010911942</span><span class="p">,</span> <span class="mf">0.25887142738699914</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8665</span><span class="p">,</span> <span class="mf">0.8925</span><span class="p">,</span> <span class="mf">0.897</span><span class="p">,</span> <span class="mf">0.907375</span><span class="p">,</span> <span class="mf">0.914125</span><span class="p">,</span> <span class="mf">0.9125</span><span class="p">,</span> <span class="mf">0.913875</span><span class="p">,</span> <span class="mf">0.911875</span><span class="p">,</span> <span class="mf">0.921125</span><span class="p">,</span> <span class="mf">0.922625</span><span class="p">,</span> <span class="mf">0.923375</span><span class="p">,</span> <span class="mf">0.924125</span><span class="p">,</span> <span class="mf">0.922625</span><span class="p">,</span> <span class="mf">0.926</span><span class="p">,</span> <span class="mf">0.915625</span><span class="p">,</span> <span class="mf">0.926125</span><span class="p">,</span> <span class="mf">0.932625</span><span class="p">,</span> <span class="mf">0.927875</span><span class="p">,</span> <span class="mf">0.93</span><span class="p">,</span> <span class="mf">0.92525</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.187068938827718</span><span class="p">,</span> <span class="mf">0.9080034740316842</span><span class="p">,</span> <span class="mf">0.6863665148329887</span><span class="p">,</span> <span class="mf">0.5706229420867301</span><span class="p">,</span> <span class="mf">0.5069490017921432</span><span class="p">,</span> <span class="mf">0.46316734996876485</span><span class="p">,</span> <span class="mf">0.42913920047885573</span><span class="p">,</span> <span class="mf">0.4107565824855874</span><span class="p">,</span> <span class="mf">0.3908677859061054</span><span class="p">,</span> <span class="mf">0.37283689377785745</span><span class="p">,</span> <span class="mf">0.3606657798388111</span><span class="p">,</span> <span class="mf">0.353545261082301</span><span class="p">,</span> <span class="mf">0.34009441143986</span><span class="p">,</span> <span class="mf">0.3239413740506559</span><span class="p">,</span> <span class="mf">0.3193119444620253</span><span class="p">,</span> <span class="mf">0.31045137204404577</span><span class="p">,</span> <span class="mf">0.3003838519091164</span><span class="p">,</span> <span class="mf">0.29092520530194615</span><span class="p">,</span> <span class="mf">0.28635713599447504</span><span class="p">,</span> <span class="mf">0.2760026559138349</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5551333333333334</span><span class="p">,</span> <span class="mf">0.6467</span><span class="p">,</span> <span class="mf">0.7338666666666667</span><span class="p">,</span> <span class="mf">0.7841333333333333</span><span class="p">,</span> <span class="mf">0.8128</span><span class="p">,</span> <span class="mf">0.82845</span><span class="p">,</span> <span class="mf">0.8430833333333333</span><span class="p">,</span> <span class="mf">0.8501666666666666</span><span class="p">,</span> <span class="mf">0.8580833333333333</span><span class="p">,</span> <span class="mf">0.8646166666666667</span><span class="p">,</span> <span class="mf">0.8667666666666667</span><span class="p">,</span> <span class="mf">0.8709833333333333</span><span class="p">,</span> <span class="mf">0.8766166666666667</span><span class="p">,</span> <span class="mf">0.8816666666666667</span><span class="p">,</span> <span class="mf">0.8812</span><span class="p">,</span> <span class="mf">0.88465</span><span class="p">,</span> <span class="mf">0.8898833333333334</span><span class="p">,</span> <span class="mf">0.8934666666666666</span><span class="p">,</span> <span class="mf">0.8940833333333333</span><span class="p">,</span> <span class="mf">0.8977666666666667</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.6463955206871033</span><span class="p">,</span> <span class="mf">0.5193838343620301</span><span class="p">,</span> <span class="mf">0.4155286856889725</span><span class="p">,</span> <span class="mf">0.3316091845035553</span><span class="p">,</span> <span class="mf">0.3148408111333847</span><span class="p">,</span> <span class="mf">0.29354524302482604</span><span class="p">,</span> <span class="mf">0.2875490103960037</span><span class="p">,</span> <span class="mf">0.26903486740589144</span><span class="p">,</span> <span class="mf">0.27737221759557723</span><span class="p">,</span> <span class="mf">0.262776792883873</span><span class="p">,</span> <span class="mf">0.25498255288600924</span><span class="p">,</span> <span class="mf">0.2390553195178509</span><span class="p">,</span> <span class="mf">0.24918611392378806</span><span class="p">,</span> <span class="mf">0.23830307483673097</span><span class="p">,</span> <span class="mf">0.23538302001357078</span><span class="p">,</span> <span class="mf">0.24996423116326333</span><span class="p">,</span> <span class="mf">0.2464654156267643</span><span class="p">,</span> <span class="mf">0.24081429636478424</span><span class="p">,</span> <span class="mf">0.23204647853970528</span><span class="p">,</span> <span class="mf">0.23771219885349273</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.763875</span><span class="p">,</span> <span class="mf">0.81925</span><span class="p">,</span> <span class="mf">0.8685</span><span class="p">,</span> <span class="mf">0.8885</span><span class="p">,</span> <span class="mf">0.8895</span><span class="p">,</span> <span class="mf">0.895625</span><span class="p">,</span> <span class="mf">0.902</span><span class="p">,</span> <span class="mf">0.904125</span><span class="p">,</span> <span class="mf">0.906125</span><span class="p">,</span> <span class="mf">0.908</span><span class="p">,</span> <span class="mf">0.909375</span><span class="p">,</span> <span class="mf">0.9145</span><span class="p">,</span> <span class="mf">0.916125</span><span class="p">,</span> <span class="mf">0.9175</span><span class="p">,</span> <span class="mf">0.91875</span><span class="p">,</span> <span class="mf">0.91425</span><span class="p">,</span> <span class="mf">0.915375</span><span class="p">,</span> <span class="mf">0.918875</span><span class="p">,</span> <span class="mf">0.91975</span><span class="p">,</span> <span class="mf">0.91825</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.4140813298491654</span><span class="p">,</span> <span class="mf">0.27481235485118843</span><span class="p">,</span> <span class="mf">0.22397600941614174</span><span class="p">,</span> <span class="mf">0.1890777693286951</span><span class="p">,</span> <span class="mf">0.16538111197112848</span><span class="p">,</span> <span class="mf">0.1448796250478132</span><span class="p">,</span> <span class="mf">0.12440053254032313</span><span class="p">,</span> <span class="mf">0.10817898457734855</span><span class="p">,</span> <span class="mf">0.09634132136696025</span><span class="p">,</span> <span class="mf">0.08548538653410352</span><span class="p">,</span> <span class="mf">0.07339220296349257</span><span class="p">,</span> <span class="mf">0.06470446296305314</span><span class="p">,</span> <span class="mf">0.060030178171393875</span><span class="p">,</span> <span class="mf">0.053294485403614034</span><span class="p">,</span> <span class="mf">0.04429284706704323</span><span class="p">,</span> <span class="mf">0.04014099264770115</span><span class="p">,</span> <span class="mf">0.03974721442450951</span><span class="p">,</span> <span class="mf">0.03304463665041803</span><span class="p">,</span> <span class="mf">0.02955428938137994</span><span class="p">,</span> <span class="mf">0.026940144761875052</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8496666666666667</span><span class="p">,</span> <span class="mf">0.8982666666666667</span><span class="p">,</span> <span class="mf">0.9162166666666667</span><span class="p">,</span> <span class="mf">0.9292166666666667</span><span class="p">,</span> <span class="mf">0.93805</span><span class="p">,</span> <span class="mf">0.9457666666666666</span><span class="p">,</span> <span class="mf">0.9534333333333334</span><span class="p">,</span> <span class="mf">0.9596</span><span class="p">,</span> <span class="mf">0.9645833333333333</span><span class="p">,</span> <span class="mf">0.9679</span><span class="p">,</span> <span class="mf">0.9726166666666667</span><span class="p">,</span> <span class="mf">0.9761666666666666</span><span class="p">,</span> <span class="mf">0.9775</span><span class="p">,</span> <span class="mf">0.9800166666666666</span><span class="p">,</span> <span class="mf">0.9842</span><span class="p">,</span> <span class="mf">0.9855333333333334</span><span class="p">,</span> <span class="mf">0.9857</span><span class="p">,</span> <span class="mf">0.98805</span><span class="p">,</span> <span class="mf">0.9895666666666667</span><span class="p">,</span> <span class="mf">0.9905833333333334</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3327465409040451</span><span class="p">,</span> <span class="mf">0.27738857254385946</span><span class="p">,</span> <span class="mf">0.23834018683433533</span><span class="p">,</span> <span class="mf">0.24359044748544692</span><span class="p">,</span> <span class="mf">0.23630736249685289</span><span class="p">,</span> <span class="mf">0.26239568686485293</span><span class="p">,</span> <span class="mf">0.23089197066426276</span><span class="p">,</span> <span class="mf">0.23183160039782524</span><span class="p">,</span> <span class="mf">0.2287161501646042</span><span class="p">,</span> <span class="mf">0.23795067170262338</span><span class="p">,</span> <span class="mf">0.2680365410447121</span><span class="p">,</span> <span class="mf">0.28079107534885406</span><span class="p">,</span> <span class="mf">0.2745736412107945</span><span class="p">,</span> <span class="mf">0.27641161236166956</span><span class="p">,</span> <span class="mf">0.2967236565724015</span><span class="p">,</span> <span class="mf">0.29836027943715454</span><span class="p">,</span> <span class="mf">0.28526886811852453</span><span class="p">,</span> <span class="mf">0.3188628684282303</span><span class="p">,</span> <span class="mf">0.3159900237545371</span><span class="p">,</span> <span class="mf">0.33990017675608397</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.876875</span><span class="p">,</span> <span class="mf">0.899875</span><span class="p">,</span> <span class="mf">0.918125</span><span class="p">,</span> <span class="mf">0.9105</span><span class="p">,</span> <span class="mf">0.918125</span><span class="p">,</span> <span class="mf">0.91</span><span class="p">,</span> <span class="mf">0.92075</span><span class="p">,</span> <span class="mf">0.922625</span><span class="p">,</span> <span class="mf">0.924</span><span class="p">,</span> <span class="mf">0.921</span><span class="p">,</span> <span class="mf">0.920875</span><span class="p">,</span> <span class="mf">0.921</span><span class="p">,</span> <span class="mf">0.9285</span><span class="p">,</span> <span class="mf">0.927625</span><span class="p">,</span> <span class="mf">0.9265</span><span class="p">,</span> <span class="mf">0.927375</span><span class="p">,</span> <span class="mf">0.925875</span><span class="p">,</span> <span class="mf">0.927</span><span class="p">,</span> <span class="mf">0.92575</span><span class="p">,</span> <span class="mf">0.925875</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.48859380523978013</span><span class="p">,</span> <span class="mf">0.3269256727337075</span><span class="p">,</span> <span class="mf">0.275135099903734</span><span class="p">,</span> <span class="mf">0.24039912359244914</span><span class="p">,</span> <span class="mf">0.21368402032566858</span><span class="p">,</span> <span class="mf">0.19328243048317523</span><span class="p">,</span> <span class="mf">0.17890911489359732</span><span class="p">,</span> <span class="mf">0.16624130663682402</span><span class="p">,</span> <span class="mf">0.15215728174088827</span><span class="p">,</span> <span class="mf">0.1416037013468299</span><span class="p">,</span> <span class="mf">0.13273427299440288</span><span class="p">,</span> <span class="mf">0.12227611260405227</span><span class="p">,</span> <span class="mf">0.11463099068699917</span><span class="p">,</span> <span class="mf">0.10616964906720179</span><span class="p">,</span> <span class="mf">0.09988978996809357</span><span class="p">,</span> <span class="mf">0.09424899211093815</span><span class="p">,</span> <span class="mf">0.08670466838887077</span><span class="p">,</span> <span class="mf">0.0835973875783781</span><span class="p">,</span> <span class="mf">0.0778748192367698</span><span class="p">,</span> <span class="mf">0.07327510508696741</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.82055</span><span class="p">,</span> <span class="mf">0.8806666666666667</span><span class="p">,</span> <span class="mf">0.9004333333333333</span><span class="p">,</span> <span class="mf">0.9117333333333333</span><span class="p">,</span> <span class="mf">0.9206333333333333</span><span class="p">,</span> <span class="mf">0.92785</span><span class="p">,</span> <span class="mf">0.9333</span><span class="p">,</span> <span class="mf">0.9384166666666667</span><span class="p">,</span> <span class="mf">0.9430333333333333</span><span class="p">,</span> <span class="mf">0.9471833333333334</span><span class="p">,</span> <span class="mf">0.95055</span><span class="p">,</span> <span class="mf">0.9540166666666666</span><span class="p">,</span> <span class="mf">0.9568833333333333</span><span class="p">,</span> <span class="mf">0.9601666666666666</span><span class="p">,</span> <span class="mf">0.9620333333333333</span><span class="p">,</span> <span class="mf">0.9652</span><span class="p">,</span> <span class="mf">0.9676833333333333</span><span class="p">,</span> <span class="mf">0.9682666666666667</span><span class="p">,</span> <span class="mf">0.9706</span><span class="p">,</span> <span class="mf">0.9724333333333334</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.34025013536214826</span><span class="p">,</span> <span class="mf">0.29788709819316866</span><span class="p">,</span> <span class="mf">0.2680273652672768</span><span class="p">,</span> <span class="mf">0.2463292105793953</span><span class="p">,</span> <span class="mf">0.23471139985322953</span><span class="p">,</span> <span class="mf">0.22580294385552407</span><span class="p">,</span> <span class="mf">0.21676637730002404</span><span class="p">,</span> <span class="mf">0.20925517010688782</span><span class="p">,</span> <span class="mf">0.23552959233522416</span><span class="p">,</span> <span class="mf">0.21975916308164598</span><span class="p">,</span> <span class="mf">0.23494828915596008</span><span class="p">,</span> <span class="mf">0.21611644634604454</span><span class="p">,</span> <span class="mf">0.22251244640350343</span><span class="p">,</span> <span class="mf">0.22066593673825263</span><span class="p">,</span> <span class="mf">0.2214409472346306</span><span class="p">,</span> <span class="mf">0.22849382662773132</span><span class="p">,</span> <span class="mf">0.24493269926309585</span><span class="p">,</span> <span class="mf">0.2397777333110571</span><span class="p">,</span> <span class="mf">0.23578458192944526</span><span class="p">,</span> <span class="mf">0.2563280282020569</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.870875</span><span class="p">,</span> <span class="mf">0.8875</span><span class="p">,</span> <span class="mf">0.900375</span><span class="p">,</span> <span class="mf">0.906625</span><span class="p">,</span> <span class="mf">0.9145</span><span class="p">,</span> <span class="mf">0.921125</span><span class="p">,</span> <span class="mf">0.92125</span><span class="p">,</span> <span class="mf">0.92425</span><span class="p">,</span> <span class="mf">0.916</span><span class="p">,</span> <span class="mf">0.923125</span><span class="p">,</span> <span class="mf">0.920375</span><span class="p">,</span> <span class="mf">0.92675</span><span class="p">,</span> <span class="mf">0.92575</span><span class="p">,</span> <span class="mf">0.924875</span><span class="p">,</span> <span class="mf">0.925</span><span class="p">,</span> <span class="mf">0.924875</span><span class="p">,</span> <span class="mf">0.922875</span><span class="p">,</span> <span class="mf">0.931125</span><span class="p">,</span> <span class="mf">0.932375</span><span class="p">,</span> <span class="mf">0.929</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.6104797730917362</span><span class="p">,</span> <span class="mf">0.42115319246994154</span><span class="p">,</span> <span class="mf">0.3527538229359874</span><span class="p">,</span> <span class="mf">0.3136731511446586</span><span class="p">,</span> <span class="mf">0.2857721160565104</span><span class="p">,</span> <span class="mf">0.26646374052426197</span><span class="p">,</span> <span class="mf">0.24732486170523965</span><span class="p">,</span> <span class="mf">0.23057452346613286</span><span class="p">,</span> <span class="mf">0.21953405395769743</span><span class="p">,</span> <span class="mf">0.20952929538100767</span><span class="p">,</span> <span class="mf">0.19584925043811677</span><span class="p">,</span> <span class="mf">0.18926965880162044</span><span class="p">,</span> <span class="mf">0.18003955145856973</span><span class="p">,</span> <span class="mf">0.17379174885878176</span><span class="p">,</span> <span class="mf">0.16635702809354644</span><span class="p">,</span> <span class="mf">0.15807223409366633</span><span class="p">,</span> <span class="mf">0.1509416516620054</span><span class="p">,</span> <span class="mf">0.1477138751140758</span><span class="p">,</span> <span class="mf">0.14028569269798266</span><span class="p">,</span> <span class="mf">0.13906246528172417</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7786833333333333</span><span class="p">,</span> <span class="mf">0.8482166666666666</span><span class="p">,</span> <span class="mf">0.8730833333333333</span><span class="p">,</span> <span class="mf">0.888</span><span class="p">,</span> <span class="mf">0.8978</span><span class="p">,</span> <span class="mf">0.9033666666666667</span><span class="p">,</span> <span class="mf">0.9089166666666667</span><span class="p">,</span> <span class="mf">0.9147666666666666</span><span class="p">,</span> <span class="mf">0.91955</span><span class="p">,</span> <span class="mf">0.9221833333333334</span><span class="p">,</span> <span class="mf">0.92715</span><span class="p">,</span> <span class="mf">0.9309666666666667</span><span class="p">,</span> <span class="mf">0.9334</span><span class="p">,</span> <span class="mf">0.93495</span><span class="p">,</span> <span class="mf">0.9376833333333333</span><span class="p">,</span> <span class="mf">0.9402666666666667</span><span class="p">,</span> <span class="mf">0.94405</span><span class="p">,</span> <span class="mf">0.9439166666666666</span><span class="p">,</span> <span class="mf">0.9466833333333333</span><span class="p">,</span> <span class="mf">0.9464833333333333</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3859497320652008</span><span class="p">,</span> <span class="mf">0.3124091213941574</span><span class="p">,</span> <span class="mf">0.28177140313386917</span><span class="p">,</span> <span class="mf">0.2564259949326515</span><span class="p">,</span> <span class="mf">0.24969424712657928</span><span class="p">,</span> <span class="mf">0.23137387067079543</span><span class="p">,</span> <span class="mf">0.22758139592409135</span><span class="p">,</span> <span class="mf">0.22978509336709976</span><span class="p">,</span> <span class="mf">0.2293499847650528</span><span class="p">,</span> <span class="mf">0.22430640310049058</span><span class="p">,</span> <span class="mf">0.21563700905442237</span><span class="p">,</span> <span class="mf">0.21529569518566133</span><span class="p">,</span> <span class="mf">0.22171301135420798</span><span class="p">,</span> <span class="mf">0.2105387990772724</span><span class="p">,</span> <span class="mf">0.21190602815151213</span><span class="p">,</span> <span class="mf">0.21494245541095733</span><span class="p">,</span> <span class="mf">0.21312989933788776</span><span class="p">,</span> <span class="mf">0.20670134457945824</span><span class="p">,</span> <span class="mf">0.2146600303351879</span><span class="p">,</span> <span class="mf">0.21474341893941165</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.86</span><span class="p">,</span> <span class="mf">0.888</span><span class="p">,</span> <span class="mf">0.89625</span><span class="p">,</span> <span class="mf">0.907</span><span class="p">,</span> <span class="mf">0.908</span><span class="p">,</span> <span class="mf">0.915</span><span class="p">,</span> <span class="mf">0.917875</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">,</span> <span class="mf">0.921125</span><span class="p">,</span> <span class="mf">0.917625</span><span class="p">,</span> <span class="mf">0.924</span><span class="p">,</span> <span class="mf">0.921875</span><span class="p">,</span> <span class="mf">0.925875</span><span class="p">,</span> <span class="mf">0.92575</span><span class="p">,</span> <span class="mf">0.928125</span><span class="p">,</span> <span class="mf">0.92775</span><span class="p">,</span> <span class="mf">0.928625</span><span class="p">,</span> <span class="mf">0.93075</span><span class="p">,</span> <span class="mf">0.92975</span><span class="p">,</span> <span class="mf">0.930375</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.1724896589194789</span><span class="p">,</span> <span class="mf">0.8803599189911315</span><span class="p">,</span> <span class="mf">0.692622532690766</span><span class="p">,</span> <span class="mf">0.5974764075837156</span><span class="p">,</span> <span class="mf">0.5319996399920124</span><span class="p">,</span> <span class="mf">0.49373906012028773</span><span class="p">,</span> <span class="mf">0.4741932853007876</span><span class="p">,</span> <span class="mf">0.45601858158927483</span><span class="p">,</span> <span class="mf">0.43706520244892216</span><span class="p">,</span> <span class="mf">0.4238534729236733</span><span class="p">,</span> <span class="mf">0.41077356216813454</span><span class="p">,</span> <span class="mf">0.38932509837882606</span><span class="p">,</span> <span class="mf">0.3771154705856019</span><span class="p">,</span> <span class="mf">0.3687882057305719</span><span class="p">,</span> <span class="mf">0.34927689276937485</span><span class="p">,</span> <span class="mf">0.3379922736602933</span><span class="p">,</span> <span class="mf">0.33547254843212393</span><span class="p">,</span> <span class="mf">0.3263144160448107</span><span class="p">,</span> <span class="mf">0.31800466419251233</span><span class="p">,</span> <span class="mf">0.3133781185822446</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5631833333333334</span><span class="p">,</span> <span class="mf">0.6579333333333334</span><span class="p">,</span> <span class="mf">0.7342166666666666</span><span class="p">,</span> <span class="mf">0.7765833333333333</span><span class="p">,</span> <span class="mf">0.8036333333333333</span><span class="p">,</span> <span class="mf">0.8197166666666666</span><span class="p">,</span> <span class="mf">0.82755</span><span class="p">,</span> <span class="mf">0.8320166666666666</span><span class="p">,</span> <span class="mf">0.8397833333333333</span><span class="p">,</span> <span class="mf">0.8432666666666667</span><span class="p">,</span> <span class="mf">0.8519333333333333</span><span class="p">,</span> <span class="mf">0.85835</span><span class="p">,</span> <span class="mf">0.86285</span><span class="p">,</span> <span class="mf">0.8641</span><span class="p">,</span> <span class="mf">0.87105</span><span class="p">,</span> <span class="mf">0.8756666666666667</span><span class="p">,</span> <span class="mf">0.8775166666666666</span><span class="p">,</span> <span class="mf">0.87965</span><span class="p">,</span> <span class="mf">0.88255</span><span class="p">,</span> <span class="mf">0.8832333333333333</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5745115535259246</span><span class="p">,</span> <span class="mf">0.4740168128013611</span><span class="p">,</span> <span class="mf">0.4092038922309876</span><span class="p">,</span> <span class="mf">0.345498643040657</span><span class="p">,</span> <span class="mf">0.32894178831577303</span><span class="p">,</span> <span class="mf">0.2999964846372604</span><span class="p">,</span> <span class="mf">0.28456189918518066</span><span class="p">,</span> <span class="mf">0.28186965006589887</span><span class="p">,</span> <span class="mf">0.26958267349004744</span><span class="p">,</span> <span class="mf">0.26703972268104553</span><span class="p">,</span> <span class="mf">0.2667745503783226</span><span class="p">,</span> <span class="mf">0.2553461962342262</span><span class="p">,</span> <span class="mf">0.25764305877685545</span><span class="p">,</span> <span class="mf">0.2528705199956894</span><span class="p">,</span> <span class="mf">0.24987997275590895</span><span class="p">,</span> <span class="mf">0.24210182267427444</span><span class="p">,</span> <span class="mf">0.2366510547697544</span><span class="p">,</span> <span class="mf">0.24053962442278862</span><span class="p">,</span> <span class="mf">0.22825994032621383</span><span class="p">,</span> <span class="mf">0.2270425768494606</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.776875</span><span class="p">,</span> <span class="mf">0.822625</span><span class="p">,</span> <span class="mf">0.848875</span><span class="p">,</span> <span class="mf">0.87825</span><span class="p">,</span> <span class="mf">0.88925</span><span class="p">,</span> <span class="mf">0.899875</span><span class="p">,</span> <span class="mf">0.9015</span><span class="p">,</span> <span class="mf">0.904375</span><span class="p">,</span> <span class="mf">0.9035</span><span class="p">,</span> <span class="mf">0.906</span><span class="p">,</span> <span class="mf">0.906875</span><span class="p">,</span> <span class="mf">0.91125</span><span class="p">,</span> <span class="mf">0.907</span><span class="p">,</span> <span class="mf">0.908625</span><span class="p">,</span> <span class="mf">0.91175</span><span class="p">,</span> <span class="mf">0.917125</span><span class="p">,</span> <span class="mf">0.91675</span><span class="p">,</span> <span class="mf">0.916125</span><span class="p">,</span> <span class="mf">0.919875</span><span class="p">,</span> <span class="mf">0.917625</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.43062501005145276</span><span class="p">,</span> <span class="mf">0.29807482149078646</span><span class="p">,</span> <span class="mf">0.2541527441585623</span><span class="p">,</span> <span class="mf">0.21918726423338278</span><span class="p">,</span> <span class="mf">0.1950343672964555</span><span class="p">,</span> <span class="mf">0.17517360023010387</span><span class="p">,</span> <span class="mf">0.16213757058244144</span><span class="p">,</span> <span class="mf">0.14869415854364</span><span class="p">,</span> <span class="mf">0.13477844860392815</span><span class="p">,</span> <span class="mf">0.12352272007129848</span><span class="p">,</span> <span class="mf">0.11392300839184412</span><span class="p">,</span> <span class="mf">0.10589898744228679</span><span class="p">,</span> <span class="mf">0.09751250602896692</span><span class="p">,</span> <span class="mf">0.089864786467088</span><span class="p">,</span> <span class="mf">0.08516462990539526</span><span class="p">,</span> <span class="mf">0.07973235945548934</span><span class="p">,</span> <span class="mf">0.07441158362824137</span><span class="p">,</span> <span class="mf">0.07053931183896578</span><span class="p">,</span> <span class="mf">0.06258528833356954</span><span class="p">,</span> <span class="mf">0.06177985634201014</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8429</span><span class="p">,</span> <span class="mf">0.88905</span><span class="p">,</span> <span class="mf">0.9052166666666667</span><span class="p">,</span> <span class="mf">0.9182166666666667</span><span class="p">,</span> <span class="mf">0.92755</span><span class="p">,</span> <span class="mf">0.9337666666666666</span><span class="p">,</span> <span class="mf">0.93835</span><span class="p">,</span> <span class="mf">0.944</span><span class="p">,</span> <span class="mf">0.9489333333333333</span><span class="p">,</span> <span class="mf">0.95365</span><span class="p">,</span> <span class="mf">0.9565333333333333</span><span class="p">,</span> <span class="mf">0.9599166666666666</span><span class="p">,</span> <span class="mf">0.9637833333333333</span><span class="p">,</span> <span class="mf">0.9659666666666666</span><span class="p">,</span> <span class="mf">0.9685666666666667</span><span class="p">,</span> <span class="mf">0.9705</span><span class="p">,</span> <span class="mf">0.9713666666666667</span><span class="p">,</span> <span class="mf">0.9738</span><span class="p">,</span> <span class="mf">0.9770166666666666</span><span class="p">,</span> <span class="mf">0.9769833333333333</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.32814766228199005</span><span class="p">,</span> <span class="mf">0.29447353577613833</span><span class="p">,</span> <span class="mf">0.25052148789167406</span><span class="p">,</span> <span class="mf">0.22761481428146363</span><span class="p">,</span> <span class="mf">0.23280890756845474</span><span class="p">,</span> <span class="mf">0.23155913531780242</span><span class="p">,</span> <span class="mf">0.21984874603152274</span><span class="p">,</span> <span class="mf">0.2166314404308796</span><span class="p">,</span> <span class="mf">0.2202563073039055</span><span class="p">,</span> <span class="mf">0.22508277136087418</span><span class="p">,</span> <span class="mf">0.2237191815972328</span><span class="p">,</span> <span class="mf">0.2246915928721428</span><span class="p">,</span> <span class="mf">0.22815296687185765</span><span class="p">,</span> <span class="mf">0.2254556802213192</span><span class="p">,</span> <span class="mf">0.2337513281852007</span><span class="p">,</span> <span class="mf">0.2381753808259964</span><span class="p">,</span> <span class="mf">0.24798179551959038</span><span class="p">,</span> <span class="mf">0.24766947883367538</span><span class="p">,</span> <span class="mf">0.24877363580465317</span><span class="p">,</span> <span class="mf">0.2518915164768696</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.879625</span><span class="p">,</span> <span class="mf">0.89025</span><span class="p">,</span> <span class="mf">0.907875</span><span class="p">,</span> <span class="mf">0.916625</span><span class="p">,</span> <span class="mf">0.91625</span><span class="p">,</span> <span class="mf">0.91825</span><span class="p">,</span> <span class="mf">0.920875</span><span class="p">,</span> <span class="mf">0.923625</span><span class="p">,</span> <span class="mf">0.922625</span><span class="p">,</span> <span class="mf">0.923</span><span class="p">,</span> <span class="mf">0.92575</span><span class="p">,</span> <span class="mf">0.927125</span><span class="p">,</span> <span class="mf">0.928625</span><span class="p">,</span> <span class="mf">0.92625</span><span class="p">,</span> <span class="mf">0.925375</span><span class="p">,</span> <span class="mf">0.925625</span><span class="p">,</span> <span class="mf">0.926375</span><span class="p">,</span> <span class="mf">0.92475</span><span class="p">,</span> <span class="mf">0.9255</span><span class="p">,</span> <span class="mf">0.92675</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.5022556754285847</span><span class="p">,</span> <span class="mf">0.3545388207554436</span><span class="p">,</span> <span class="mf">0.2965180559564374</span><span class="p">,</span> <span class="mf">0.2689443711818917</span><span class="p">,</span> <span class="mf">0.24340009927622544</span><span class="p">,</span> <span class="mf">0.22504497168144819</span><span class="p">,</span> <span class="mf">0.21177587015574167</span><span class="p">,</span> <span class="mf">0.19926073912507308</span><span class="p">,</span> <span class="mf">0.18498492261557692</span><span class="p">,</span> <span class="mf">0.1792394390810273</span><span class="p">,</span> <span class="mf">0.16716771742809555</span><span class="p">,</span> <span class="mf">0.16088557891500022</span><span class="p">,</span> <span class="mf">0.15540826101420022</span><span class="p">,</span> <span class="mf">0.1471743908549931</span><span class="p">,</span> <span class="mf">0.14383414784458273</span><span class="p">,</span> <span class="mf">0.1351151093741311</span><span class="p">,</span> <span class="mf">0.1312572255915305</span><span class="p">,</span> <span class="mf">0.12904865093140014</span><span class="p">,</span> <span class="mf">0.12332957751079918</span><span class="p">,</span> <span class="mf">0.11934908895072208</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8186333333333333</span><span class="p">,</span> <span class="mf">0.8711666666666666</span><span class="p">,</span> <span class="mf">0.8905666666666666</span><span class="p">,</span> <span class="mf">0.9020666666666667</span><span class="p">,</span> <span class="mf">0.9106333333333333</span><span class="p">,</span> <span class="mf">0.9169333333333334</span><span class="p">,</span> <span class="mf">0.9227</span><span class="p">,</span> <span class="mf">0.9258166666666666</span><span class="p">,</span> <span class="mf">0.9317</span><span class="p">,</span> <span class="mf">0.9329666666666667</span><span class="p">,</span> <span class="mf">0.9384833333333333</span><span class="p">,</span> <span class="mf">0.9394333333333333</span><span class="p">,</span> <span class="mf">0.94185</span><span class="p">,</span> <span class="mf">0.9447666666666666</span><span class="p">,</span> <span class="mf">0.9449833333333333</span><span class="p">,</span> <span class="mf">0.9489</span><span class="p">,</span> <span class="mf">0.9506</span><span class="p">,</span> <span class="mf">0.9520333333333333</span><span class="p">,</span> <span class="mf">0.95295</span><span class="p">,</span> <span class="mf">0.9556833333333333</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.37072600054740906</span><span class="p">,</span> <span class="mf">0.2894986196160316</span><span class="p">,</span> <span class="mf">0.2896255247592926</span><span class="p">,</span> <span class="mf">0.2553737629055977</span><span class="p">,</span> <span class="mf">0.2347450014948845</span><span class="p">,</span> <span class="mf">0.23144772934913635</span><span class="p">,</span> <span class="mf">0.22532679361104965</span><span class="p">,</span> <span class="mf">0.2152210614681244</span><span class="p">,</span> <span class="mf">0.21610748746991157</span><span class="p">,</span> <span class="mf">0.22872606116533278</span><span class="p">,</span> <span class="mf">0.22058768355846406</span><span class="p">,</span> <span class="mf">0.20230921444296837</span><span class="p">,</span> <span class="mf">0.2118315652012825</span><span class="p">,</span> <span class="mf">0.20028054055571556</span><span class="p">,</span> <span class="mf">0.20844366964697839</span><span class="p">,</span> <span class="mf">0.20884322375059128</span><span class="p">,</span> <span class="mf">0.21231223946809769</span><span class="p">,</span> <span class="mf">0.19875787001848222</span><span class="p">,</span> <span class="mf">0.2072589308321476</span><span class="p">,</span> <span class="mf">0.22480831852555275</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.862</span><span class="p">,</span> <span class="mf">0.894</span><span class="p">,</span> <span class="mf">0.892375</span><span class="p">,</span> <span class="mf">0.906375</span><span class="p">,</span> <span class="mf">0.912625</span><span class="p">,</span> <span class="mf">0.91375</span><span class="p">,</span> <span class="mf">0.916875</span><span class="p">,</span> <span class="mf">0.918875</span><span class="p">,</span> <span class="mf">0.92125</span><span class="p">,</span> <span class="mf">0.9185</span><span class="p">,</span> <span class="mf">0.920375</span><span class="p">,</span> <span class="mf">0.92825</span><span class="p">,</span> <span class="mf">0.9255</span><span class="p">,</span> <span class="mf">0.92925</span><span class="p">,</span> <span class="mf">0.926875</span><span class="p">,</span> <span class="mf">0.9285</span><span class="p">,</span> <span class="mf">0.926375</span><span class="p">,</span> <span class="mf">0.93075</span><span class="p">,</span> <span class="mf">0.931125</span><span class="p">,</span> <span class="mf">0.922875</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.6208003907124879</span><span class="p">,</span> <span class="mf">0.4341448332582201</span><span class="p">,</span> <span class="mf">0.3655890760454796</span><span class="p">,</span> <span class="mf">0.3245583019102179</span><span class="p">,</span> <span class="mf">0.3000562671722888</span><span class="p">,</span> <span class="mf">0.2840681741280215</span><span class="p">,</span> <span class="mf">0.2686156402947679</span><span class="p">,</span> <span class="mf">0.25843519997844566</span><span class="p">,</span> <span class="mf">0.24892204790227196</span><span class="p">,</span> <span class="mf">0.23988707410469493</span><span class="p">,</span> <span class="mf">0.22968693327770304</span><span class="p">,</span> <span class="mf">0.22323107979953416</span><span class="p">,</span> <span class="mf">0.21376596502403714</span><span class="p">,</span> <span class="mf">0.21353628940340172</span><span class="p">,</span> <span class="mf">0.208721635311143</span><span class="p">,</span> <span class="mf">0.20283085862393063</span><span class="p">,</span> <span class="mf">0.19862186088204892</span><span class="p">,</span> <span class="mf">0.1939613972542319</span><span class="p">,</span> <span class="mf">0.18833921627917968</span><span class="p">,</span> <span class="mf">0.18451892669552933</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7769666666666667</span><span class="p">,</span> <span class="mf">0.8453333333333334</span><span class="p">,</span> <span class="mf">0.86965</span><span class="p">,</span> <span class="mf">0.88425</span><span class="p">,</span> <span class="mf">0.8911</span><span class="p">,</span> <span class="mf">0.8957666666666667</span><span class="p">,</span> <span class="mf">0.90125</span><span class="p">,</span> <span class="mf">0.9056666666666666</span><span class="p">,</span> <span class="mf">0.9083833333333333</span><span class="p">,</span> <span class="mf">0.9122666666666667</span><span class="p">,</span> <span class="mf">0.91455</span><span class="p">,</span> <span class="mf">0.9176833333333333</span><span class="p">,</span> <span class="mf">0.92035</span><span class="p">,</span> <span class="mf">0.9217</span><span class="p">,</span> <span class="mf">0.9232333333333334</span><span class="p">,</span> <span class="mf">0.9238333333333333</span><span class="p">,</span> <span class="mf">0.9270333333333334</span><span class="p">,</span> <span class="mf">0.9283</span><span class="p">,</span> <span class="mf">0.93035</span><span class="p">,</span> <span class="mf">0.9312333333333334</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.390482270359993</span><span class="p">,</span> <span class="mf">0.3140819278359413</span><span class="p">,</span> <span class="mf">0.286346542596817</span><span class="p">,</span> <span class="mf">0.26530489122867584</span><span class="p">,</span> <span class="mf">0.25648517191410064</span><span class="p">,</span> <span class="mf">0.25534764647483826</span><span class="p">,</span> <span class="mf">0.24066219604015351</span><span class="p">,</span> <span class="mf">0.22813884472846985</span><span class="p">,</span> <span class="mf">0.22091108289361</span><span class="p">,</span> <span class="mf">0.22591463786363603</span><span class="p">,</span> <span class="mf">0.22548504903912545</span><span class="p">,</span> <span class="mf">0.21807716876268388</span><span class="p">,</span> <span class="mf">0.23463654381036758</span><span class="p">,</span> <span class="mf">0.21917386519908905</span><span class="p">,</span> <span class="mf">0.2077158398628235</span><span class="p">,</span> <span class="mf">0.2112607652246952</span><span class="p">,</span> <span class="mf">0.205703763961792</span><span class="p">,</span> <span class="mf">0.21748955991864205</span><span class="p">,</span> <span class="mf">0.20092388433218003</span><span class="p">,</span> <span class="mf">0.20742826372385026</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.859125</span><span class="p">,</span> <span class="mf">0.884375</span><span class="p">,</span> <span class="mf">0.89225</span><span class="p">,</span> <span class="mf">0.9035</span><span class="p">,</span> <span class="mf">0.9045</span><span class="p">,</span> <span class="mf">0.904875</span><span class="p">,</span> <span class="mf">0.907875</span><span class="p">,</span> <span class="mf">0.915375</span><span class="p">,</span> <span class="mf">0.914875</span><span class="p">,</span> <span class="mf">0.915375</span><span class="p">,</span> <span class="mf">0.916375</span><span class="p">,</span> <span class="mf">0.92075</span><span class="p">,</span> <span class="mf">0.91575</span><span class="p">,</span> <span class="mf">0.91825</span><span class="p">,</span> <span class="mf">0.92375</span><span class="p">,</span> <span class="mf">0.924</span><span class="p">,</span> <span class="mf">0.924875</span><span class="p">,</span> <span class="mf">0.917125</span><span class="p">,</span> <span class="mf">0.926875</span><span class="p">,</span> <span class="mf">0.920875</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.1608194957918196</span><span class="p">,</span> <span class="mf">0.8736483463918222</span><span class="p">,</span> <span class="mf">0.7270457689632485</span><span class="p">,</span> <span class="mf">0.6118623841482439</span><span class="p">,</span> <span class="mf">0.5539627463769302</span><span class="p">,</span> <span class="mf">0.5169604117872872</span><span class="p">,</span> <span class="mf">0.4843029365547176</span><span class="p">,</span> <span class="mf">0.4664089765979537</span><span class="p">,</span> <span class="mf">0.449539397952399</span><span class="p">,</span> <span class="mf">0.4308713404481599</span><span class="p">,</span> <span class="mf">0.4170197155842903</span><span class="p">,</span> <span class="mf">0.4104185118508746</span><span class="p">,</span> <span class="mf">0.3983522486299086</span><span class="p">,</span> <span class="mf">0.3890672579232945</span><span class="p">,</span> <span class="mf">0.38423672571047535</span><span class="p">,</span> <span class="mf">0.38125834129512437</span><span class="p">,</span> <span class="mf">0.36963055836461756</span><span class="p">,</span> <span class="mf">0.36898326972273116</span><span class="p">,</span> <span class="mf">0.3608236700328174</span><span class="p">,</span> <span class="mf">0.35822524538617145</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.56785</span><span class="p">,</span> <span class="mf">0.6591833333333333</span><span class="p">,</span> <span class="mf">0.71765</span><span class="p">,</span> <span class="mf">0.7660333333333333</span><span class="p">,</span> <span class="mf">0.7931666666666667</span><span class="p">,</span> <span class="mf">0.8079666666666667</span><span class="p">,</span> <span class="mf">0.8198833333333333</span><span class="p">,</span> <span class="mf">0.8275166666666667</span><span class="p">,</span> <span class="mf">0.8349833333333333</span><span class="p">,</span> <span class="mf">0.8422</span><span class="p">,</span> <span class="mf">0.8473666666666667</span><span class="p">,</span> <span class="mf">0.8486833333333333</span><span class="p">,</span> <span class="mf">0.85425</span><span class="p">,</span> <span class="mf">0.85675</span><span class="p">,</span> <span class="mf">0.8578666666666667</span><span class="p">,</span> <span class="mf">0.8603333333333333</span><span class="p">,</span> <span class="mf">0.8643333333333333</span><span class="p">,</span> <span class="mf">0.8637833333333333</span><span class="p">,</span> <span class="mf">0.8684333333333333</span><span class="p">,</span> <span class="mf">0.8680166666666667</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5984484012126923</span><span class="p">,</span> <span class="mf">0.5152713191509247</span><span class="p">,</span> <span class="mf">0.42289899206161496</span><span class="p">,</span> <span class="mf">0.3746640253067017</span><span class="p">,</span> <span class="mf">0.3369040569067001</span><span class="p">,</span> <span class="mf">0.32359291434288023</span><span class="p">,</span> <span class="mf">0.2978636801838875</span><span class="p">,</span> <span class="mf">0.2998174095153809</span><span class="p">,</span> <span class="mf">0.2883352539539337</span><span class="p">,</span> <span class="mf">0.2839300352931023</span><span class="p">,</span> <span class="mf">0.2775397801399231</span><span class="p">,</span> <span class="mf">0.2616970262527466</span><span class="p">,</span> <span class="mf">0.259125192284584</span><span class="p">,</span> <span class="mf">0.25470315623283385</span><span class="p">,</span> <span class="mf">0.2535187450051308</span><span class="p">,</span> <span class="mf">0.2600560383200645</span><span class="p">,</span> <span class="mf">0.25031394577026367</span><span class="p">,</span> <span class="mf">0.2547155976295471</span><span class="p">,</span> <span class="mf">0.23950587111711502</span><span class="p">,</span> <span class="mf">0.24401323813199996</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.750875</span><span class="p">,</span> <span class="mf">0.78025</span><span class="p">,</span> <span class="mf">0.86225</span><span class="p">,</span> <span class="mf">0.869875</span><span class="p">,</span> <span class="mf">0.884875</span><span class="p">,</span> <span class="mf">0.891625</span><span class="p">,</span> <span class="mf">0.898875</span><span class="p">,</span> <span class="mf">0.89275</span><span class="p">,</span> <span class="mf">0.901875</span><span class="p">,</span> <span class="mf">0.9005</span><span class="p">,</span> <span class="mf">0.899875</span><span class="p">,</span> <span class="mf">0.908375</span><span class="p">,</span> <span class="mf">0.91125</span><span class="p">,</span> <span class="mf">0.910375</span><span class="p">,</span> <span class="mf">0.910375</span><span class="p">,</span> <span class="mf">0.907</span><span class="p">,</span> <span class="mf">0.9135</span><span class="p">,</span> <span class="mf">0.910375</span><span class="p">,</span> <span class="mf">0.914125</span><span class="p">,</span> <span class="mf">0.911625</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.5018121279410716</span><span class="p">,</span> <span class="mf">0.3649225841834347</span><span class="p">,</span> <span class="mf">0.31199926770985253</span><span class="p">,</span> <span class="mf">0.2825479824850554</span><span class="p">,</span> <span class="mf">0.25993211727057186</span><span class="p">,</span> <span class="mf">0.2431308363737074</span><span class="p">,</span> <span class="mf">0.22870161555913973</span><span class="p">,</span> <span class="mf">0.22126636312587428</span><span class="p">,</span> <span class="mf">0.2113911879540824</span><span class="p">,</span> <span class="mf">0.20279224649834227</span><span class="p">,</span> <span class="mf">0.19300907663603836</span><span class="p">,</span> <span class="mf">0.18686007729360163</span><span class="p">,</span> <span class="mf">0.1815741605866057</span><span class="p">,</span> <span class="mf">0.1759802805684777</span><span class="p">,</span> <span class="mf">0.17041425832084564</span><span class="p">,</span> <span class="mf">0.16513840764014323</span><span class="p">,</span> <span class="mf">0.15892388751861383</span><span class="p">,</span> <span class="mf">0.1548161118118557</span><span class="p">,</span> <span class="mf">0.1498002242614656</span><span class="p">,</span> <span class="mf">0.14744469122107284</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8158</span><span class="p">,</span> <span class="mf">0.8648</span><span class="p">,</span> <span class="mf">0.8846833333333334</span><span class="p">,</span> <span class="mf">0.8954666666666666</span><span class="p">,</span> <span class="mf">0.9035333333333333</span><span class="p">,</span> <span class="mf">0.9097666666666666</span><span class="p">,</span> <span class="mf">0.9142666666666667</span><span class="p">,</span> <span class="mf">0.91615</span><span class="p">,</span> <span class="mf">0.9219166666666667</span><span class="p">,</span> <span class="mf">0.9239333333333334</span><span class="p">,</span> <span class="mf">0.9268166666666666</span><span class="p">,</span> <span class="mf">0.9287666666666666</span><span class="p">,</span> <span class="mf">0.9304833333333333</span><span class="p">,</span> <span class="mf">0.9327333333333333</span><span class="p">,</span> <span class="mf">0.9365</span><span class="p">,</span> <span class="mf">0.9368666666666666</span><span class="p">,</span> <span class="mf">0.9395333333333333</span><span class="p">,</span> <span class="mf">0.9418833333333333</span><span class="p">,</span> <span class="mf">0.9445</span><span class="p">,</span> <span class="mf">0.9450166666666666</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.35916801404953</span><span class="p">,</span> <span class="mf">0.30038927191495896</span><span class="p">,</span> <span class="mf">0.2824265750646591</span><span class="p">,</span> <span class="mf">0.28094157111644746</span><span class="p">,</span> <span class="mf">0.2402345055937767</span><span class="p">,</span> <span class="mf">0.24779821130633353</span><span class="p">,</span> <span class="mf">0.2263277245759964</span><span class="p">,</span> <span class="mf">0.22270147562026976</span><span class="p">,</span> <span class="mf">0.22010754531621932</span><span class="p">,</span> <span class="mf">0.20850908517837524</span><span class="p">,</span> <span class="mf">0.21723379525542258</span><span class="p">,</span> <span class="mf">0.20454896742105483</span><span class="p">,</span> <span class="mf">0.2065480750799179</span><span class="p">,</span> <span class="mf">0.20593296563625335</span><span class="p">,</span> <span class="mf">0.21030707907676696</span><span class="p">,</span> <span class="mf">0.2015896993279457</span><span class="p">,</span> <span class="mf">0.19770563289523124</span><span class="p">,</span> <span class="mf">0.19552358242869378</span><span class="p">,</span> <span class="mf">0.197759574085474</span><span class="p">,</span> <span class="mf">0.19900305101275445</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.867125</span><span class="p">,</span> <span class="mf">0.890875</span><span class="p">,</span> <span class="mf">0.896875</span><span class="p">,</span> <span class="mf">0.896</span><span class="p">,</span> <span class="mf">0.912125</span><span class="p">,</span> <span class="mf">0.90875</span><span class="p">,</span> <span class="mf">0.9185</span><span class="p">,</span> <span class="mf">0.916875</span><span class="p">,</span> <span class="mf">0.920375</span><span class="p">,</span> <span class="mf">0.925125</span><span class="p">,</span> <span class="mf">0.919375</span><span class="p">,</span> <span class="mf">0.92675</span><span class="p">,</span> <span class="mf">0.927125</span><span class="p">,</span> <span class="mf">0.924625</span><span class="p">,</span> <span class="mf">0.924125</span><span class="p">,</span> <span class="mf">0.9275</span><span class="p">,</span> <span class="mf">0.928</span><span class="p">,</span> <span class="mf">0.928875</span><span class="p">,</span> <span class="mf">0.93325</span><span class="p">,</span> <span class="mf">0.930125</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.564780301424359</span><span class="p">,</span> <span class="mf">0.41836969141385705</span><span class="p">,</span> <span class="mf">0.3581543931924204</span><span class="p">,</span> <span class="mf">0.3251280398018706</span><span class="p">,</span> <span class="mf">0.30215959723538427</span><span class="p">,</span> <span class="mf">0.28700008430778345</span><span class="p">,</span> <span class="mf">0.27507679125488693</span><span class="p">,</span> <span class="mf">0.26540731782439164</span><span class="p">,</span> <span class="mf">0.25373875692105496</span><span class="p">,</span> <span class="mf">0.24964979071734048</span><span class="p">,</span> <span class="mf">0.24098571216357922</span><span class="p">,</span> <span class="mf">0.23604591902512223</span><span class="p">,</span> <span class="mf">0.2270722362135392</span><span class="p">,</span> <span class="mf">0.2229606584985373</span><span class="p">,</span> <span class="mf">0.22031292727570545</span><span class="p">,</span> <span class="mf">0.21439386613126885</span><span class="p">,</span> <span class="mf">0.21020108821200156</span><span class="p">,</span> <span class="mf">0.2042837777872012</span><span class="p">,</span> <span class="mf">0.20376247368149283</span><span class="p">,</span> <span class="mf">0.20021205727082453</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7927</span><span class="p">,</span> <span class="mf">0.8474166666666667</span><span class="p">,</span> <span class="mf">0.8672166666666666</span><span class="p">,</span> <span class="mf">0.8811833333333333</span><span class="p">,</span> <span class="mf">0.8883</span><span class="p">,</span> <span class="mf">0.8952833333333333</span><span class="p">,</span> <span class="mf">0.89795</span><span class="p">,</span> <span class="mf">0.9011333333333333</span><span class="p">,</span> <span class="mf">0.9055833333333333</span><span class="p">,</span> <span class="mf">0.9071166666666667</span><span class="p">,</span> <span class="mf">0.9100333333333334</span><span class="p">,</span> <span class="mf">0.911</span><span class="p">,</span> <span class="mf">0.91515</span><span class="p">,</span> <span class="mf">0.9162166666666667</span><span class="p">,</span> <span class="mf">0.91775</span><span class="p">,</span> <span class="mf">0.9197833333333333</span><span class="p">,</span> <span class="mf">0.9218666666666666</span><span class="p">,</span> <span class="mf">0.9239</span><span class="p">,</span> <span class="mf">0.9236833333333333</span><span class="p">,</span> <span class="mf">0.92455</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.39558523416519165</span><span class="p">,</span> <span class="mf">0.3187315353155136</span><span class="p">,</span> <span class="mf">0.30105597496032716</span><span class="p">,</span> <span class="mf">0.2717038299441338</span><span class="p">,</span> <span class="mf">0.25286867189407347</span><span class="p">,</span> <span class="mf">0.24664685553312302</span><span class="p">,</span> <span class="mf">0.24286985045671464</span><span class="p">,</span> <span class="mf">0.23643679201602935</span><span class="p">,</span> <span class="mf">0.23006864881515504</span><span class="p">,</span> <span class="mf">0.2277349520921707</span><span class="p">,</span> <span class="mf">0.22591854375600814</span><span class="p">,</span> <span class="mf">0.2165311907827854</span><span class="p">,</span> <span class="mf">0.21385486593842506</span><span class="p">,</span> <span class="mf">0.21402871897816658</span><span class="p">,</span> <span class="mf">0.2096972267627716</span><span class="p">,</span> <span class="mf">0.21242560443282127</span><span class="p">,</span> <span class="mf">0.2098898750245571</span><span class="p">,</span> <span class="mf">0.2062524998188019</span><span class="p">,</span> <span class="mf">0.19932547932863234</span><span class="p">,</span> <span class="mf">0.20170186588168143</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.850625</span><span class="p">,</span> <span class="mf">0.88125</span><span class="p">,</span> <span class="mf">0.8845</span><span class="p">,</span> <span class="mf">0.897125</span><span class="p">,</span> <span class="mf">0.9065</span><span class="p">,</span> <span class="mf">0.9085</span><span class="p">,</span> <span class="mf">0.907625</span><span class="p">,</span> <span class="mf">0.91275</span><span class="p">,</span> <span class="mf">0.917125</span><span class="p">,</span> <span class="mf">0.9135</span><span class="p">,</span> <span class="mf">0.91825</span><span class="p">,</span> <span class="mf">0.922625</span><span class="p">,</span> <span class="mf">0.91925</span><span class="p">,</span> <span class="mf">0.921125</span><span class="p">,</span> <span class="mf">0.923625</span><span class="p">,</span> <span class="mf">0.92225</span><span class="p">,</span> <span class="mf">0.923375</span><span class="p">,</span> <span class="mf">0.922875</span><span class="p">,</span> <span class="mf">0.925625</span><span class="p">,</span> <span class="mf">0.92775</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.6916971901205303</span><span class="p">,</span> <span class="mf">0.4947840944567977</span><span class="p">,</span> <span class="mf">0.41710148827988963</span><span class="p">,</span> <span class="mf">0.38678343986460906</span><span class="p">,</span> <span class="mf">0.36429949198513906</span><span class="p">,</span> <span class="mf">0.34339441834831796</span><span class="p">,</span> <span class="mf">0.33055868282564665</span><span class="p">,</span> <span class="mf">0.3199633415272114</span><span class="p">,</span> <span class="mf">0.31550557391920575</span><span class="p">,</span> <span class="mf">0.3022628513289921</span><span class="p">,</span> <span class="mf">0.2959158662110885</span><span class="p">,</span> <span class="mf">0.2941135993993867</span><span class="p">,</span> <span class="mf">0.28555906579089063</span><span class="p">,</span> <span class="mf">0.27903660322462065</span><span class="p">,</span> <span class="mf">0.2769482293601102</span><span class="p">,</span> <span class="mf">0.27154609372716215</span><span class="p">,</span> <span class="mf">0.26548120195963487</span><span class="p">,</span> <span class="mf">0.26188135733291795</span><span class="p">,</span> <span class="mf">0.2588035051009929</span><span class="p">,</span> <span class="mf">0.2574938320115939</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7497333333333334</span><span class="p">,</span> <span class="mf">0.8236833333333333</span><span class="p">,</span> <span class="mf">0.8482333333333333</span><span class="p">,</span> <span class="mf">0.8618666666666667</span><span class="p">,</span> <span class="mf">0.8703666666666666</span><span class="p">,</span> <span class="mf">0.8772166666666666</span><span class="p">,</span> <span class="mf">0.8803333333333333</span><span class="p">,</span> <span class="mf">0.8829166666666667</span><span class="p">,</span> <span class="mf">0.88525</span><span class="p">,</span> <span class="mf">0.88945</span><span class="p">,</span> <span class="mf">0.89275</span><span class="p">,</span> <span class="mf">0.8937166666666667</span><span class="p">,</span> <span class="mf">0.8969</span><span class="p">,</span> <span class="mf">0.8977666666666667</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.90175</span><span class="p">,</span> <span class="mf">0.9041666666666667</span><span class="p">,</span> <span class="mf">0.9035</span><span class="p">,</span> <span class="mf">0.9049</span><span class="p">,</span> <span class="mf">0.9046166666666666</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.41916924858093263</span><span class="p">,</span> <span class="mf">0.3380992366075516</span><span class="p">,</span> <span class="mf">0.31549062132835387</span><span class="p">,</span> <span class="mf">0.2921286026239395</span><span class="p">,</span> <span class="mf">0.2786481494307518</span><span class="p">,</span> <span class="mf">0.28516836106777194</span><span class="p">,</span> <span class="mf">0.25556409001350405</span><span class="p">,</span> <span class="mf">0.2538892236948013</span><span class="p">,</span> <span class="mf">0.24726227968931197</span><span class="p">,</span> <span class="mf">0.24262803781032563</span><span class="p">,</span> <span class="mf">0.24080126863718032</span><span class="p">,</span> <span class="mf">0.24242325466871262</span><span class="p">,</span> <span class="mf">0.23416680485010147</span><span class="p">,</span> <span class="mf">0.22847312396764755</span><span class="p">,</span> <span class="mf">0.22423979061841964</span><span class="p">,</span> <span class="mf">0.2311997367441654</span><span class="p">,</span> <span class="mf">0.22794704174995423</span><span class="p">,</span> <span class="mf">0.21943940049409866</span><span class="p">,</span> <span class="mf">0.21820387506484987</span><span class="p">,</span> <span class="mf">0.21150743806362152</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8435</span><span class="p">,</span> <span class="mf">0.87725</span><span class="p">,</span> <span class="mf">0.88425</span><span class="p">,</span> <span class="mf">0.890375</span><span class="p">,</span> <span class="mf">0.898125</span><span class="p">,</span> <span class="mf">0.89275</span><span class="p">,</span> <span class="mf">0.905625</span><span class="p">,</span> <span class="mf">0.906125</span><span class="p">,</span> <span class="mf">0.911</span><span class="p">,</span> <span class="mf">0.910625</span><span class="p">,</span> <span class="mf">0.911</span><span class="p">,</span> <span class="mf">0.909875</span><span class="p">,</span> <span class="mf">0.914875</span><span class="p">,</span> <span class="mf">0.915375</span><span class="p">,</span> <span class="mf">0.917875</span><span class="p">,</span> <span class="mf">0.915</span><span class="p">,</span> <span class="mf">0.91475</span><span class="p">,</span> <span class="mf">0.919625</span><span class="p">,</span> <span class="mf">0.923875</span><span class="p">,</span> <span class="mf">0.92425</span><span class="p">]],</span> <span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.162218615571573</span><span class="p">,</span> <span class="mf">0.8284856370453642</span><span class="p">,</span> <span class="mf">0.7309887468624217</span><span class="p">,</span> <span class="mf">0.6590983641744931</span><span class="p">,</span> <span class="mf">0.6089096262510906</span><span class="p">,</span> <span class="mf">0.5663433943285363</span><span class="p">,</span> <span class="mf">0.5383681068733048</span><span class="p">,</span> <span class="mf">0.5242803116787725</span><span class="p">,</span> <span class="mf">0.49926126579930785</span><span class="p">,</span> <span class="mf">0.48940120944018556</span><span class="p">,</span> <span class="mf">0.4789252862779062</span><span class="p">,</span> <span class="mf">0.46633604049746163</span><span class="p">,</span> <span class="mf">0.4596060775458686</span><span class="p">,</span> <span class="mf">0.4464966354847971</span><span class="p">,</span> <span class="mf">0.4418302221593064</span><span class="p">,</span> <span class="mf">0.43759817490254893</span><span class="p">,</span> <span class="mf">0.42892070028827645</span><span class="p">,</span> <span class="mf">0.4226101264516428</span><span class="p">,</span> <span class="mf">0.418694807601763</span><span class="p">,</span> <span class="mf">0.4110745745840103</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.58005</span><span class="p">,</span> <span class="mf">0.6824666666666667</span><span class="p">,</span> <span class="mf">0.7223333333333334</span><span class="p">,</span> <span class="mf">0.7464333333333333</span><span class="p">,</span> <span class="mf">0.7711333333333333</span><span class="p">,</span> <span class="mf">0.7891833333333333</span><span class="p">,</span> <span class="mf">0.8012333333333334</span><span class="p">,</span> <span class="mf">0.80635</span><span class="p">,</span> <span class="mf">0.8172666666666667</span><span class="p">,</span> <span class="mf">0.82225</span><span class="p">,</span> <span class="mf">0.8271833333333334</span><span class="p">,</span> <span class="mf">0.831</span><span class="p">,</span> <span class="mf">0.8335833333333333</span><span class="p">,</span> <span class="mf">0.8371833333333333</span><span class="p">,</span> <span class="mf">0.8412166666666666</span><span class="p">,</span> <span class="mf">0.84265</span><span class="p">,</span> <span class="mf">0.8458833333333333</span><span class="p">,</span> <span class="mf">0.8471166666666666</span><span class="p">,</span> <span class="mf">0.8497666666666667</span><span class="p">,</span> <span class="mf">0.8522833333333333</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5945872340202332</span><span class="p">,</span> <span class="mf">0.518519122838974</span><span class="p">,</span> <span class="mf">0.4681703653335571</span><span class="p">,</span> <span class="mf">0.42978407418727876</span><span class="p">,</span> <span class="mf">0.40349935555458066</span><span class="p">,</span> <span class="mf">0.37377681517601014</span><span class="p">,</span> <span class="mf">0.35234942865371705</span><span class="p">,</span> <span class="mf">0.3359788683652878</span><span class="p">,</span> <span class="mf">0.3217720929384232</span><span class="p">,</span> <span class="mf">0.3279728285074234</span><span class="p">,</span> <span class="mf">0.3114012089371681</span><span class="p">,</span> <span class="mf">0.3060767319202423</span><span class="p">,</span> <span class="mf">0.2949701727628708</span><span class="p">,</span> <span class="mf">0.2981588536500931</span><span class="p">,</span> <span class="mf">0.2855641575455666</span><span class="p">,</span> <span class="mf">0.28112928783893587</span><span class="p">,</span> <span class="mf">0.28212732630968096</span><span class="p">,</span> <span class="mf">0.27846804082393645</span><span class="p">,</span> <span class="mf">0.27372796374559405</span><span class="p">,</span> <span class="mf">0.27415593349933626</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.78525</span><span class="p">,</span> <span class="mf">0.8215</span><span class="p">,</span> <span class="mf">0.820125</span><span class="p">,</span> <span class="mf">0.844375</span><span class="p">,</span> <span class="mf">0.86375</span><span class="p">,</span> <span class="mf">0.875125</span><span class="p">,</span> <span class="mf">0.876625</span><span class="p">,</span> <span class="mf">0.882</span><span class="p">,</span> <span class="mf">0.887875</span><span class="p">,</span> <span class="mf">0.884625</span><span class="p">,</span> <span class="mf">0.890375</span><span class="p">,</span> <span class="mf">0.892125</span><span class="p">,</span> <span class="mf">0.897125</span><span class="p">,</span> <span class="mf">0.894125</span><span class="p">,</span> <span class="mf">0.902625</span><span class="p">,</span> <span class="mf">0.89975</span><span class="p">,</span> <span class="mf">0.89975</span><span class="p">,</span> <span class="mf">0.90125</span><span class="p">,</span> <span class="mf">0.902</span><span class="p">,</span> <span class="mf">0.90075</span><span class="p">]]]</span>


<span class="n">Dropout1</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="c1">#param {type:"slider", min:0, max:0.75, step:0.25}</span>
<span class="n">Dropout2</span> <span class="o">=</span> <span class="mf">0.75</span> <span class="c1">#param {type:"slider", min:0, max:0.75, step:0.25}</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">Dropout1</span><span class="p">,</span> <span class="n">Dropout2</span><span class="p">):</span>
  <span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">validation_loss</span><span class="p">,</span> <span class="n">validation_acc</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">Dropout1</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span><span class="o">*</span><span class="mi">4</span><span class="o">+</span><span class="nb">int</span><span class="p">(</span><span class="n">Dropout2</span><span class="o">*</span><span class="mi">4</span><span class="p">)]</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">)</span>
  <span class="n">plot_loss_accuracy</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span><span class="p">,</span> <span class="n">validation_loss</span><span class="p">,</span> <span class="n">validation_acc</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="kn">import</span> <span class="nn">io</span><span class="o">,</span> <span class="nn">base64</span>
  <span class="n">my_stringIObytes</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">my_stringIObytes</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">'png'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
  <span class="n">my_stringIObytes</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">my_base64_jpgData</span> <span class="o">=</span> <span class="n">base64</span><span class="o">.</span><span class="n">b64encode</span><span class="p">(</span><span class="n">my_stringIObytes</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
  <span class="n">p</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="s2">"""&lt;img src="data:image/png;base64,"""</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">my_base64_jpgData</span><span class="p">)[</span><span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="s2">"""" alt="Graph"&gt;"""</span>

<span class="n">d1</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"Dropout 1"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="p">{</span><span class="s1">'description_width'</span><span class="p">:</span> <span class="s1">'initial'</span><span class="p">,</span> <span class="s1">'width'</span><span class="p">:</span> <span class="s1">'800px'</span><span class="p">},</span> <span class="p">)</span>
<span class="n">d2</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"Dropout 2"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="p">{</span><span class="s1">'description_width'</span><span class="p">:</span> <span class="s1">'initial'</span><span class="p">,</span> <span class="s1">'width'</span><span class="p">:</span> <span class="s1">'800px'</span><span class="p">},</span> <span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">HTML</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="s2">"aasdsd"</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">interactive_output</span><span class="p">(</span><span class="n">plot</span><span class="p">,</span> <span class="p">{</span><span class="s2">"Dropout1"</span><span class="p">:</span><span class="n">d1</span><span class="p">,</span> <span class="s2">"Dropout2"</span><span class="p">:</span> <span class="n">d2</span><span class="p">})</span>
<span class="c1">#w.layout.height = '450px'</span>

<span class="n">display</span><span class="p">(</span><span class="n">widgets</span><span class="o">.</span><span class="n">VBox</span><span class="p">([</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">w</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-2-2-how-much-does-augmentation-help">
<h2>Coding Exercise 2.2: How much does augmentation help?<a class="headerlink" href="#coding-exercise-2-2-how-much-does-augmentation-help" title="Permalink to this headline">¶</a></h2>
<p>Last week you also learned how data augmentation can  regularize a network. Let’s add data augmentation to our model via transforms and see if that helps our model to better generalize! In the following cell, add the transforms you want in the list <code class="docutils literal notranslate"><span class="pre">augmentation_transforms</span></code>. We will then run the same network you created in the above exercise (with regularization) and then plot the loss and accuracies.</p>
<p>Here’s the link to the list of transforms available in pytorch: https://pytorch.org/vision/stable/transforms.html</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transforms_custom</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
  <span class="c1"># basic preprocessing</span>
  <span class="n">preprocessing_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                              <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))]</span>
  <span class="c1"># add the augmentation transforms to the preprocessing</span>
  <span class="n">train_transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span><span class="n">get_augmentation_transforms</span><span class="p">()</span> <span class="o">+</span> \
                                      <span class="n">preprocessing_transforms</span><span class="p">)</span>
  <span class="c1"># load the FashonMNIST dataset with the transforms</span>
  <span class="n">train_data</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                    <span class="n">transform</span><span class="o">=</span><span class="n">train_transform</span><span class="p">)</span>
  <span class="c1"># reduce to our two classes to speed up training</span>
  <span class="n">train_data</span> <span class="o">=</span> <span class="n">reduce_classes</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
  <span class="c1"># get the data loader instances for the dataset</span>
  <span class="n">train_loader</span><span class="p">,</span> <span class="n">validation_loader</span><span class="p">,</span> <span class="n">test_loader</span> <span class="o">=</span> <span class="n">get_data_loaders</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span>
                                                                  <span class="n">validation_data</span><span class="p">,</span>
                                                                  <span class="n">test_data</span><span class="p">,</span>
                                                                  <span class="n">seed</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">validation_loader</span><span class="p">,</span> <span class="n">test_loader</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_augmentation_transforms</span><span class="p">():</span>
  <span class="c1">####################################################################</span>
  <span class="c1"># Fill in missing code below (...),</span>
  <span class="c1"># then remove or comment the line below to test your function</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Add Transforms"</span><span class="p">)</span>
  <span class="c1">####################################################################</span>
  <span class="n">augmentation_transforms</span> <span class="o">=</span> <span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

  <span class="k">return</span> <span class="n">augmentation_transforms</span>


<span class="n">set_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">net3</span> <span class="o">=</span> <span class="n">FMNIST_Net2</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>  <span class="c1"># get the network</span>

<span class="c1">## Uncomment below to test your function</span>
<span class="c1"># train_loader, validation_loader, test_loader = transforms_custom(SEED)</span>
<span class="c1"># train_loss, train_acc, validation_loss, validation_acc = train(net3, DEVICE, train_loader, validation_loader, 20)</span>
<span class="c1"># plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/solutions/W2D1_Tutorial2_Solution_8d4116f3.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/static/W2D1_Tutorial2_Solution_8d4116f3_3.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/static/W2D1_Tutorial2_Solution_8d4116f3_3.png" style="width: 2195.0px; height: 755.0px;"/></a>
<p>Run the next cell to get the accuracy on the data!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test</span><span class="p">(</span><span class="n">net3</span><span class="p">,</span> <span class="n">DEVICE</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="think-3-1-data-augmentation">
<h2>Think! 3.1: Data Augmentation<a class="headerlink" href="#think-3-1-data-augmentation" title="Permalink to this headline">¶</a></h2>
<p>Did the training accuracy reduce further compared to with dropout alone? Is the model still overfitting?</p>
<p><a class="reference external" href="https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/solutions/W2D1_Tutorial2_Solution_ae125a93.py"><em>Click for solution</em></a></p>
<p>Great! In this section you trained what may have been your very first CNN. You added regularization and data augmentation in order to get a model that generalizes well. All the pieces are beginning to fit together!</p>
<p>Next we will talk about RNNs, which parameter share over time.</p>
</div>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W2D1_ConvnetsAndRecurrentNeuralNetworks/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
<div class="prev-next-bottom">
<a class="left-prev" href="W2D1_Tutorial1.html" id="prev-link" title="previous page">Tutorial 1: Introduction to CNNs</a>
<a class="right-next" href="W2D1_Tutorial3.html" id="next-link" title="next page">Tutorial 3: Introduction to RNNs</a>
</div>
</div>
</div>
<footer class="footer mt-5 mt-md-0">
<div class="container">
<p>
        
          By Neuromatch<br>
        
            © Copyright 2021.<br>
</br></br></p>
</div>
</footer>
</main>
</div>
</div>
<script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>
</body>
</html>